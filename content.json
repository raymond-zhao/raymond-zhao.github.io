{"meta":{"title":"吹梦到西洲","subtitle":"如果我的墓志铭需要一句话，我希望是: 我已尽我所能。","description":"","author":"吹梦到西洲","url":"http://raymond-zhao.top","root":"/"},"pages":[{"title":"","date":"2023-03-09T00:37:44.973Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"404.html","permalink":"http://raymond-zhao.top/404.html","excerpt":"","text":""},{"title":"二十多岁的你","date":"2020-03-21T02:19:08.000Z","updated":"2022-05-07T03:06:08.000Z","comments":true,"path":"about/index.html","permalink":"http://raymond-zhao.top/about/index.html","excerpt":"","text":"未完成你高中时学习一般，高考时虽然考了个一本分数但是也只能上个二本学校，你花了很长时间去责怪高考制度地区化太严重，教育资源不均匀，等到年龄更大了点，读的书更多了点，日记写的更深刻了点，你开始接受你身边的事情，对你来说事情不再有好坏之分，你越来越相信每个人在这个社会中都有自己所要扮演的角色，认为这就是这个社会的运转方式。 你因为以前没怎么接触过计算机，在本科第一学期挂了 C 语言，虽然还是排在专业前 15%，但你傻傻地在申请表上第一句写上“小挂 C 语言”而与 5000 块的国家励志奖学金失之交臂，你看到其他那些撒谎申报得到国家励志奖学金的人开心的样子，下决心下次一定要做第一名。 上课不听讲，上自习不规律，考试靠突击，同学帮一把的话也能每科考到七八十分，但是与优秀总有很大距离。 你家境一般，父母都是普通员工，你在这个城市的生活费是每月一千二，没事下下馆子，一个月添件衣服，想买台相机，咬咬牙才能买双自己喜欢的鞋。 你几乎没有特长，不会弹吉他，不会弹钢琴，不会跳舞，不会画画，想学摄影却不会使用图片处理软件，想上台演出却没信心，学校晚会比赛的时候，你经常是站在台下围观的人群里的一员，你与聚光灯环绕的舞台几乎绝缘。 你长相一般，不算英俊或者不算美丽，身材不算臃肿但也没什么肌肉或者没什么曲线，平时只是稍稍打扮一下，看上去并不出众，只能算整洁，与人擦肩而过是对方不会多留意你一眼。 你的感情也是一般，有时候会遇见自己心仪的那个人，但是总抓不住机会，眨眼间那个人就被其他人俘获，你就开始伤心、抱怨，但是几天之后又开始寻找新的心上人，就这样看着一个个心上人走过，直到你毕业，与其中任何一个都没有发展。 总之，你没有什么特别的地方 ，就和周围的千万个普通人一样。 你不甘心拿不到奖学金，看见别人得奖学金的时候你会说那完全是突击的结果，于是你开始上自习，不过你只坚持了一星期。 你不甘心自己的父辈平平，于是你批评讽刺自己周围的“官二代”、“富二代”，立志要努力学习争取成功，也好让自己的孩子成为“富二代”，你的热情持续了一个星期。 你不甘心自己什么特长都没有，于是你开始学弹吉他、买滑轮鞋、借来摄影方面的书籍，你对着镜子微笑着说：“你是最棒的。” 这份虚假的信心维持了一个星期。 你不甘心自己没有伴侣，你决心洗心革面重新做人，你删掉电脑里的偶像剧肥皂剧，你收拾起床上的懒人桌，把零食袋子统统扔掉，然后洗了个澡并且修饰了一下自己，你往发型上喷了啫喱水，好让自己看起来很精神，你怀揣着一本成功学的书决定出去走走，开始新的生活。这样的状态，你稀稀拉拉地坚持了一个星期。 一个星期之后，你还是和周围千万个人一样，你还是和一星期前的自己一样。 你逛网络论坛，看到了这样一句话：“二十岁是人生最美好的时光，不应该局限在学校里教室里，应该享受生活。” 于是你相信了，你觉得二十岁的你就应该“随心所欲”，享受“人生中最后的自由时光”；就应该“快乐地去恋爱”“风华正茂”“挥斥方遒”······ 现在的你，用着父母的血汗钱，用着名牌包、穿着名牌跑鞋、骑着捷安特山地车、用着佳能牌的相机和苹果牌的手机，还经常去星巴克喝喝咖啡体验一下小资情调······ 那么，请允许我猜测一下你的未来—— 在大四将要结束时，你考研落榜。你风风火火的参加校园招聘会，很多公司你都看不上，嫌他们不是体制内单位、平台窄、规模小，直到毕业，你还没有找到心仪的工作。你收拾好行李回到老家，父母让你试着参加各种招聘考试或者参加当地的应聘会，你不去，因为你觉得那些工作太简单了，不适合你，你应该去寻找更好的就业机会。可是，当你去那些你看得上的公司应聘时，你的竞争对手太多了，而且都不差，你表现平平，理所当然地被拒之门外······ 现在的你，也许还在上大学，也许和恋人恩恩爱爱，每天黏在一起，午饭晚饭一起去吃，晚自习后还会一起在操场散步。你们讨论起未来，最后的结论总是：不要想得太多，认真过好现在就好。 不幸运的话，几个月后，你们就分手了，你凄凄惨惨戚戚，反复问自己究竟哪里做错了；幸运的话，你们会一直恋爱到毕业，最终，你觉得自己不够优秀没能力去对方所在的城市读研或者工作，所以你们带着不舍和悔恨分手了。 现实很残酷，至此，你信了。 现在的你喜欢刷微博，你会全力支持那些你赞同的观点，你会激烈否定那些你反对的观点。你爱憎分明，看起来很有正义感。你觉得血气方刚的年轻人就应该敢于说出自己的心声。你可能从来不会去想一个问题：你的观点，来自哪里？其实，它们绝大部分来自网络，它们已经蚕食了你的判断力。 现在，我只想问你一个问题：二十岁的你，有什么资本。 你只是千千万万人中微不足道的一个人，少了你，地球还是一样会转。 我敢打赌，一定很久没人和你说过“吃得苦中苦，方为人上人”这句话了吧？ 你知道“责任”两个字是怎么写的吗? 当你谈论飞翔的时候，你是不是忘记了地心引力的存在？ 现在的你，如果还是放纵着自己的懒惰与幼稚，虚度着光阴，那么，你就虚度去吧。反正我已经过了二十岁的年纪，我还有未来，我得直奔向前了，不陪你了。 再见。"},{"title":"分类","date":"2020-03-21T02:17:09.000Z","updated":"2021-06-20T05:59:48.000Z","comments":false,"path":"categories/index.html","permalink":"http://raymond-zhao.top/categories/index.html","excerpt":"","text":""},{"title":"","date":"2023-03-09T00:37:35.356Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"config/Volantis.json","permalink":"http://raymond-zhao.top/config/Volantis.json","excerpt":"","text":"{\"info\":{\"theme_name\":\"Volantis\",\"theme_version\":\"4.0.0\",\"theme_docs\":\"https://volantis.js.org/\",\"theme_repo\":\"https://github.com/volantis-x/hexo-theme-volantis\",\"cdn\":{\"js\":null,\"css\":null}},\"navbar\":{\"visiable\":\"auto\",\"logo\":{\"img\":\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/Logo-NavBar@3x.png\",\"icon\":null,\"title\":null},\"menu\":[{\"name\":\"博客\",\"icon\":\"fas fa-rss\",\"url\":\"/\"},{\"name\":\"分类\",\"icon\":\"fas fa-folder-open\",\"url\":\"categories/\"},{\"name\":\"标签\",\"icon\":\"fas fa-tags\",\"url\":\"tags/\"},{\"name\":\"归档\",\"icon\":\"fas fa-archive\",\"url\":\"archives/\"},{\"name\":\"友链\",\"icon\":\"fas fa-link\",\"url\":\"friends/\"},{\"name\":\"关于\",\"icon\":\"fas fa-info-circle\",\"url\":\"about/\"}],\"search\":\"Search...\"},\"cover\":{\"height_scheme\":\"full\",\"layout_scheme\":\"search\",\"display\":{\"home\":true,\"archive\":true,\"others\":false},\"background\":\"/images/cover-2.jpg\",\"logo\":null,\"title\":\"吹梦到西洲\",\"subtitle\":\"\",\"search\":\"Looking for something?\",\"features\":[{\"name\":\"博客\",\"icon\":\"fas fa-rss\",\"url\":\"/\"},{\"name\":\"分类\",\"icon\":\"fas fa-folder-open\",\"url\":\"categories/\"},{\"name\":\"标签\",\"icon\":\"fas fa-tags\",\"url\":\"tags/\"},{\"name\":\"归档\",\"icon\":\"fas fa-archive\",\"url\":\"archives/\"},{\"name\":\"友链\",\"icon\":\"fas fa-link\",\"url\":\"friends/\"},{\"name\":\"关于\",\"icon\":\"fas fa-info-circle\",\"url\":\"about/\"}]},\"pages\":{\"friends\":{\"layout_scheme\":\"sites\"}},\"article\":{\"preview\":{\"scheme\":\"landscape\",\"pin_icon\":\"https://cdn.jsdelivr.net/gh/twitter/twemoji@13.0/assets/svg/1f4cc.svg\",\"auto_title\":true,\"auto_excerpt\":true,\"line_style\":\"solid\",\"author\":false,\"readmore\":\"auto\"},\"body\":{\"top_meta\":[\"author\",\"category\",\"date\",\"counter\"],\"footer_widget\":{\"references\":{\"title\":\"参考资料\",\"icon\":\"fas fa-quote-left\"},\"related_posts\":{\"enable\":false,\"title\":\"相关文章\",\"icon\":\"fas fa-bookmark\",\"max_count\":5,\"placeholder_img\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=\"},\"copyright\":{\"enable\":true,\"permalink\":\"本文永久链接是：\",\"content\":[\"博客内容遵循 署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议\",\"permalink\"]},\"donate\":{\"enable\":false,\"images\":[\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/qrcode/github@volantis.png\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/qrcode/github@volantis.png\"]}},\"bottom_meta\":[\"updated\",\"tags\",\"share\"],\"meta_library\":{\"author\":{\"avatar\":null,\"name\":\"吹梦到西洲\",\"url\":\"/\"},\"date\":{\"icon\":\"fas fa-calendar-alt\",\"title\":\"发布于：\",\"format\":\"ll\"},\"updated\":{\"icon\":\"fas fa-edit\",\"title\":\"更新于：\",\"format\":\"ll\"},\"category\":{\"icon\":\"fas fa-folder-open\"},\"counter\":{\"icon\":\"fas fa-eye\",\"unit\":\"次浏览\"},\"valinecount\":{\"icon\":\"fas fa-comment-dots\",\"desc\":\"\"},\"wordcount\":{\"icon_wordcount\":\"fas fa-keyboard\",\"icon_duration\":\"fas fa-hourglass-half\"},\"tags\":{\"icon\":\"fas fa-hashtag\"},\"share\":[{\"id\":\"qq\",\"img\":\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/qq.png\"},{\"id\":\"qzone\",\"img\":\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/qzone.png\"},{\"id\":\"weibo\",\"img\":\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/weibo.png\"},{\"id\":null,\"img\":null},{\"id\":null,\"img\":null}]}}},\"comments\":{\"title\":\" 评论\",\"subtitle\":null,\"service\":\"valine\",\"valine\":{\"appId\":\"v6cb35sRa1lfGq6EOtTQHHyf-gzGzoHsz\",\"appKey\":\"2ulfq63r4brxg2l9QfXp0cTM\",\"path\":null,\"placeholder\":\"快来评论吧~\",\"meta\":\"nick,mail,link\",\"requiredFields\":[],\"enableQQ\":true,\"recordIP\":true,\"avatar\":\"robohash\",\"pageSize\":10,\"lang\":\"zh-cn\",\"highlight\":true,\"visitor\":true,\"mathJax\":true},\"minivaline\":{\"appId\":null,\"appKey\":null,\"js\":\"https://cdn.jsdelivr.net/npm/minivaline@3/dist/MiniValine.min.js\",\"path\":null,\"placeholder\":\"快来评论吧~\",\"mode\":\"xCss\",\"math\":false,\"md\":true,\"enableQQ\":false,\"NoRecordIP\":true,\"closeFlag\":false,\"closeUA\":false,\"region\":false,\"visitor\":false,\"maxNest\":6,\"pageSize\":6,\"barrager\":0,\"role\":\"admin\",\"cloudflag\":false,\"adminEmailMd5\":\"de8a7aa53d07e6b6bceb45c64027763d\",\"tagMeta\":[\"管理员\",\"小伙伴\",\"访客\"],\"master\":[\"de8a7aa53d07e6b6bceb45c64027763d\"],\"friends\":[\"b5bd5d836c7a0091aa8473e79ed4c25e\",\"adb7d1cd192658a55c0ad22a3309cecf\",\"3ce1e6c77b4910f1871106cb30dc62b0\",\"cfce8dc43725cc14ffcd9fb4892d5bfc\"],\"lang\":null,\"emoticonUrl\":[\"https://cdn.jsdelivr.net/npm/alus@latest\",\"https://cdn.jsdelivr.net/gh/MiniValine/qq@latest\",\"https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest\",\"https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest\",\"https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest\",\"https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest\"]},\"disqus\":{\"shortname\":null,\"autoload\":false,\"path\":null},\"gitalk\":{\"clientID\":null,\"clientSecret\":null,\"repo\":null,\"owner\":null,\"admin\":null,\"path\":null},\"vssue\":{\"owner\":null,\"repo\":null,\"clientId\":null,\"clientSecret\":null},\"livere\":{\"uid\":null},\"isso\":{\"url\":\"https://example.com/(path/)\",\"src\":\"https://example.com/(path/)js/embed.min.js\"},\"hashover\":{\"src\":\"https://example.com/(path/)comments.php\"}},\"sidebar\":{\"for_page\":[\"blogger\",\"category\",\"tagcloud\",\"qrcode\"],\"for_post\":[\"toc\"],\"widget_library\":{\"blogger\":{\"class\":\"blogger\",\"display\":[\"desktop\",\"mobile\"],\"avatar\":\"images/3d-architecture.svg\",\"shape\":\"circle\",\"url\":\"/about/\",\"title\":null,\"subtitle\":null,\"jinrishici\":true,\"social\":[{\"icon\":\"fas fa-envelope\",\"url\":\"mailto:qq740567396@gmail.com\"},{\"icon\":\"fab fa-github\",\"url\":\"https://github.com/raymond-zhao\"},{\"icon\":\"fas fa-map\",\"url\":\"https://raymond-zhao.top/campus-interview/\"},{\"icon\":\"fas fa-headphones-alt\",\"url\":\"/\"}]},\"toc\":{\"class\":\"toc\",\"display\":[\"desktop\",\"mobile\"],\"header\":{\"icon\":\"fas fa-list\",\"title\":\"本文目录\"},\"list_number\":false,\"min_depth\":2,\"max_depth\":5},\"category\":{\"class\":\"category\",\"display\":[\"desktop\"],\"header\":{\"icon\":\"fas fa-folder-open\",\"title\":\"文章分类\",\"url\":\"/blog/categories/\"}},\"tagcloud\":{\"class\":\"tagcloud\",\"display\":[\"desktop\",\"mobile\"],\"header\":{\"icon\":\"fas fa-tags\",\"title\":\"热门标签\",\"url\":\"/blog/tags/\"},\"min_font\":14,\"max_font\":24,\"color\":true,\"start_color\":\"#999\",\"end_color\":\"#555\"},\"donate\":{\"class\":\"qrcode\",\"display\":[\"desktop\",\"mobile\"],\"height\":\"64px\",\"images\":[\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/qrcode/github@volantis.png\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/qrcode/github@volantis.png\"]},\"webinfo\":{\"class\":\"webinfo\",\"display\":[\"desktop\"],\"header\":{\"icon\":\"fas fa-award\",\"title\":\"站点信息\"},\"type\":{\"article\":{\"enable\":true,\"text\":\"文章数目：\",\"unit\":\"篇\"},\"runtime\":{\"enable\":true,\"data\":\"2020/01/01\",\"text\":\"已运行时间：\",\"unit\":\"天\"},\"wordcount\":{\"enable\":true,\"text\":\"本站总字数：\",\"unit\":\"字\"},\"visitcounter\":{\"service\":\"leancloud\",\"siteuv\":{\"enable\":true,\"text\":\"本站访客数：\",\"unit\":\"人\"},\"sitepv\":{\"enable\":true,\"text\":\"本站总访问量：\",\"unit\":\"次\"}},\"lastupd\":{\"enable\":true,\"friendlyShow\":true,\"text\":\"最后活动时间：\",\"unit\":\"日\"}}}}},\"tag_plugins\":{\"note\":{\"icon\":\"\\\\f054\",\"color\":\"\",\"iconfont\":\"Font Awesome 5 Free\"},\"checkbox\":{\"interactive\":false,\"color\":\"\"},\"link\":{\"placeholder\":\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/256/safari.png\"}},\"site_footer\":{\"layout\":[\"aplayer\",\"social\",\"license\",\"analytics\",\"info\",\"copyright\"],\"social\":[{\"icon\":null,\"url\":null},{\"img\":null,\"url\":null},{\"avatar\":null,\"url\":null}],\"source\":\"https://github.com/volantis-x/volantis-docs/\",\"analytics\":\"本站总访问量为 次 访客数为 人\\n\",\"copyright\":\"[Copyright © 2017-2020 XXX](/)\",\"br\":\"\"},\"plugins\":{\"jquery\":\"https://cdn.jsdelivr.net/npm/jquery@3.5/dist/jquery.min.js\",\"fontawesome\":\"https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14/css/all.min.css\",\"preload\":{\"enable\":true,\"service\":\"flying_pages\",\"instant_page\":\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@2/js/instant_page.js\",\"flying_pages\":\"https://cdn.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js\"},\"lazyload\":{\"enable\":true,\"js\":\"https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.1.0/dist/lazyload.min.js\",\"onlypost\":false,\"loadingImg\":null,\"blurIn\":true},\"highlightjs\":{\"enable\":null,\"js\":\"https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.1/build/highlight.min.js\",\"css\":\"https://cdn.jsdelivr.net/npm/highlight.js@9.18.1/styles/solarized-light.css\"},\"scrollreveal\":{\"enable\":null,\"js\":\"https://cdn.jsdelivr.net/npm/scrollreveal@4.0.6/dist/scrollreveal.min.js\",\"distance\":\"32px\",\"duration\":800,\"interval\":20,\"scale\":1},\"clipboard\":{\"enable\":null,\"js\":\"https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js\"},\"wordcount\":{\"enable\":null},\"nodewaves\":{\"enable\":null,\"css\":\"https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css\",\"js\":\"https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js\"},\"fontawesome_animation\":{\"enable\":null,\"css\":\"https://cdn.jsdelivr.net/gh/l-lin/font-awesome-animation/dist/font-awesome-animation.min.css\"},\"comment_typing\":{\"enable\":null,\"js\":\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@2/js/comment_typing.js\"},\"backstretch\":{\"enable\":null,\"js\":\"https://cdn.jsdelivr.net/npm/jquery-backstretch@2.1.18/jquery.backstretch.min.js\",\"position\":\"cover\",\"shuffle\":true,\"duration\":10000,\"fade\":1500,\"images\":[\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/001.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/002.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/003.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/004.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/005.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/006.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/012.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/016.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/019.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/025.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/033.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/034.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/035.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/038.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/039.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/042.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/046.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/051.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/052.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/054.jpg\",\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/056.jpg\"]},\"aplayer\":{\"enable\":null,\"js\":[\"https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js\",\"https://cdn.jsdelivr.net/npm/meting@2.0/dist/Meting.min.js\"],\"server\":\"netease\",\"type\":\"playlist\",\"id\":3175833810,\"fixed\":false,\"theme\":\"#1BCDFC\",\"autoplay\":false,\"order\":\"list\",\"loop\":\"all\",\"volume\":0.7,\"list_max_height\":\"320px\",\"list_folded\":true},\"pjax\":{\"enable\":true,\"timeout\":5000,\"cacheBust\":false,\"animation\":\"nprogress\",\"banUrl\":null},\"artitalk\":{\"appId\":null,\"appKey\":null},\"issues\":{\"enable\":null,\"js\":null},\"darkmodejs\":{\"enable\":null,\"js\":\"https://cdn.jsdelivr.net/npm/darkmode-js@1.5/lib/darkmode-js.min.js\",\"button\":{\"enable\":true,\"left\":\"16px\",\"bottom\":\"32px\",\"buttonDark\":\"#333\",\"buttonLight\":\"#ddd\"},\"compatible\":true}},\"rightmenu\":{\"enable\":null,\"layout\":[\"home\",\"help\",\"examples\",\"contributors\",\"hr\",\"source_docs\",\"source_theme\",\"hr\",\"print\",\"hr\",\"music\"],\"print\":{\"name\":\"打印页面\",\"icon\":\"fa fa-print\",\"onclick\":\"document.execCommand('print')\"},\"help\":{\"name\":\"常见问题\",\"icon\":\"fa fa-question\",\"url\":\"https://volantis.js.org/faqs/\"},\"examples\":{\"name\":\"示例博客\",\"icon\":\"fa fa-rss\",\"url\":\"https://volantis.js.org/examples/\"},\"contributors\":{\"name\":\"加入社区\",\"icon\":\"fa fa-fan fa-spin\",\"url\":\"https://volantis.js.org/contributors/\"},\"source_docs\":{\"name\":\"本站源码\",\"icon\":\"fa fa-code-branch\",\"url\":\"https://github.com/volantis-x/volantis-docs/\"},\"source_theme\":{\"name\":\"主题源码\",\"icon\":\"fa fa-code-branch\",\"url\":\"https://github.com/volantis-x/hexo-theme-volantis/\"}},\"search\":{\"enable\":true,\"service\":\"hexo\",\"js\":null,\"google\":{\"apiKey\":null,\"engineId\":null},\"algolia\":{\"applicationID\":null,\"apiKey\":null,\"indexName\":null},\"azure\":{\"serviceName\":null,\"indexName\":null,\"queryKey\":null},\"baidu\":{\"apiId\":null}},\"color_scheme\":{\"common\":{\"theme\":\"#44D7B6\",\"link\":\"#2196f3\",\"button\":\"#44D7B6\",\"hover\":\"#ff5722\",\"inner\":\"#fff\",\"selection\":\"alpha(#2196f3, 0.2)\"},\"light\":{\"site_bg\":\"#f4f4f4\",\"site_inner\":\"#fff\",\"site_footer\":\"#666\",\"card\":\"#fff\",\"text\":\"#444\",\"block\":\"#f6f6f6\",\"codeblock\":\"#FFF7EA\",\"inlinecode\":\"#D56D28\",\"h1\":\"#3a3a3a\",\"h2\":\"#3a3a3a\",\"h3\":\"#333\",\"h4\":\"#444\",\"h5\":\"#555\",\"h6\":\"#666\",\"p\":\"#444\",\"list\":\"#666\",\"list_hl\":\"mix($color-theme, #000, 80)\",\"meta\":\"#888\"},\"dark\":{\"site_bg\":\"#222\",\"site_inner\":\"#eee\",\"site_footer\":\"#aaa\",\"card\":\"#333\",\"text\":\"#eee\",\"block\":\"#3a3a3a\",\"codeblock\":\"#343a3c\",\"inlinecode\":\"#D56D28\",\"h1\":\"#eee\",\"h2\":\"#eee\",\"h3\":\"#ddd\",\"h4\":\"#ddd\",\"h5\":\"#ddd\",\"h6\":\"#ddd\",\"p\":\"#bbb\",\"list\":\"#aaa\",\"list_hl\":\"mix($color-theme, #fff, 80)\",\"meta\":\"#888\",\"brightness\":\"70%\"}},\"custom_css\":{\"cursor\":{\"enable\":null,\"text\":\"https://cdn.jsdelivr.net/gh/inkss/common@master/cursor/text.png\",\"pointer\":\"https://cdn.jsdelivr.net/gh/inkss/common@master/cursor/pointer.png\",\"default\":\"https://cdn.jsdelivr.net/gh/inkss/common@master/cursor/left_ptr.png\",\"not-allowed\":\"https://cdn.jsdelivr.net/gh/inkss/common@master/cursor/circle.png\",\"zoom-out\":\"https://cdn.jsdelivr.net/gh/inkss/common@master/cursor/zoom-out.png\",\"zoom-in\":\"https://cdn.jsdelivr.net/gh/inkss/common@master/cursor/zoom-in.png\",\"grab\":\"https://cdn.jsdelivr.net/gh/inkss/common@master/cursor/openhand.png\"},\"font_smoothing\":true,\"max_width\":\"1080px\",\"scrollbar\":{\"size\":\"4px\",\"border\":\"2px\"},\"navbar\":{\"height\":\"64px\",\"width\":\"auto\",\"effect\":[\"shadow\",\"blur\"]},\"sidebar\":{\"effect\":[\"shadow\"]},\"body\":{\"effect\":[\"shadow\"],\"highlight\":{\"language\":true,\"copy_btn\":true,\"grayscale\":false},\"text_align\":{\"h1\":\"left\",\"h2\":\"left\",\"h3\":\"left\",\"h4\":\"left\",\"p\":\"justify\"}},\"gap\":{\"h2\":\"48px\",\"h3\":\"24px\",\"h4\":\"16px\",\"p\":\"1em\",\"line_height\":1.6},\"border_radius\":{\"card\":\"8px\",\"codeblock\":\"4px\",\"searchbar\":\"8px\",\"button\":\"4px\"},\"fontsize\":{\"root\":\"16px\",\"h1\":\"1.5rem\",\"h2\":\"1.5rem\",\"h3\":\"1.25rem\",\"h4\":\"1.125rem\",\"h5\":\"1rem\",\"h6\":\"1rem\",\"list\":\".9375rem\",\"meta\":\".875rem\",\"code\":\".8125rem\",\"footnote\":\".78125rem\"},\"fontfamily\":{\"logofont\":{\"fontfamily\":\"\\\"Varela Round\\\", \\\"PingFang SC\\\", \\\"Microsoft YaHei\\\", Helvetica, Arial\",\"name\":\"Varela Round\",\"url\":\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-fonts/VarelaRound/VarelaRound-Regular.ttf\",\"weight\":\"normal\",\"style\":\"normal\"},\"bodyfont\":{\"fontfamily\":\"UbuntuMono, \\\"Varela Round\\\", \\\"PingFang SC\\\", \\\"Microsoft YaHei\\\", Helvetica, Arial\",\"name\":\"UbuntuMono\",\"url\":\"https://cdn.jsdelivr.net/gh/volantis-x/cdn-fonts/UbuntuMono/UbuntuMono-Regular.ttf\",\"weight\":\"normal\",\"style\":\"normal\"},\"codefont\":{\"fontfamily\":\"Menlo, UbuntuMono, Monaco\"}}},\"analytics\":{\"busuanzi\":null,\"leancloud\":{\"app_id\":\"v6cb35sRa1lfGq6EOtTQHHyf-gzGzoHsz\",\"app_key\":\"2ulfq63r4brxg2l9QfXp0cTM\",\"custom_api_server\":null}},\"seo\":{\"use_tags_as_keywords\":true,\"use_excerpt_as_description\":true,\"robots\":{\"home_first_page\":\"index,follow\",\"home_other_pages\":\"noindex,follow\",\"archive\":\"noindex,follow\",\"category\":\"noindex,follow\",\"tag\":\"noindex,follow\"}}}"},{"title":"我的朋友们","date":"2022-09-12T02:19:54.288Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"friends/index.html","permalink":"http://raymond-zhao.top/friends/index.html","excerpt":"","text":""},{"title":"","date":"2023-03-09T00:37:37.971Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"config/NexT.json","permalink":"http://raymond-zhao.top/config/NexT.json","excerpt":"","text":"{\"override\":false,\"reminder\":true,\"cache\":{\"enable\":true},\"minify\":false,\"custom_file_path\":null,\"favicon\":{\"small\":\"/images/avatar.jpg\",\"medium\":\"/images/avatar.jpg\",\"apple_touch_icon\":\"/images/avatar.jpg\",\"safari_pinned_tab\":\"/images/logo.svg\"},\"language_switcher\":true,\"footer\":{\"icon\":{\"name\":\"fa fa-heart\",\"animated\":true,\"color\":\"#ff0000\"},\"copyright\":null,\"powered\":true,\"beian\":{\"enable\":false,\"icp\":null,\"gongan_id\":null,\"gongan_num\":null,\"gongan_icon_url\":null}},\"creative_commons\":{\"license\":\"by-nc-sa\",\"sidebar\":false,\"post\":false,\"language\":null},\"scheme\":\"Gemini\",\"darkmode\":false,\"menu\":{\"home\":\"/ || fa fa-home\",\"about\":\"/about/ || fa fa-user\",\"tags\":\"/tags/ || fa fa-tags\",\"categories\":\"/categories/ || fa fa-th\",\"archives\":\"/archives/ || fa fa-archive\"},\"menu_settings\":{\"icons\":true,\"badges\":true},\"sidebar\":{\"position\":\"left\",\"Pisces | Gemini\":240,\"display\":\"post\",\"padding\":18,\"offset\":12,\"onmobile\":false},\"avatar\":{\"url\":\"/images/avatar.jpg\",\"rounded\":true,\"rotated\":false},\"site_state\":true,\"social\":{\"GitHub\":\"https://github.com/raymond-zhao || github\",\"E-Mail\":\"mailto:qq740567396@gmail.com || envelope\",\"Weibo\":\"https://weibo.com/5709671078 || weibo\",\"Twitter\":\"https://twitter.com/Raymondgalaxy || twitter\",\"Facebook\":\"https://www.facebook.com/ray.zhao.568 || facebook\",\"Instagram\":\"https://instagram.com/raymond__zhao || instagram\"},\"social_icons\":{\"enable\":true,\"icons_only\":false,\"transition\":true},\"links_settings\":{\"icon\":\"fa fa-link\",\"title\":\"友情链接\",\"layout\":\"inline\"},\"links\":{\"陈某人\":\"https://ramona-chen.top/\",\"SZYINK\":\"https://inkss.cn/\",\"思建的NLP之旅\":\"https://lisijian.cn/\"},\"toc\":{\"enable\":true,\"number\":false,\"wrap\":true,\"expand_all\":false,\"max_depth\":6},\"chat\":{\"enable\":true,\"service\":\"chatra\"},\"excerpt_description\":true,\"read_more_btn\":true,\"post_meta\":{\"item_text\":true,\"created_at\":true,\"updated_at\":{\"enable\":true,\"another_day\":true},\"categories\":true},\"symbols_count_time\":{\"separated_meta\":true,\"item_text_post\":true,\"item_text_total\":false},\"tag_icon\":true,\"reward_settings\":{\"enable\":false,\"animation\":false},\"reward\":null,\"follow_me\":null,\"related_posts\":{\"enable\":true,\"title\":null,\"display_in_home\":true,\"params\":{\"maxCount\":5,\"isExcerpt\":true}},\"post_edit\":{\"enable\":false},\"post_navigation\":\"left\",\"tagcloud\":{\"min\":12,\"max\":30,\"start\":\"#ccc\",\"end\":\"#111\",\"amount\":200},\"calendar\":{\"calendar_id\":\"\",\"api_key\":\"\",\"orderBy\":\"startTime\",\"offsetMax\":24,\"offsetMin\":4,\"showDeleted\":false,\"singleEvents\":true,\"maxResults\":250},\"text_align\":{\"desktop\":\"justify\",\"mobile\":\"justify\"},\"mobile_layout_economy\":false,\"android_chrome_color\":\"#222\",\"custom_logo\":null,\"codeblock\":{\"highlight_theme\":\"normal\",\"copy_button\":{\"enable\":true,\"show_result\":true,\"style\":\"mac\"}},\"back2top\":{\"enable\":true,\"sidebar\":false,\"scrollpercent\":true},\"reading_progress\":{\"enable\":false,\"position\":\"bottom\",\"color\":\"#37c6c0\",\"height\":\"3px\"},\"bookmark\":{\"enable\":true,\"color\":\"#222\",\"save\":\"auto\"},\"github_banner\":{\"enable\":true,\"permalink\":\"https://github.com/raymond-zhao\",\"title\":\"Follow me on GitHub\"},\"font\":{\"enable\":true,\"host\":null,\"global\":{\"external\":true,\"family\":null,\"size\":null},\"title\":{\"external\":true,\"family\":null,\"size\":null},\"headings\":{\"external\":true,\"family\":null,\"size\":null},\"posts\":{\"external\":true,\"family\":null},\"codes\":{\"external\":true,\"family\":\"Roboto Mono\"}},\"disable_baidu_transformation\":false,\"index_with_subtitle\":false,\"exturl\":true,\"google_site_verification\":null,\"bing_site_verification\":null,\"yandex_site_verification\":null,\"baidu_site_verification\":null,\"baidu_push\":true,\"math\":{\"per_page\":false,\"mathjax\":{\"enable\":true,\"mhchem\":false},\"katex\":{\"enable\":false,\"copy_tex\":false}},\"pjax\":true,\"fancybox\":false,\"mediumzoom\":true,\"lazyload\":true,\"pangu\":true,\"quicklink\":{\"enable\":false,\"home\":false,\"archive\":false,\"delay\":true,\"timeout\":3000,\"priority\":true,\"ignores\":null},\"comments\":{\"style\":\"tabs\",\"active\":\"valine\",\"storage\":true,\"lazyload\":true,\"nav\":null},\"disqus\":{\"enable\":false,\"shortname\":null,\"count\":true},\"disqusjs\":{\"enable\":false,\"api\":null,\"apikey\":null,\"shortname\":null},\"changyan\":{\"enable\":false,\"appid\":null,\"appkey\":null},\"valine\":{\"enable\":true,\"appid\":\"v6cb35sRa1lfGq6EOtTQHHyf-gzGzoHsz\",\"appkey\":\"2ulfq63r4brxg2l9QfXp0cTM\",\"notify\":false,\"verify\":false,\"placeholder\":\"快来评论吧\",\"avatar\":\"mm\",\"guest_info\":\"nick,mail,link\",\"pageSize\":10,\"language\":null,\"visitor\":false,\"comment_count\":false,\"recordIP\":true,\"serverURLs\":null},\"livere_uid\":null,\"gitalk\":{\"enable\":false,\"github_id\":\"raymond-zhao\",\"repo\":\"raymond-zhao.github.io\",\"client_id\":\"084b51df47b170666ed5\",\"client_secret\":\"d2dcc92a5309b1ab03cdf67ee3bebb98bc83ca21\",\"admin_user\":\"raymond-zhao\",\"distraction_free_mode\":true,\"language\":null},\"rating\":{\"enable\":false,\"id\":null,\"color\":\"fc6423\"},\"google_analytics\":{\"tracking_id\":\"UA-153445863-1\",\"only_pageview\":false},\"baidu_analytics\":\"889b45174b1bfd8b974e0944a5c47850\",\"growingio_analytics\":null,\"cnzz_siteid\":null,\"leancloud_visitors\":{\"enable\":false,\"appid\":\"v6cb35sRa1lfGq6EOtTQHHyf-gzGzoHsz\",\"appkey\":\"2ulfq63r4brxg2l9QfXp0cTM\",\"server_url\":null,\"security\":false},\"firestore\":{\"enable\":false,\"collection\":\"articles\",\"apiKey\":null,\"projectId\":null},\"busuanzi_count\":{\"enable\":true,\"total_visitors\":true,\"total_visitors_icon\":\"fa fa-user\",\"total_views\":true,\"total_views_icon\":\"fa fa-eye\",\"post_views\":true,\"post_views_icon\":\"fa fa-eye\"},\"algolia_search\":{\"enable\":false,\"hits\":{\"per_page\":10},\"labels\":{\"input_placeholder\":\"Search for Posts\",\"hits_empty\":\"We didn't find any results for the search: ${query}\",\"hits_stats\":\"${hits} results found in ${time} ms\"}},\"local_search\":{\"enable\":true,\"trigger\":\"auto\",\"top_n_per_article\":1,\"unescape\":false,\"preload\":false},\"swiftype_key\":null,\"chatra\":{\"enable\":true,\"async\":true,\"id\":\"WW9yp8QpPcAKpJMYy\"},\"tidio\":{\"enable\":false,\"key\":null},\"note\":{\"style\":\"flat\",\"icons\":false,\"light_bg_offset\":0},\"tabs\":{\"transition\":{\"tabs\":false,\"labels\":true}},\"pdf\":{\"enable\":false,\"height\":\"500px\"},\"mermaid\":{\"enable\":false,\"theme\":\"forest\"},\"motion\":{\"enable\":true,\"async\":false,\"transition\":{\"post_block\":\"fadeIn\",\"post_header\":\"slideDownIn\",\"post_body\":\"slideDownIn\",\"coll_header\":\"slideLeftIn\",\"sidebar\":\"slideUpIn\"}},\"pace\":{\"enable\":true,\"theme\":\"minimal\"},\"three\":{\"enable\":false,\"three_waves\":false,\"canvas_lines\":false,\"canvas_sphere\":false},\"canvas_ribbon\":{\"enable\":false,\"size\":300,\"alpha\":0.6,\"zIndex\":-1},\"vendors\":{\"_internal\":\"lib\",\"anime\":\"//cdn.jsdelivr.net/npm/animejs@3.1.0/lib/anime.min.js\",\"fontawesome\":\"//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css\",\"mathjax\":\"//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\",\"katex\":null,\"copy_tex_js\":null,\"copy_tex_css\":null,\"pjax\":\"//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js\",\"jquery\":null,\"mediumzoom\":\"//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js\",\"lazyload\":\"//cdnjs.cloudflare.com/ajax/libs/lozad.js/1.14.0/lozad.min.js\",\"pangu\":\"//cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js\",\"quicklink\":null,\"disqusjs_js\":null,\"disqusjs_css\":null,\"valine\":\"//cdnjs.cloudflare.com/ajax/libs/valine/1.3.10/Valine.min.js\",\"gitalk_js\":null,\"gitalk_css\":null,\"algolia_search\":null,\"instant_search\":null,\"mermaid\":null,\"velocity\":\"//cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.min.js\",\"velocity_ui\":\"//cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.ui.min.js\",\"pace\":\"//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js\",\"pace_css\":\"//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/themes/blue/pace-theme-minimal.min.css\",\"three\":null,\"three_waves\":null,\"canvas_lines\":null,\"canvas_sphere\":null,\"canvas_ribbon\":null},\"css\":\"css\",\"js\":\"js\",\"images\":\"images\"}"},{"title":"","date":"2022-09-12T02:19:57.935Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"mylist/index.html","permalink":"http://raymond-zhao.top/mylist/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-03-21T02:15:34.000Z","updated":"2021-06-20T05:59:48.000Z","comments":false,"path":"tags/index.html","permalink":"http://raymond-zhao.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Spring 核心技术 - IoC 容器","slug":"2023-03-10-Spring-Core-Technologies-IoC-Container","date":"2023-03-10T02:01:13.000Z","updated":"2023-03-14T14:57:35.000Z","comments":true,"path":"2023/03/10/2023-03-10-Spring-Core-Technologies-IoC-Container/","link":"","permalink":"http://raymond-zhao.top/2023/03/10/2023-03-10-Spring-Core-Technologies-IoC-Container/","excerpt":"","text":"Spring IoC 容器与 Bean 简介 It is a process whereby objects define their dependencies (that is, the other objects they work with) only through constructor arguments, arguments to a factory method, or properties that are set on the object instance after it is constructed or returned from a factory method. The container then injects those dependencies when it creates the bean. - Spring Documentation IoC 也被称为 DI，它表示了这样一种过程：对象通过且仅通过构造器参数/将参数传递给工厂方法来定义/定位其所需依赖，或者手动实例化对象/从工厂获得实例之后再设置所需属性。 org.springframework.beans 与 org.springframework.context 包是 Spring 框架中 IoC 容器的基础，BeanFactory 接口提供了一种可以管理任意类型的对象的配置机制，ApplicationContext 是 BeanFactory 的一个子接口，它添加了以下一些特性： 简便地与 Spring AOP 的集成 消息资源处理（用于国际化） 事件发布 应用层面的特定上下文，如 Web 应用中的 WebApplicationContext 简而言之，BeanFactory 提供了配置框架和基本功能，ApplicationContext 添加了一些企业级的功能。ApplicationContext 是 BeanFactory 的完全超集并不仅被用于在本章描述 Spring IoC 容器，如需了解 BeanFactory 而非 ApplicationContet 的更多使用，可以参考 BeanFactory API。 在 Spring 中，组成应用的轴心骨 - 被 Spring IoC 容器所管理的对象被称为 Bean，Bean 的含义是“被 Spring IoC 容器初始化、组装、管理的对象”，Bean 也是应用程序中的众多对象之一。Bean 以及 Bean 所需的依赖，被容器作为配置元数据反射调用。 容器概览org.springframework.context.ApplicationContext 接口代表着 Spring 的 IoC 容器，并且负责初始化/实例化、配置、组装 Bean，IoC 通过读取配置元数据来决定初始化/配置/组装什么对象，配置元数据通常以XML、Java 注解、或 Java 代码的方式出现，配置元数据使开发者可以表达对应用所需对象的组合方式以及对象之间的内联依赖/关系。 Spring 提供了众多 ApplicationContext 接口的实现，在单机应用中，通常是创建 ClassPathXmlApplicationContext 或 FileSystemXmlApplicationContext。尽管 XML 时传统的定义配置元数据的方式，开发者也可以指示容器使用 Java 注解或代码作为配置元数据，但是也需要使用 XML 进行一些简单的配置以表明需要支持这些扩展的数据格式。 在大多数的应用场景中，并不需要显式地用户代码来初始化一个/多个 Spring IoC 容器的实例，如在 Web 应用中，通过一个 web.xml 就可以声明一个 Spring IoC 容器。 Spring IoC 容器的基本工作方式： 输入：业务对象（POJOs）+ 配置元数据（Configuration Metadata） 处理：Spring IoC 容器 ApplicationContext 处理输入数据 输出：开箱即用/完全配置的应用系统 配置元数据如前文/上图所述，Spring IoC 容器接受某种类型的配置元数据，这些元数据指示 Spring 容器初始化/配置/组装应用对象。 配置元数据通常为 XML 格式，也是 Spring 官方文档用来讲解 Spring IoC 容器关键特性与概念的数据格式。 Note: Spring IoC 容器与 XML 是完全解耦合的，目前 Java 配置才是主流方式。 关于如何使用 XML 格式之外的其他数据格式来配置元数据，可以参考： 基于注解的配置 基于 Java 的配置：如 @Configuration，@Bean，@Import，@DependOn 注解。 Spring 配置包含至少一个（通常超过一个）容器必须管理的 Bean，XML 配置元数据使用 &lt;bean/&gt; 配置 Bean，并内嵌在 &lt;beans/&gt; 之内，Java 代码则使用 @Bean 注解标注在被 @Configuration 注解标注的 Class 中的方法上。 Bean 的定义与组成应用的真是对象是互相对应的，通常，开发者定义服务层对象，持久层对象 repositories 或 DAOs，展示层对象 Web Controller，底层对象如 JPA EntityManageFactory，JMS 队列等。通常来说，并不需要在容器中定义精心雕琢过的领域对象，因为创建和加载领域对象是 repositories 和业务逻辑的责任。 1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean id=\"...\" class=\"...\"&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;!-- id 可被合作对象引用 --&gt; &lt;property name=\"...\" ref=\"...\"&gt; &lt;property name=\"...\" ref=\"...\"&gt; &lt;/bean&gt; &lt;bean id=\"...\" class=\"...\"&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;!-- more bean definitions go here --&gt;&lt;/beans&gt; 在这面这个例子中，property name 指向 JavaBean 属性，ref 指向另一个 Bean 的定义，id 与 ref 之间的联系表现了协作对象之间的联系。 初始化容器传递给 ApplicationContext 构造器的路径会引导容器从外部资源加载配置元数据，如本地文件系统、Java 类路径等。 1ApplicationContext context = new ClassPathXmlApplication(\"service.xml\", \"daos.xml); Note: 在了解了 Spring IoC 容器之后，也许还需要了解 Spring 的抽象资源 Resource，Resource 提供了便捷的从路径中读取以 URI 语法定义的输入流机制。特别的，Resource 路径被用于构造容器的上下文，详情可参考 Application Contexts and Resource Paths。 编写基于 XML 配置元数据将 Bean 的定义分散在多个不同的 XML 在某些场景下是非常有用的，每一个独立的 XML 配置文件代表了应用架构中的某一个逻辑层。 可以使用应用上下文构造器加载多个不同 XML 片段中的 Bean 定义，这个构造器接受一个或多个 Resource 路径，也可以在 XML 文件中使用 &lt;import/&gt; 标签加载其他文件中的 Bean 定义，以下是一个例子。 123456789&lt;beans&gt; &lt;import resource=\"services.xml\"/&gt; &lt;!-- 虽然也支持 classpath 与绝对路径，但尽量不要这么做。--&gt; &lt;import resource=\"resources/messageSource.xml\"/&gt; &lt;import resource=\"/resources/themeSource.xml\"/&gt; &lt;bean id=\"bean1\" class=\"...\"/&gt; &lt;bean id=\"bean2\" class=\"...\"/&gt;&lt;/beans&gt; 使用 IoC 容器ApplicationContext 是管理不同 Bean 注册及其依赖的高级接口，通过 T getBean(String name, Class&lt;T&gt; requiredType) 方法可以获取 Bean 的实例。 12345678// 创建并配置 BeanApplicationContext context = new ClassPathXmlApplication(\"services.xml\", \"daos.xml\");// 获取配置好的实例PetStoreService service = context.getBean(\"petStore\", PetStoreService.class);// 使用配置好的实例List&lt;String&gt; userList = service.getUsernameList(); ApplicationContext 接口也有一些除 getBean(...) 之外的获取 Bean 实例的方法，但是在应用程序中，一般来说，除非有对 Spring API 的直接调用，否则不应该出现对 getBean 的显式使用。 Bean 简介Spring IoC 容器管理一个或多个 Bean，这些 Bean 通过通过配置元数据被加载到容器之中。在容器内部，这些 Bean 的定义由 BeanDefinition 对象表示，包含以下一些属性： 包限定类名：通常是某个 Bean 的具体实现类 Bean 的行为配置元素，表明了 Bean 应该在 Container 的状态与行为，如作用域、生命周期回调等。 完成当前 Bean 所需功能而对其他 Bean 的引用，这些引用也被称为依赖/合作对象。 其他需要为新建对象设置的属性，如一个管理连接池的 Bean 所支持的连接数量以及连接池的大小。 上面这些元数据表明了组成每个 Bean 定义的基本属性，下表则描述了这些属性： 属性 链接 Class Bean 的初始化 Name Bean 的命名 Scope Bean 的作用域 Constructor arguments 依赖注入 Properties 依赖注入 Autowiring mode 自动装配协作器 Lazy initialization mode Bean 的懒初始化 Initialization method 初始化回调 Destruction method 销毁回调 除去创建某个包含特定 Bean 定义的 Bean 之外，ApplicationContext 的实现也允许某些用户自建对象注册到容器之中，可以通过BeanFactory 的 getBeanFactory() 方法实现，这个方法返回了 DefaultListableBeanFactory。DefaultListableBeanFactory 支持通过 registerSingleton(..) 与 registerBeanDefinition(..) 方法注册 Bean。 Bean 的元数据与手动注册的 Bean 实例需要尽早注册，以便于容器在自动装配及解析时可以正确使用。尽管容器在某种情况下支持重写已存在的元数据和单例实例，但这并不包括在运行时注册 Bean，这可能会导致并发访问，Bean 数据不一致性等问题。 Bean 的命名每个 Bean 在容器内可以拥有一个或多个具备唯一性的标识符，但通常来说每个 Bean 只有一个标识符。然而，如果需要有超过一个的标识符，这些标识符可以以“别名（alias）”的形式存在。 在基于 XML 的配置元数据中，使用 id 和 name 属性作为 Bean 标识符，如果 name 属性有多个值存在时，可以使用逗号 ,，分号 ; 或空格分隔。 Bean 的 id 和 name 属性并不是必须提供的，如果缺省，容器会为 Bean 创建一个唯一的名字，然而，如果需要使用 ref 属性应用一个 Bean，那就必须提供 name。 关于为什么可以不提供 name，可以参考 inner beans 和 autowiring collaborators。 Bean 一般使用驼峰命名法，并有利于 Spring AOP 应用 Advice。 Note 在扫描类路径中的组件时，Spring 会为未未名的组件生成 Bean 名，规则是：将类的 simpleName 的首字母转为小写。 然而，在一些特殊的场景中，如包含不止一个字符的命名中，首字母和第二个字符均为大写时，则会保留原始的命名，此规则定义在 java.beans.Introspector.decapitalize。 为 Bean 起别名的例子： 123456&lt;!-- fromName 与 toName 均指向同一个 Bean --&gt;&lt;alias name=\"fromName\" alias=\"toName\"/&gt;&lt;!-- 以下两个语句中的三个不同的名字，都指向同一个 Bean --&gt;&lt;alias name=\"myApp-dataSource\" alias=\"subsystemA-dataSource\"/&gt;&lt;alias name=\"myApp-dataSource\" alias=\"subsystemB-dataSource\"/&gt; 另外，也可以使用 Java 代码在 @Bean 注解中添加 alias 属性，可参考 Using the @Bean Annotation。 初始化 BeanBean 的定义本质上是用来创建一个或多个对象的清单/目录/指示，当容器被要求使用某个配置元数据时，容器就在这个清单中查找某个名字的 Bean 定义，并创建出实际的对象。 在基于 XML 的配置元数据中，通过 &lt;bean/&gt; 元素的 class 属性指定要初始化的对象类型通常是必须的（实际上是 BeanDefinition 实例的 Class 属性），但也有例外，可参考 使用实例工厂方法进行实例化 与 Bean 定义与继承，Class 属性有以下两种使用方式： 通常，容器指定要构建的 Bean 的类型，直接通过反射调用 Bean 的构造器直接创建 Bean，等价于 Java 代码的 new 操作符。 在包含用于创建对象的静态工厂方法的实际类中，容器直接调用类中的静态工厂方法创建 Bean 这种操作并不常见，通过静态工厂方法返回的对象的类型可能属于同一类型，但也可能完全不同。 内部类名称如果需要为内部类配置 Bean 定义，要么使用二进制名，要么使用嵌套类的实际名。 例如，假设在 com.example 包下存在一个名为 SomeThing 的类，而且 SomeThing 类有一个静态内部类 OtherThing，可以使用 $ 或 . 来分割外部类与内部类。所以 Bean 定义中的 class 属性为 com.example.SomeThing$OtherThing 或 com.example.SomeThing.OtherThing。 使用构造器实例化当使用构造器创建 Bean 时，所有的普通类对 Spring 来说都是可用的并且可兼容的，简单来说就是开发出的类并不需要实现任何接口或者按照特定的风格编码，只需要指定 Bean 的类即可，然而对于某个 Bean 而言，可能需要根据 IoC 的类型指定一个无参构造器。 Spring IoC 容器可以虚拟地管理你想要管理的任何类，并不仅限于 JavaBean，但是大多数的 Spring 用户偏向于只拥有一个无参构造器以及若干 getters/setters 的 JavaBean。 更多关于构造器初始化的内容，可参考 Injecting Dependenies 使用静态工厂方法实例化使用实例工厂方法实例化确定 Bean 的运行时类型","categories":[{"name":"Spring","slug":"Spring","permalink":"http://raymond-zhao.top/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://raymond-zhao.top/tags/Spring/"},{"name":"IoC","slug":"IoC","permalink":"http://raymond-zhao.top/tags/IoC/"}]},{"title":"《Kafka 权威指南》：深入 Kafka","slug":"2021-04-26-KafkaChapter5","date":"2021-04-26T05:54:24.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2021/04/26/2021-04-26-KafkaChapter5/","link":"","permalink":"http://raymond-zhao.top/2021/04/26/2021-04-26-KafkaChapter5/","excerpt":"","text":"5.1 集群成员关系Kafka 使用 Zookeeper 来维护集群成员的信息。每个 broker 都有一个唯一标识符，这个标识符可以在配置文件里指定，也可以自动生成。在 broker 启动的时候，它通过创建临时节点把自己的 ID 注册到 Zookeeper。Kafka 组件订阅 Zookeeper 的 /brokers/ids 路径，当有 broker 加入集群或退出集群时，这些组件就可以获得通知。 在 broker 停机、出现网络分区或长时间垃圾回收停顿时，broker 会从 Zookeeper 上断开连接，此时 broker 在启动时创建的临时节点会自动从 Zookeeper 上移除。监听 broker 列表的 Kafka 组件会被告知该 broker 已移除。 在关闭 broker 时，它对应的节点也会消失，不过它的 ID 会继续存在于其他数据结构中。 例如，主题的副本列表里就可能包含这些 ID。在完全关闭一个 broker 之后，如果使用相同的 ID 启动另一个全新的 broker，它会立即加入集群，并拥有与旧 broker 相同的分区和主题。 5.2 控制器控制器其实就是一个 broker，只不过它除了具有一般 broker 的功能之外，还负责分区首领的选举。集群里第一个启动的 broker 通过在 Zookeeper 里创建一个临时节点 /controller 让自己成为控制器。其他 broker 在启动时也会尝试创建这个节点，不过它们会收到一个“节点已存在”的异常，然后“意识”到控制器节点已存在。其他 broker 在控制器节点上创建 Zookeeper watch 对象，这样它们就可以收到这个节点的变更通知。这种方式可以确保集群里一次只有一个控制器存在。 如果控制器被关闭或者与 Zookeeper 断开连接，Zookeeper 上的临时节点就会消失。集群里的其他 broker 通过 watch 对象得到控制器节点消失的通知，它们会尝试让自己成为新的控制器。第一个在 Zookeeper 里成功创建控制器节点的 broker 就会成为新的控制器，其他节点会收到“节点已存在”的异常，然后在新的控制器节点上再次创建 watch 对象。每个新选出的控制器通过 Zookeeper 的条件递增操作获得一个全新的、数值更大的 controller epoch。其他 broker 在知道当前 controller epoch 后，如果收到由控制器发出的包含较旧 epoch 的消息，就会忽略它们。 当控制器发现一个 broker 已经离开集群（通过观察相关的 Zookeeper 路径），它就知道，那些失去首领的分区需要一个新首领。控制器遍历这些分区，并确定谁应该成为新首领， 然后向所有包含新首领或现有跟随者的 broker 发送请求。该请求消息包含了谁是新首领以及谁是分区跟随者的信息。随后，新首领开始处理来自生产者和消费者的请求，而跟随者开始从新首领那里复制消息。 当控制器发现一个 broker 加入集群时，它会使用 broker ID 来检查新加入的 broker 是否包含现有分区的副本。如果有，控制器就把变更通知发送给新加入的 broker 和其他 broker， 新 broker 上的副本开始从首领那里复制消息。 简而言之，Kafka 使用 Zookeeper 的临时节点来选举控制器，并在节点加入集群或退出集群时通知控制器。控制器负责在节点加入或离开集群时进行分区首领选举。控制器使用 epoch 来避免“脑裂”。“脑裂”是指两个节点同时认为自己是当前的控制器。 5.3 复制Kafka 使用主题来组织数据，每个主题被分为若干个分区，每个分区有多个副本。那些副本被保存在 broker 上，每个 broker 可以保存成百上千个属于不同主题和分区的副本。 副本有首领副本（Leader replica）与跟随者副本（Follower replica）两种类型。 首领副本：每个分区都有一个首领副本。为了保证一致性，所有生产者请求和消费者请求都会经过这个副本。 跟随者副本：首领以外的副本都是跟随者副本。跟随者副本不处理来自客户端的请求，它们唯一的任务就是从首领那里复制消息，保持与首领一致的状态。如果首领发生崩溃，其中的一个跟随者会被临危受命。 首领的另一个任务是搞清楚哪个跟随者的状态与自己是一致的。跟随者为了保持与首领的状态一致，在有新消息到达时尝试从首领那里复制消息，不过有各种原因会导致同步失败。 为了与首领保持同步，跟随者向首领发送获取数据的请求，这种请求与消费者为了读取消息而发送的请求是一样的。首领将响应消息发给跟随者。请求消息里包含了跟随者想要获取消息的偏移量，而且这些偏移量总是有序的。 一个跟随者副本先请求消息 1，接着请求消息 2，然后请求消息 3，在收到这 3 个请求的响应之前，它是不会发送第 4 个请求消息的。如果跟随者发送了请求消息 4，那么首领就知道它已经收到了前面 3 个请求的响应。通过查看每个跟随者请求的最新偏移量，首领就会知道每个跟随者复制的进度。如果跟随者在 10s 内没有请求任何消息，或者虽然在请求消息，但在 10s 内没有请求最新的数据，那么它就会被认为是不同步的。如果一个副本无法与首领保持一致，在首领发生失效时，它就不可能成为新首领—毕竟它没有包含全部的消息。 相反，持续请求得到的最新消息副本被称为同步的副本。在首领发生失效时，只有同步副本才有可能被选为新首领。 跟随者的正常不活跃时间或在成为不同步副本之前的时间是通过 replica.lag.time.max.ms 参数来配置的。这个时间间隔直接影响着首领选举期间的客户端行为和数据保留机制。 除了当前首领之外，每个分区都有一个首选首领—创建主题时选定的首领就是分区的首选首领。之所以把它叫作首选首领，是因为在创建分区时，需要在 broker 之间均衡首领。因此，我们希望首选首领在成为真正的首领时，broker 间的负载最终会得到均衡。默认情况下，Kafka 的 auto.leader.rebalance. enable 被设为 true，它会检查首选首领是不是当前首领，如果不是，并且该副本是同步的，那么就会触发首领选举，让首选首领成为当前首领。 找到首选首领 通过分区的副本清单，清单里的第一个副本一般就是首选首领。 5.4 处理请求broker 的大部分工作是处理客户端、分区副本和控制器发送给分区首领的请求。Kafka 提供了一个二进制协议(基于 TCP)，指定了请求消息的格式以及 broker 如何对请求作出响应—包括成功处理请求或在处理请求过程中遇到错误。客户端发起连接并发送请求， broker 处理请求并作出响应。broker 按照请求到达的顺序来处理它们—这种顺序保证让 Kafka 具有了消息队列的特性，同时保证保存的消息也是有序的。 所有的请求消息都包含一个标准消息头： Request type（也就是 API key） Request version（broker 可以处理不同版本的客户端请求，并根据客户端版本作出不同的响应） Correlation ID—一个具有唯一性的数字，用于标识请求消息，同时也会出现在响应消息和错误日志里 Client ID—用于标识发送请求的客户端 broker 会在它所监听的每一个端口上运行一个 Acceptor 线程，这个线程会创建一个连接， 并把它交给 Processor 线程去处理。Processor 线程的数量是可配置的。网络线程负责从客户端获取请求消息，把它们放进请求队列，然后从响应队列获取响应消息，把它们发送给客户端。下图为 Kafka 处理请求的内部流程。 请求消息被放到请求队列后，IO 线程会负责处理它们。下面是几种最常见的请求类型。 生产请求：生产者发送的请求，它包含客户端要写入 broker 的消息。与 acks=0/1/all 参数密切相关。 获取请求：在消费者和跟随者副本需要从 broker 读取消息时发送的请求。可以好好看看书。 元数据请求：包含了客户端感兴趣的主题列表。服务器的响应消息里知名了这些主题所包含的分区、每个分区有哪些副本，以及哪个副本是首领。 其他请求：基于 Kafka 协议的其他请求。 生产请求和获取请求都必须发送给分区的首领副本。如果 broker 收到一个针对特定分区的请求，而该分区的首领在另一个 broker 上，那么发送请求的客户端会收到一个“非分区首领”的错误响应。当针对特定分区的获取请求被发送到一个不含有该分区首领的 broker 上，也会出现同样的错误。Kafka 客户端要自己负责把生产请求和获取请求发送到正确的 broker 上。 那么客户端怎么知道该往哪里发送请求呢？客户端使用了另一种请求类型，也就是元数据请求。这种请求包含了客户端感兴趣的主题列表。服务器端的响应消息里指明了这些主题所包含的分区、每个分区都有哪些副本，以及哪个副本是首领。元数据请求可以发送给任意一个 broker，因为所有 broker 都缓存了这些信息。 一般情况下，客户端会把这些信息缓存起来，并直接往目标 broker 上发送生产请求和获取请求。它们需要时不时地通过发送元数据请求来刷新这些信息，从而知道元数据是否发生了变更。比如，在新 broker 加入集群时，部分副本会被移动到新的 broker 上如下图所示。另外，如果客户端收到“非首领”错误，它会在尝试重发请求之前先刷新元数据，因为这个错误说明了客户端正在使用过期的元数据信息，之前的请求被发到了错误的 broker 上。 5.5 物理存储Kafka 的基本存储单元是分区。分区无法在多个 broker 间进行再细分，也无法在同一个 broker 的多个磁盘上进行再细分。log.dirs 配置的是存储分区的目录清单，而不要将其认为是系统的错误日志清单，错误日志清单的配置在 log4j.properties 中。 在物理存储中涉及以下几个主要问题。 数据是如何被分配到集群的 broker 上以及 broker 的目录里的 broker 是如何管理这些文件的，也别是如何进行数据保留的。 文件和索引的格式是怎么样的 Kafka 的日志压缩及其工作原理","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://raymond-zhao.top/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://raymond-zhao.top/tags/Kafka/"}]},{"title":"《Kafka 权威指南》：Kafka 消费者","slug":"2021-04-24-KafkaChapter4","date":"2021-04-24T06:24:08.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2021/04/24/2021-04-24-KafkaChapter4/","link":"","permalink":"http://raymond-zhao.top/2021/04/24/2021-04-24-KafkaChapter4/","excerpt":"","text":"4.1 KafkaConsumer 概念4.1.1 消费者和消费者群组Kafka 消费者从属于消费者群组。一个群组里的消费者订阅的是同一个主题，每个消费者接收主题一部分分区的消息。这样可以协调消费者与生产者之间收发消息的不同速率以及 Broker 存储消息的能力。 一个消费者可以接收多个主题分区的消息，但是如下图所示，如果消费者的数量超过主题中分区的数量，那么多余的消费者就会被闲置，不会接收到任何消息。 ![5个消费者受到4个分区的消息](/Users/raymond/Library/Application Support/typora-user-images/image-20210425102753887.png) 往群组里增加消费者是横向伸缩消费能力的主要方式，有必要为主题创建大量的分区，在负载增长时可以加入更多的消费者，但是消费者的数量不要超过主题分区的数量。 除横向收缩外，也可以使用多个应用程序从同一个主题读取数据，只要保证每个应用程序都有自己的消费者群组，就可以读取到主题内所有的消息。如下图所示，如果增加 Consumer Group 2，它也不会影响 Consumer Group 1 接收消息主题全部分区的消息。 4.1.2 消费者群组和分区再均衡一个新的消费者加入群组时，它读取的是原本由其他消费者读取的消息。当一个消费者被关闭或发生崩溃 时，它就离开群组，原本由它读取的分区将由群组里的其他消费者来读取。分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为再均衡（rebalance），它提供了高可用性和伸缩性。 在再均衡期间，消费者无法读取消息，造成整个群组一小段时间的不可用。另外，当分区被重新分配给另一个消费者时，消费者当前的读取状态会丢失，它有可能还需要去刷新缓存，在它重新恢复状态之前会拖慢应用程序。 消费者通过向被指派为群组协调器的 broker（不同的群组可以有不同的协调器）发送心跳来维持它们和群组的从属关系，以及它们对分区的所有权关系。只要消费者以正常的时间间隔发送心跳就被认为是活跃的，说明它还在读取分区里的消息。消费者会在轮询消息或提交偏移量时发送心跳。如果消费者停止发送心跳的时间足够长，会话就会过期，群组协调器认为它已经死亡，就会触发一次再均衡。 如果一个消费者发生崩溃并停止读取消息，群组协调器会等待几秒钟，确认它死亡了才 会触发再均衡。在这几秒钟时间里，死掉的消费者不会读取分区里的消息。在清理消费者时，消费者会通知协调器它将要离开群组，协调器会立即触发一次再均衡，尽量降低处理停顿。 kafka 0.10.1 在版本引入了一个独立的心跳进程，可以在轮询消息的空档发送心跳，使得发送心跳的频率与消息轮询的频率相互独立。在新版本的 Kafka 里，可以指定消费者在离开群组并触发再均衡之前可以有多长时间不进行消息轮询，这样可以避免出现活锁（livelock）。 分配分区是怎样的一个过程？ 消费者要加入群组时，先向群组协调器（group coordinator）发送 JoinGroup 请求。第一个加入群组的消费者将成为群主（leader）。 群主从协调器获得群组成员列表（包含所有最近发送过心跳的消费者），并为每一个消费者分配分区。群主实现了 PartitionAssignor 接口来决定哪些分区应该被分配给哪个消费者。 分配完毕后，群主将分配情况列表发送给群组协调器，协调器再把这些消息发送给所有消费者。每个消费者只能看到自己的分配信息，只有群主知道所有消费者的分配信息。 （再均衡时自然也会发生再分区） Kafka 的分区分配策略： org.apache.kafka.clients.consumer.RangeAssignor（默认） org.apache.kafka.clients.consumer.RoundRobinAssignor org.apache.kafka.clients.consumer.StickyAssignor org.apache.kafka.clients.consumer.CooperativeStickyAssignor 也可以通过实现 org.apache.kafka.clients.consumer.ConsumerPartitionAssignor 接口自定义分区策略。 4.2 创建 Kafka 消费者创建 KafkaConsumer 与创建 KafkaProducer 过程极为相似，需要提供 3 个必要的属性：bootstrap.servers、key.deserializer、value.deserializer。下面是 Kafka 0.10.1 中创建消费者的示例代码。 12345678910111213141516private final KafkaConsumer&lt;Integer, String&gt; consumer;private final String topic;public Consumer(String topic) &#123; Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\"); props.put(ConsumerConfig.GROUP_ID_CONFIG, \"DemoConsumer\"); props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"true\"); props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, \"1000\"); props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, \"30000\"); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.IntegerDeserializer\"); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\"); consumer = new KafkaConsumer&lt;&gt;(props); this.topic = topic;&#125; 4.3 订阅主题消费者的 subscribe() 方法接受一个主题列表作为参数。 1consumeer.subscribe(Collections.singletonList(\"customerCountries\")); 也可以在调用 subscribe 方法时传入正则表达式，用以匹配多个主题。如果创建了新主题，并且主题名与正则表达式相匹配，将会立即触发一次再均衡。要订阅所有与 test 相关的主题，可以 1consumer.subscribe(\"test.*\"); 4.4 轮询(Poll Loop)消息轮询是消费者的核心 API，一旦消费者订阅了主题，轮询就会处理所有的细节，包括群组协调、分区再均衡、发送心跳和获取数据。消费者代码主要部分如下： 12345678910111213141516171819202122try &#123; while (true) &#123; // 消费者长期运行，持续轮询。 // poll() 的参数是超时时间，它会在指定的毫秒数内一直等待 broker 返回数据。 ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; log.debug(\"topic = %s, partition = %s, offset = %d, consumer = %s, country = %s\\n\", record.topic(), record.partition(), record.offset(), record.key(), record.value()); int updateCount = 1; if (custCountryMap.countainsValue(record.value())) &#123; updateCount = custCountryMap.get(record.value()) + 1; &#125; custCountryMap.put(record.value(), updateCount); JSONObject json = new JSONObject(custCountryMap); System.out.println(json.toString(4)); &#125; &#125;&#125; finally &#123; // 关闭后，网络连接和 Socket 也会随之关闭，并立即触发一次 再均衡， // 而不是等待群组协调器发现它不再发送发送心跳并认定它已死亡，节省了时间。 consumer.close();&#125; 轮询不只是获取数据那么简单，在第一次调用新消费者的 poll() 方法时，它会负责查找 GroupCoordinator，然后加入群组，接受分配的分区。如果发生了再均衡，整个过程也是在轮询期间进行的。当然，心跳也是从轮询里发送出去的。所以，要确保在轮询期间所做的任何处理工作都应该尽快完成。 线程安全 在同一个群组里，无法让一个线程运行多个消费者，也无法让多个线程安全地共享一个消费者。一个消费者应该单独使用一个线程，如果要在同一个消费者群组里运行多个消费者，需要让每个消费者运行在自己的线程里。最好是把消费者的逻辑封装在自己的对象里，然后使用 Java 的 ExecutorService 启动多个线程，使每个消费者运行在自己的线程上。 4.5 消费者的配置关于消费者的详细配置可参考 Apache Kafka Consumer Configuration，不再赘述。 4.6 提交和偏移量每次调用 poll() 方法，它总是返回由生产者写入 Kafka 但还没有被消费者读取过的记录， 因此可以追踪到哪些记录是被群组里的哪个消费者读取的。Kafka 不会像其他 JMS 队列那样需要得到消费者的确认，相反，消费者可以使用 Kafka 来追踪消息在分区里的位置（偏移量）。 更新分区当前位置的操作被叫做提交。 消费者往一个叫作 _consumer_offset 的特殊主题发送消息，消息里包含每个分区的偏移量。当消费者发生崩溃或者有新的消费者加入群组时就会触发再均衡，再均衡之后，每个消费者可能分配到新的分区，而不是之前处理的分区。为了能够继续之前的工作，消费者需要读取每个分区最后一次提交的偏移量，然后从偏移量指定的地方继续处理。 如果提交的偏移量小于客户端处理的最后一条消息的偏移量，那么处理两个偏移量之间的消息会被重复处理。如下图所示。 如果提交的偏移量大于客户端处理的最后一个消息的偏移量，那么处于两个偏移量之间的消息将会丢失。如下图所示。 4.6.1 自动提交如果 enable.auto.commit 被设为 true，那 么每过 5s，消费者会自动把从 poll() 方法接收到的最大偏移量提交上去。提交时间间隔 由 auto.commit.interval.ms 控制，默认值是 5s。与消费者里的其他东西一样，自动提交也是在轮询里进行的。消费者每次在进行轮询时会检查是否该提交偏移量了，如果是，那么就会提交从上一次轮询返回的偏移量。 在使用自动提交时，每次调用轮询方法都会把上一次调用返回的偏移量提交上去，它并不知道具体哪些消息已经被处理了。 4.6.2 提交当前偏移量开发者既可以通过控制偏移量提交时间，也可以通过提交偏移量来消除丢失消息的可能性，并在发生再均衡时减少重复消息的数量。 把 auto.commit.offset 设为 false，让应用程序决定何时提交偏移量。消费者的 commitSync() API 会提交由 poll() 方法返回的最新偏移量，提交成功后马上返回，如果提交失败就抛出异常。 要记住：commitSync() 将会提交由 poll() 返回的最新偏移量，所以在处理完所有记录后要确保调用了 commitSync()，否则还是会有丢失消息的风险。如果发生了再均衡，从最近一批消息到发生再均衡之间的所有消息都将被重复处理。 4.6.3 异步提交手动提交有一个不足之处，在 broker 对提交请求作出回应之前，应用程序会一直阻塞，这会影响吞吐量，可以通过降低提交频率来提升吞吐量，但如果发生了再均衡，会增加重复消息的数量。此时可以使用异步提交 API commitAsync()，只管发送提交请求，而无需等待 broker 的响应。 在成功提交或碰到无法恢复的错误之前，commitSync() 会一直重试，但是 commitAsync() 不会，如果在服务器出现更大偏移量提交成功的情况下，出现了再均衡，就会出现重复消息。 commitAsync() 支持回调，在 broker 作出响应时会执行回调。 1234567consumer.commitAsync(new OffsetCommitCallback() &#123; public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception e) &#123; if (e != null) &#123; log.error(\"Commit failed for offsets &#123;&#125;\", offsets, e); &#125; &#125;&#125;); 4.6.4 同步和异步组合提交一般情况下，针对偶尔出现的提交失败，不进行重试不会有太大问题，因为如果提交失败是因为临时问题导致的，那么后续的提交总会有成功的。但如果这是发生在关闭消费者或再均衡前的最后一次提交，就要确保能够提交成功。 如果一切正常，使用 commitAsync() 方法来提交。这样速度更快，而且即使这次提交失败，下一次提交很可能会成功。 如果直接关闭消费者，就没有所谓的”下一次提交“了。使用 commitSync() 方法会一 直重试，直到提交成功或发生无法恢复的错误。 4.6.5 提交特定的偏移量我们都知道，提交偏移量的频率与处理消息批次的频率是一样的。但如果想要更频繁地提交，或者在批次中间提交偏移量可以使用消费者 API，它允许在调用 commitSync() 和 commitAsync() 方法时传进去希望提交的分区和偏移量的 map。 12345678910111213private Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets = new HashMap&lt;&gt;();int count = 0;// ...while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(\"Topic, Partition, Offset, customer, country\"); currentOffsets.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1, \"no metadata\")); if (count % 1000 == 0) // 每处理 1000 条记录就提交一次偏移量 consumer.commitAsync(currentOffsets, null); count++; &#125;&#125; 4.7 再均衡监听器上一节提到过，消费者在退出和进行分区再均衡之前，会做一些清理工作。 应该在消费者失去对一个分区的所有权之前提交最后一个已处理记录的偏移量，还可能需要处理缓冲区累积下来的记录、关闭文件句柄、数据库连接等。 在为消费者分配新分区或移除旧分区时， 可以通过消费者 API 执行程序代码，在调用 subscribe() 方法时传进 ConsumerRebalanceListener 实例，这个接口有两个需要实现的方法。 public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) 方法会在再均衡开始之前和消费者停止读取消息之后被调用，如果在这里提交偏移量，下一个接管分区的消费者就知道该从哪里开始读取了。 public void onPartitionAssigned(Collection&lt;TopicPartition&gt; partitions) 方法会在重新分配分区之后和消费者开始读取消息之前被调用。 12345678910111213141516171819202122232425262728293031323334353637383940private Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets = new HashMap&lt;&gt;();private class HandleRebalance implements ConsumerRebalanceListener &#123; @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; collection) &#123; &#125; @Override public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; System.out.println(\"Lost partitions in rebalance, Committing current offsets: \" + currentOffsets); consumer.commitSync(currentOffsets); &#125;&#125;public void someMethod() &#123; try &#123; consumer.subscribe(Collections.singleton(topic), new HandleRebalance()); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100)); for (ConsumerRecord&lt;String, String &gt; record : records) &#123; System.out.println(\"record: \" + record); currentOffsets.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1, \"no metadata\")); &#125; consumer.commitAsync(currentOffsets, null); &#125; &#125; catch (WakeupException e) &#123; log.error(\"msg\"); &#125; catch (Exception e) &#123; log.error(\"Unexpected error \", e); &#125; finally &#123; try &#123; consumer.commitSync(currentOffsets); &#125; finally &#123; consumer.close(); System.out.println(\"Closed consumer and we are done\"); &#125; &#125;&#125; 4.8 从特定偏移量处开始处理记录poll() 方法从各个分区的最新偏移量开始处理消息，如果想从分区的起始位置，或者直接跳到分区的末尾开始读取消息，可以使用 seekToBeginning(Collection&lt;TopicPartition&gt; tp) 和 seekToEnd(Collection&lt;TopicPartition&gt;) tp 这两个方法。 Kafka 也提供了用于查找特定偏移量的 API，它可以向后回退或向前跳过几个消息。在消费者启动或分配到新分区时，可以使用 seek() 方法查找保存在数据库里的偏移量。使用 ConsumerRebalanceListener 和 seek() 方法可以确保是从数据库里保存的偏移量所指定的位置开始处理消息的。 1234567891011121314151617181920212223242526272829303132public class SaveOffsetsOnRebalance implements ConsumerRebalanceListener &#123; @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; // commitDBTransactions(); 虚构的数据库事务，保存记录和偏移量。 &#125; @Override public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; for (TopicPartition partition : partitions) &#123; // 虚构方法从数据库获取偏移量 consumer.seek(partition, getOffsetFromDB(partition)); &#125; &#125;&#125;public void someMethod1() &#123; consumer.subscribe(Collections.singleton(topic), new SaveOffsetsOnRebalance(consumer)); for (TopicPartition partition : consumer.assignment()) &#123; consumer.seek(partition, getOffsetFromDB(partition)); &#125; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(0)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; // processRecord(record); // storeRecordInDB(record); // storeOffsetInDB(record.topic(), record.partition(), record.offset()); &#125; // commitDBTransaction(); &#125;&#125; 4.9 如何退出如果确定要退出 while(true) 循环，需要通过另一个线程调用 consumer.wakeup() 方法。如果循环运行在主线程里，可以在 ShutdownHook 里调用该方法。要记住，consumer.wakeup() 是消费者唯一一个可以从其他线程里安全调用的方法。调用 consumer.wakeup() 可以退出 poll()， 并抛出 WakeupException 异常，或者如果调用 consumer.wakeup() 时线程没有等待轮询，那么异常将在下一轮调用 poll() 时抛出。不需要处理 WakeupException，因为它只是用于跳出循环的一种方式。不过，在退出线程之前调用 consumer.close() 是很有必要的，它会提交任何还没有提交的东西，并向群组协调器发送消息，告知自己要离开群组，接下来就会触发再均衡，而不需要等待会话超时。 优雅退出的完整代码： CloseConsumer 4.10 反序列化器前文提到，生产者需要用序列化器把对象转换成字节数组再发送给 Kafka。 类似地，消费者需要用反序列化器把从 Kafka 接收到的字节数组转换成 Java 对象。 很显然，生成消息使用的序列化器与读取消息使用的反序列化器应该是一一对应的。 使用 Avro 和 schema 注册表进行序列化和反序列化的优势在于： AvroSerializer 可以保证写入主题的数据与主题的 schema 是兼容的，也就是说，可以使用相应的反序列化器和 schema 来反序列化数据。 另外，在生产者或消费者里出现的任何一个与兼容性有关的错误都会被捕捉到，它们都带有消息描述，也就是说，在出现序列化错误时，就没必要再去调试字节数组了。 不推荐自定义序列化器与反序列化器，序列化与反序列化的逻辑相反，这里也不再赘述了。 4.11 无群组的独立消费者此前提到的消费者均为属于某个群组的消费者，消费者在加入消费者群组后被自动分配分区，在群组里新增或移除消费者时自动触发再均衡。如果只需要单个消费者，而且不需要它加入消费者群组，那么就不需要让它订阅主题，取而代之的是它为自己分配分区。 一个消费者可以订阅主题并加入消费者群组，或者为自己分配分区，但不能同时做这两件事情。下面代码展示了一个消费者如何为自己分配分区并从分区里读取消息的。 123456789101112131415161718List&lt;PartitionInfo&gt; partitionInfos = null;// 1. 向集群请求主题可用的分区。如果只打算读取特定分区，可以跳过。partitionInfos = consumer.partitionsFor(\"topic\");if (partitionInfos != null) &#123; for (PartitionInfo partition : partitionInfos) &#123; partitions.add(new TopicPartition(partition.topic(), partition.partition())); &#125; // 2. 知道需要哪些分区后，调用 assign() 方法。 consumer.assign(partitions); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(1000)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(\"record: \" + record); &#125; consumer.commitSync(); &#125;&#125; 除了不会发生再均衡，也不需要手动查找分区。不过，如果主题增加了新的分区，消费者不会收到通知。所以，要么周期性地调用 consumer.partitionsFor() 方法来检查是否有分区加入，要么在添加新分区后重启应用程序。","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://raymond-zhao.top/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://raymond-zhao.top/tags/Kafka/"}]},{"title":"《Kafka 权威指南》：Kafka 生产者","slug":"2021-04-23-KafkaChapter3","date":"2021-04-23T10:47:51.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2021/04/23/2021-04-23-KafkaChapter3/","link":"","permalink":"http://raymond-zhao.top/2021/04/23/2021-04-23-KafkaChapter3/","excerpt":"","text":"3.1 Kafka 概览Kafka 的应用场景中多，如记录用户活动、记录度量指标、保存日志消息、记录智能家电信息、与其他应用程序进行异步通信、缓冲即将写入到数据库的数据等等。多样的应用场景自然也意味着 Kafka 对消息的处理策略会有所不同，如哪些消息允许少量丢失或重复、哪些消息需要有严格的时延以及吞吐量等等。 下图展示了向 Kafka 发送消息的主要步骤，主要包括 创建 ProducerRecord 对象 ProducerRecord 对象包含目标主题和要发送的内容，还可以指定键或分区。 在发送 ProducerRecord 对象时，生产者要先把键和值对象序列化成字节数组，这样它们才能够在网络上传输。 接下来，数据被传给分区器（Partitioner）。 如果之前在 ProducerRecord 对象里指定了分区，那么分区器就不会再做任何事情，直接把指定的分区返回。 如果没有指定分区，那么分区器会根据 ProducerRecord 对象的键（Key）来选择一个分区。选好分区以后，生产者就知道该往哪个主题和分区发送这条记录了。 紧接着，这条记录被添加到一个记录批次（Batch）里，这个批次里的所有消息会被发送到相同的主题和分区上。 之后，一个独立的线程负责把这些记录批次发送到相应的 broker 上。 服务器在收到这些消息时会返回响应。 如果消息成功写入 Kafka，就返回一个 RecordMetaData 对象，它包含了主题和分区信息，以及记录在分区里的偏移量。 如果写入失败，则会返回一个错误。生产者在收到错误之后会尝试重新（Retry）发送消息，几次重试仍然失败，就返回错误信息。 3.2 创建 Kafka 生产者在此选择 Kafka 0.10.1.0 中的示例源码，其中 bootstrap.servers、key.serializer、value.serializer 必须设置。 1234567891011121314private final String topic; // 主题private final Boolean isAsync; // 发送消息方式：同步、异步。public Producer(String topic, Boolean isAsync) &#123; Properties props = new Properties(); // 1. 新建 Peoperties 对象 props.put(\"bootstrap.servers\", \"localhost:9092\"); // 2. 指定 broker 地址清单，可多个。 props.put(\"client.id\", \"DemoProducer\"); // 3. 指定 Key 与 Value 的序列化方式。 props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.IntegerSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); producer = new KafkaProducer&lt;&gt;(props); this.topic = topic; this.isAsync = isAsync;&#125; 3.3 发送消息在实例化 Producer 对象后便可以开始发送消息，主要有以下 $3$ 种方式。 发送并忘记（fire-and-forget）：只管把消息发送出去，而不关心其是否正常到达。 同步发送（synchronous send）：返回 Future 对象，调用 get() 方法进行等待，就可以知道消息是否发送成功。 异步发送（asynchronous send）：指定回调函数，服务器在返回响应时调用回调函数。 1234567891011121314151617181920212223// 一个简单的发送消息的例子public void run() &#123; int messageNo = 1; // 消息编号 while (true) &#123; String messageStr = \"消息-\" + messageNo; long startTime = System.currentTimeMillis(); if (isAsync) &#123; // 异步发送：发送后无序等待直接返回，所以需要传入回调函数 Callback 待消息成功后回调提醒。 producer.send(new ProducerRecord&lt;&gt;(topic, messageNo, messageStr), new DemoCallBack(startTime, messageNo, messageStr)); &#125; else &#123; // 同步发送，返回的是 Future 对象，使用 get() 方法获取返回内容。 try &#123; producer.send(new ProducerRecord&lt;&gt;(topic, messageNo, messageStr)).get(); System.out.println(\"发送消息: (\" + messageNo + \", \" + messageStr + \")\"); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; ++messageNo; // 每发送完一条信息后计数器 +1。 &#125;&#125; 回调函数，需要实现 org.apache.kafka.clients.producer.Callback 接口，实现 onCompletion 方法。 123456789101112131415161718192021222324class DemoCallBack implements Callback &#123; private final long startTime; private final int key; private final String message; public DemoCallBack(long startTime, int key, String message) &#123; this.startTime = startTime; this.key = key; this.message = message; &#125; public void onCompletion(RecordMetadata metadata, Exception exception) &#123; long elapsedTime = System.currentTimeMillis() - startTime; if (metadata != null) &#123; System.out.println( \"message(\" + key + \", \" + message + \") sent to partition(\" + metadata.partition() + \"), \" + \"offset(\" + metadata.offset() + \") in \" + elapsedTime + \" ms\"); &#125; else &#123; exception.printStackTrace(); &#125; &#125;&#125; 3.4 Kafka 的配置Kafka 官方有详细的生产者配置信息，可前往 Producer 查看，出去刚才提到的 $3$ 个，还有一些比较重要的。 Kafka 可以保证同一个分区里的消息是有序的。 配置项 说明 acks 指定必须要有多少个分区副本收到消息，生产者才会认为消息时写入成功的。常用的值有 0、1、all 等。 buffer.memory 设置生产者内存缓冲区的大小，生产者用缓冲区缓存要发送到服务器的消息。 compression.type 消息被发送给 broker 之前的压缩算法，可选值为 snappy、gzip、lz4、zstd。默认为 none。 retries 生产者重发消息的次数，达到后生产者会放弃重试并返回错误。 batch.size 一个批次可以使用的内存大小，按照字节数计算，而非消息个数。 linger.ms 生产者在发送批次之前等待更多消息加入批次的时间。KafkaProducer 会在批次填满或 linger.ms 达到上限时把批次发送出去。 client.id 可以是任意字符串。 max.in.flight.requests.per.connection 生产者在收到服务器相应之前可以发送的消息个数。 request.timeout.ms 生产者在发送数据时等待服务器返回响应的时间 metadata.fetch.timeout.ms 生产者在获取元数据时等待服务器返回响应的时间 timeout.ms broker 等待同步副本返回消息确认的时间 max.request.size 可以指能发送的单个消息的最大值，也可以指单个请求里所有消息总的大小。 receiver/send.buffer.bytes TCP Socket 接收和发送数据包的缓冲区大小。 3.5 序列化器创建生产者对象必须指定序列化器，Kafka 提供了整型和字节数组序列化器，但也可以利用 Avro 序列化器，或者自定义序列化器。 3.5.1 自定义序列化器假设有一个类 Customer，它看起来是 12345@Datapublic class Customer &#123; private int customerID; private String customerName;&#125; 那么，Customer 类的序列化器可以是这样的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class CustomerSerializer implements Serializer&lt;Customer&gt; &#123; @Override public void configure(Map configs, boolean isKey) &#123; // Nothing to configure. &#125; @Override /** * Customer 对象被序列化成： * 表示 customerID 的 4 字节整数 * 表示 customerName 长度的 4 字节整数（为空时为 0） * 表示 customerName 的 N 个字节 */ public byte[] serialize(String topic, Customer data) &#123; try &#123; byte[] serializedName; int stringSize; if (data == null) &#123; return null; &#125; else &#123; if (data.getName() != null) &#123; serializedName = data.getName().getBytes(\"UTF-8\"); stringSize = serializedName.length; &#125; else &#123; serializedName = new byte[0]; stringSize = 0; &#125; &#125; ByteBuffer buffer = ByteBuffer.allocate(4 + 4 + stringSize); buffer.putInt(data.getID()); buffer.putInt(stringSize); buffer.put(serializedName); return buffer.array(); &#125; catch (Exception e) &#123; throw new SerializationException(\"序列化失败：\" + e); &#125; &#125; @Override public void close() &#123; // Nothing to close. &#125;&#125; 但是，这个自定义的序列化器真的很烂。还是使用 Avro 比较好。 3.5.2 使用 Avro 序列化Apache Arvo 是一种与编程语言无关的序列化格式，由 Doug Cutting 创建，目的是提供一种共享数据文件的方式。数据可被序列化为二进制文件或 JSON 文件，不过一般会使用二进制文件。 当负责写消息的应用程序使用了新的 schema，负责读消息的应用程序可以继续处理消息而无需做任何改动。不过有两个需要注意的地方： 用于写入数据和读取数据的 schema 必须是相互兼容的。 反序列化器需要用到用于写入数据的 schema，即使它可能与用于读取数据的 schema 不一样。 3.5.3 在 Kafka 里使用 Avro写入数据需要用到的 schema 被保存在注册表里，然后在记录里引用 schema 的标识符。负责读取数据的应用程序使用标识符从注册表里拉取 schema 来反序列化记录。序列化器和反序列化器分别负责处理 schema 的注册和拉取。 关于如何把 Avro 生成的 Avro对象 发送到 Kafka 可参考 Apache Avro Documentation，与前文创建生产者类似，可以在创建生产者时指定序列化方式以及 Schema 的存储位置。 123props.put(\"key.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");props.put(\"value.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");props.put(\"schema.registry.url\", schemaUrl); 如果选择使用一般的 Avro 对象而非生成的 Avro 对象，只需提供 schema 即可。 3.6 分区前文提到，ProducerRecord 对象包含了 目标主题、键 和 值。Kafka 的消息是一个个的键值对，ProducerRecord 对象可以只包含主题和值，键可以设置为默认的 null。键有两个用途： 作为消息的附加信息 被用来决定消息该被写到主题的哪个分区，拥有相同键的消息将被写到同一个分区。 如果一个进程只从一个主题的分区读取数据，那么具有相同键的所有记录都会被该进程读取，创建包含键值的 ProducerRecord 如下。 123ProducerRecord&lt;Integer, String&gt; record = new ProducerRecord&lt;&gt;(\"CustomerCountry\", \"Laboratory Equipment\", \"USA\");// 如果要创建键为 null 的消息，不指定键即可，第一个参数为主题 Topic。ProducerRecord&lt;Integer, String&gt; record = new ProducerRecord&lt;&gt;(\"CustomerCountry\", \"USA\"); 如果键值为 null 并且使用了默认的分区器，记录将被随机发送到主体内各个可用的分区上。分区器使用轮询（Round Robin）算法将消息均衡地分不到各个分区上。 如果键不为 null 并且使用了默认的分区器，Kafka 将会对键进行散列，然后根据散列值把消息映射到特定的分区上。同一个键总是被映射到同一个分区上。 只有在不改变主题分区数量的情况下，键与分区之间的映射才能保持不变。 下面是一个自定义分区器的例子： 1234567891011121314151617181920public class BananaPartitioner implements Partitioner &#123; public void configure(Map&lt;String, ?&gt; configs) &#123; // Nothing to configure. &#125; public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if ((keyBytes == null) || (!(key instanceof String))) throw new InvalidRecordException(\"Some shit.\"); if (((String) key).equals(\"Banana\")) return numPartitions; // Banana 总是被分配到最后一个分区。 return (Math.abs(Utils.murmur2(keyBytes)) % (numPartitions - 1)) &#125; public void close() &#123; // Nothing to close. &#125;&#125;","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://raymond-zhao.top/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://raymond-zhao.top/tags/Kafka/"}]},{"title":"《Kafka 权威指南》：安装Kafka","slug":"2021-02-07-KafkaChapter2","date":"2021-02-07T10:47:51.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2021/02/07/2021-02-07-KafkaChapter2/","link":"","permalink":"http://raymond-zhao.top/2021/02/07/2021-02-07-KafkaChapter2/","excerpt":"","text":"2.1 安装 Kafka Broker 在安装 Zookeeper 和 Kafka 之前，需要先安装 Java 环境，因为 Kafka 许多功能使用 Java 语言编写，如果运行源码时还需要安装 Scala。 Zookeeper 用于保存集群的元数据信息和消费者信息。 在 MacOS 环境下安装，使用 Homebrew 只需一条命令即可，在安装 Kafka 时 Homebrew 会自动下载相应的依赖，自然也包括 Zookeeper、OpenJDK。 1$ brew install kafka Homebrew 会将 Kafka 安装到 /usr/local/Cellar 目录，不过某些文件会被链接到其他目录。 二进制文件和脚本文件在 /usr/local/bin 目录下。 Kafka 配置文件在 /usr/local/etc/kafka 目录下。 Zookeeper 配置文件在 /usr/local/etc/zookeeper 目录下，需要配置 zoo.cfg。 log.dirs （Kafka 的数据目录）被设置为 /usr/local/var/lib/kafka-logs。 安装完毕后，就可以启动并测试 Zookeeper 和 Kafka了。 12345678910111213141516171819202122232425262728293031$ zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties &amp; kafka-server-start /usr/local/etc/kafka/server.properties$ brew install telnet$ telnet localhost 2181Trying ::1...Connected to localhost.Escape character is '^]'.srvrZookeeper version: 3.5.8...Latency min/avg/max: 0/10/44Received: 80Sent: 81Connections: 2Outstanding: 0Zxid: 0x1dMode: standaloneNode count: 27Connection closed by foreign host.# 创建 Topic$ kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic testCreated topic test.# 验证 Topic$ kafka-topics --zookeeper localhost:2181 --describe --topic testTopic: test PartitionCount: 1 ReplicationFactor: 1 Configs: Topic: test Partition: 0 Leader: 0 Replicas: 0 Isr: 0# 往测试 Topic 上发送消息，输入任意消息然后 Ctrl+C 退出，使用 Consumer 消费信息即可。$ kafka-console-producer --broker-list localhost:9092 --topic test &gt;Test Message 1&gt;Test Message 2^C# 从测试 Topic 上消费消息$ kafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginning Zookeeper 集群被称为群组，使用的是一致性协议，因此建议集群中包含奇数个节点，因为只有当群组里的大多数节点处于可用状态，Zookeeper 才能处理外部的请求。 2.2 Broker 配置 更为详细的配置信息可参考 Kafka 官网：Configuration 2.2.1 常规配置位于 /usr/local/etc/kafka/server.properties 。 配置项 说明 broker.id Kafka 集群内需唯一，默认值为 0。 port 默认 9092，若设置到 1024 以下需要 root 权限启动。 zookeeper.connect 默认 2181，冒号分割的一组 host:port/path log.dirs 逗号分隔的本地文件系统路径，broker 会根据“最少使用”原则把同一个分区的日志片段保存到同一个路径下。 num.recovery.threads.per.data.dir 每个数据目录的线程数，所配置的数字对应的是 log.dirs 指定的单个日志目录。 auto.create.topics.enable 是否启动自动创建主题 num.recovery.threads.per.data.dir：Kafka 使用可配置的线程池来处理日志片段的几种情况 服务器正常启动，用于打开每个分区的日志片段； 服务器崩溃后重启，用于检查和截短每个分区的日志片段； 服务器正常关闭，用于关闭日志片段。 auto.create.topics.enable：Kafka 自动创建主题的几种情况 当一个 Producer 向 Topic 写入消息时； 当一个 Consumer 从 Topic 读取消息时； 当任意一个 Client 向 Topic 发送 Metadata 请求时。 2.2.2 主题的默认配置 配置项 说明 num.partitions 新建主题所包含的分区个数，默认值为 1。 log.retention.hours 数据被保留的时间，默认 168 小时，即一周，也可将hours换成 minutes与ms。 log.retention.bytes 通过消息字节数判断消息是否过期，作用在每一个分区上。 log.segment.bytes 关闭日志片段的阈值，默认 1GB，超过此值将开启新的日志片段。 log.segment.ms 关闭日志片段的阈值，以时间为单位。与log.segment.bytes 非互斥关系。 message.max.bytes 单条消息被压缩后的最大阈值，默认为 1 000 000，即 1MB，超过则会被拒绝。 Kafka 集群通过 Partition 实现对 Topic 的横向扩展，当有新的 Broker 加入集群时，可以通过 Partition 来实现集群的负载均衡。那么，如何确定分区的数量？ Topic 需要多大的吞吐量？ 单个分区读取数据的最大吞吐量是多少？ 生产者向单个分区写入数据的吞吐量是多少？ 每个 broker 包含的分区个数、可用的磁盘空间和网络带宽。 可以使用 主题吞吐量/消费者吞吐量 计算分区个数。 消息何时会被清除？ 通过 log.retention.hours/minutes/ms 与 log.retention.bytes 确定，当消息条件满足任意条件之一时都将会被删除。 日志片段被关闭之前消息是不会过期的，即在达到 log.segment.bytes 设置的阈值之后日志片段关闭，然后再达到 log.retention.xxxx 条件之后消息才会真正过期。 在服务端和客户端之间协调消息大小的配置 消费者客户端所设置的 fetch.message.max.bytes 需要与服务器设置的消息大小 message.max.bytes 所协调，如果这个值比 message.max.bytes 小，那么消费者将无法读取较大的消息，导致消费者被阻塞。 集群中的 broker 配置 replica.fetch.max.bytes，也遵循与上类似的原则。 2.3 Kafka 硬件的选择影响 Kafka 性能的一些硬件因素主要包括：磁盘吞吐量（机械硬盘还是固态硬盘）、磁盘容量（1GB 还是 1TB）、内存大小、网络吞吐量、CPU、Kafka 集群。 2.3.1 需要多少个 broker首先，考虑需要多少磁盘空间来保留数据，以及单个 broker 有多少空间可用；其次要考虑的因素是集群处理请求的能力，因磁盘吞吐量低和系统内存不足造成的性能问题，可以通过扩展多个 broker 来解决。 2.3.2 broker 的配置要把一个 broker 加入到集群里，只需要修改两个配置参数。 所有 broker 都必须配置相同的 zookeeper.connect，该参数指定了用于保存元数据的 Zookeeper 群组和路径。 每个 broker 都必须为 broker.id 参数设置唯一的值。 2.3.3 操作系统调优需要关注的是虚拟内存、磁盘以及网络。 2.3.4 生产环境的注意事项需要注意的是 JVM 垃圾回收器的设置、数据中心的布局以及 Zookeeper 的配置。","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://raymond-zhao.top/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://raymond-zhao.top/tags/Kafka/"}]},{"title":"《Kafka 权威指南》：初识Kafka","slug":"2021-02-07-KafkaChapter1","date":"2021-02-07T04:59:59.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2021/02/07/2021-02-07-KafkaChapter1/","link":"","permalink":"http://raymond-zhao.top/2021/02/07/2021-02-07-KafkaChapter1/","excerpt":"","text":"1.1 发布与订阅消息系统数据（消息）的发送者（发布者）不会直接把消息发送给接收者，这是发布与订阅消息系统的一个特点。发布者以某种方式对消息进行分类，接收者（订阅者）订阅它们，以便接收特定类型的消息。发布与订阅系统一般会有一个 broker，也就是发布消息的中心点。 1.1.1 如何开始发布与订阅消息系统的大部分应用场景都是从一个简单的消息队列或一个进程间通道开始的。 但是这样容易出现一种情况：公司因此要为数据队列维护多个系统，每个系统又有各自的缺陷和不足。而且，接下来可能会有更多的场景需要用到消息系统。此时真正需要的是一个单一的集中式系统，它可以用来发布通用类型的数据，它的规模可以随着公司业务的增长而增长。 1.2 Kafka 登场Apache Kafka，一般被称为“分布式提交日志”或者“分布式流平台”。文件系统或数据库提交日志用来提供所有事务的持久记录，通过重放这些日志可以重建系统状态。同样地，Kafka 的数据是按照一定顺序持久化保存的，可以按需读取。此外，Kafka 的数据分布在整个系统里，具备数据故障保护和性能伸缩能力。 1.2.1 消息和批次 Kafka 的数据单元被称为消息。类似于数据库中的一个“数据行”或一条“记录”。 消息由字节数组组成。 消息可以有一个可选的元数据，又成为“键”。键也是一个字节数组，与消息一样。当消息以一种可控的方式写入不同的分区时，会用到键。 为了提高效率，消息被分批次写入 Kafka。批次是指一组“消息”，这些消息属于同一个主题和分区。可以减少网络开销，但是会影响时间时间延迟和吞吐量。 1.2.2 模式（Schema）对于 Kafka 来说，消息不过是晦涩难懂的字节数组。为了便于理解及支持强类型处理，产生了一些额外的数据结构。 Apache Avro 提供了一种紧凑的序列化格式，模式和消息体分开，当模式发生变化时，不需要重新生成代码;它还支持强类型和模式进化，其版本既向前兼容，也向后兼容。 数据格式的一致性消除了消息读写操作之间的耦合性。 1.2.3 主题和分区 Kafka 的消息通过主题进行分类。主题（Topic）类似于数据中的表（Table），或者文件系统中的文件夹（Directory）。 主题可以被分为若干个分区，一个分区就是一个提交日志。消息以追加方式写入，按先入先出（FIFO）顺序读取。 由于一个主题一般包含几个分区，因此无法在整个主题范围内保证消息的顺序，但可以保证消息在单个分区内的顺序。 一个主题可以横跨多个服务器，提供强大性能。 流（Stream）是一组从生产者移动到消费者的数据。 Kafka Streams、Apache Samza 和 Storm 以实时方式处理消息，即流式处理。Hadoop 被用于离线处理。 ![包含多个分区的主题表示](/Users/raymond/Library/Application Support/typora-user-images/image-20210207134551097.png) 1.2.4 生产者和消费者Kafka 的客户端就是 Kafka 系统的用户，分为生产者与消费者，另有高级客户端，用于数据集成的 Kafka Connect API 和用于流式处理的 Kafka Streams。 生产者 生产者创建消息。一般一条消息会被发布到一个特定的主题。生产者在默认情况下把消息均衡地分布到主题的所有分区上。在某些情况下，生产者会把消息直接写到指定的分区。这通常是通过消息键和分区器来实现的。 分区器为键生成一个散列值，并将其映射到指定的分区上。这样可以保证包含同一个键的消息会被写到同一个分区上。 消费者 消费者订阅一个或多个主题，并按照消息生成的顺序读取它们。 消费者通过检查消息的偏移量来区分已经读取过的消息。偏移量是另一种元数据，它是一个不断递增的整数值，在创建消息时，Kafka 会把它添加到消息里。 消费者把每个分区最后读取的消息偏移量保存在 Zookeeper 或 Kafka 上，如果消费者关闭或重启，它的读取状态不会丢失。 消费者是消费者群组的一部分，也就是说，会有一个或多个消费者共同读取一个主题。群组保证每个分区只能被一个消费者使用。 消费者与分区之间的映射通常被称为消费者对分区的所有权关系。 1.2.5 broker和集群 一个独立的 Kafka 服务器被称为 broker。 broker 接收来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。 broker 为消费者提供服务，对读取分区的请求作出响应，返回已经提交到磁盘上的消息。根据特定的硬件及其性能特征，单个 broker 可以轻松处理数 千个分区以及每秒百万级的消息量。 broker 是集群的组成部分。每个集群都有一个 broker 同时充当了集群控制器的角色（自动从集群的活跃成员中选举出来）。 控制器负责管理工作，包括将分区分配给 broker 和监控 broker。 在集群中，一个分区从属于一个 broker，该 broker 被称为分区的首领（Leader）。 一个分区可以分配给多个 broker，这个时候会发生分区复制。如下图 复制机制为分区提供了消息冗余，如果有一个 broker 失效，其他 broker 可以接管领导权。但是相关的消费者和生产者都要重新连接到新的首领（Leader）。 保留消息（在一定期限内）是 Kafka 的一个重要特性。 旧消息就会过期并被删除，可以自定义配置。Kafka broker 默认的消息保留策略： 要么保留一段时间（比如 7 天）； 要么保留到消息达到一定大小的字节数（比如 1GB）。 1.2.6 多集群多集群的好处： 数据类型隔离 安全需求隔离 多数据中心（灾难恢复） Kafka 的消息复制机制只能在单个集群里进行，不能在多个集群之间进行。Kafka 提供了一个叫作 MirrorMaker 的工具，可以用它来实现集群间的消息复制。 MirrorMaker 的核心组件包含了一个生产者和一个消费者，两者之间通过一个队列相连。 消费者从一个集群读取消息，生产者把消息发送到另一个集群上。 下图展示了一个使用 MirrorMaker 的例子，两个“本地”集群的消息被聚集到一个“聚合”集群上，然后将该集群复制到其他数据中心。 1.3 为什么选择Kafka 多个生产者：Kafka 可以无缝地支持多个生产者。 多个消费者：Kafka 支持多个消费者从一个单独的消息流上读取数据。 基于磁盘的数据存储。 伸缩性。 高性能。通过横向扩展生产者、消费者和 broker，Kafka 可以轻松处理巨大的消息流。在处理大量数据的同时，它还能保证亚秒级的消息延迟。 1.4 数据生态系统Kafka 为数据生态系统带来了循环系统，如下图所示。它在基础设施的各个组件之间传递消息，为所有客户端提供一致的接口。 使用场景： 活动跟踪：页面访问次数、点击量、添加用户资料。 传递消息：向用户发送通知、邮件等。 度量指标和日志记录。 提交日志。 流处理。 1.5 起源起源于 LinkedIn。","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://raymond-zhao.top/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://raymond-zhao.top/tags/Kafka/"}]},{"title":"《Java8实战》高效Java 8编程","slug":"2021-02-02-Java8InActionPart3","date":"2021-02-02T11:48:48.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2021/02/02/2021-02-02-Java8InActionPart3/","link":"","permalink":"http://raymond-zhao.top/2021/02/02/2021-02-02-Java8InActionPart3/","excerpt":"","text":"11 CompletableFuture: 组合式异步编程12 新的日期和时间API Date 和 Calendar这两个类增加了程序员的困惑，到底该使用哪一个类？此外，有的特性只在某一个类有提供，比如用于以语言无关方式格式化和解析日期或时间的 DateFormat 方法就只在 Date 类里有。 DateFormat 不是线程安全的。这意味着两个线程如果尝试使用同一个 formatter 解析日期，可能会得到无法预期的结果。 Date 和 Calendar 类都是可以变的。这种设计会将难以维护。 为了解决这些问题，Oracle 决定在原生的 Java API 中提供高质量的日期和时间支持。Java 8 在 java.time 包中整合了很多 Joda-Time 的特性。 12.1 主要类 LocalDate、LocalTime、Instant、Duration *以及 *Period 12.1.1 使用LocalDate和LocalTime创建一个 LocalDate 对象并读取其值。 12345678LocalDate date = LocalDate.of(2021, 2, 5);int year = date.getYear();Month month = date.getMonth();int dayOfMonth = date.getDayOfMonth();DayOfWeek dayOfWeek = date.getDayOfWeek();int lengthOfMonth = date.lengthOfMonth();boolean leapYear = date.isLeapYear();System.out.println(\"LocalDate.now() = \" + LocalDate.now()); 也可以通过传递一个 TemporalField 参数给 get 方法拿到同样的信息。ChronoField 枚举实现了 TemporalField接口。 12int year1 = date.get(ChronoField.YEAR);int month1 = date.get(ChronoField.MONTH_OF_YEAR); 类似地，一天中的时间，比如 13:45:20，可以使用 LocalTime 类表示。 1234LocalTime time = LocalTime.of(13, 45, 20);LocalTime now = LocalTime.now();int hour = now.getHour();int nano = now.getNano(); LocalDate 和 LocalTime 都可以通过静态方法 parse 解析代表字符串创建。可以向 parse 方法传递一个 DateTimeFormatter。 12LocalDate date = LocalDate.parse(\"2014-03-18\");LocalTime time = LocalTime.parse(\"13:45:20\"); 12.1.2 合并日期和时间LocalDateTime，是 LocalDate 和 LocalTime 的合体，不带有时区信息，可以直接创建，也可以通过合并日期和时间对象创建。 12345678LocalDateTime dt1 = LocalDateTime.of(2021, Month.FEBRUARY, 5, 17, 21, 20);LocalDateTime dt2 = LocalDateTime.of(date, time);LocalDateTime dt3 = date.atTime(13, 45, 20);LocalDateTime dt4 = date.atTime(time);LocalDateTime dt5 = time.atDate(date);LocalDate date1 = dt1.toLocalDate();LocalTime time1 = dt1.toLocalTime(); 12.1.3 机器日期和时间可以通过向静态工厂方法 ofEpochSecond 传递一个代表秒数的值创建一个该类的实例。 ofEpochSecond 的重载版本，接收第二个以纳秒为单位的参数值，对传入作为秒数的参数进行调整。重载的版本会调整纳秒参数，确保保存的纳秒分片在 0 到 999 999 999 之间。下面几个方法都会返回几乎同样的 Instant 对象。 1234Instant.ofEpochSecond(3);Instant.ofEpochSecond(3, 0);Instant.ofEpochSecond(2, 1_000_000_000);Instant.ofEpochSecond(4, -1_000_000_000); Instant 设计初衷是便于机器使用。它包含的是由秒及纳秒所构成的数字。所以，它无法处理常见的的时间单位，日月年等。 12.1.4 定义Duration或Period在本节之前出现的所有类都实现了 Temporal 接口。Duration 类的静态工厂方法 between 可以创建两个 Temporal 对象之间的间隔。 如果需要以年、月或者日的方式对多个时间单位建模，可以使用 Period 类。使用该类的工厂方法 between，可以使用得到两个 LocalDate 之间的时间间隔。 12345678Period tenDays = Period.between(LocalDate.of(2021, 2, 5), LocalDate.of(2021, 2, 15));Duration threeMinutes = Duration.ofMinutes(3);Duration threeMinutes = Duration.of(3, ChronoUnit.MINUTES);Period tenDays = Period.ofDays(10);Period threeWeeks = Period.ofWeeks(3);Period twoYearsSixMonthsOneDay = Period.of(2, 6, 1); Duration 与 Period 类有许多的共享方法，如果用到的话当然还是去看源码，在这列表格也没什么用。 上面的这些日期和时间对象都是不可修改的，这是为了更好地支持函数式编 程，确保线程安全。 12.2 操纵、解析和格式化日期withAttribute 方法会创建对象的一个副本，并按照需要修改它的属性。下面的代码都返回一个修改了属性的对象，它们都不会修改原来的对象。 12345LocalDate date1 = LocalDate.of(2021, 2, 5);LocalDate date2 = date1.withYear(2021);LocalDate date3 = date2.withDayOfMonth(5);// 第一个参数是一个 TemporalField 对象LocalDate date4 = date3.with(ChronoField.MONTH_OF_YEAR, 9); 使用 get 和 with 方法，可以将 Temporal 对象值的读取和修改区分开。还能以声明的方式操纵LocalDate对象。 1234LocalDate date1 = LocalDate.of(2021, 2, 5);LocalDate date2 = date1.plusWeeks(1);LocalDate date3 = date2.minusYear(3);LocalDate date4 = date3.plus(6, ChronoUnit.MONTHS); 表示时间点的日期-时间类的通用方法，用到的时候当然还是 Ctrl + Click。 12.2.1 使用TemporalAdjuster有时候需要进行一些复杂操作，比如，将日期调整到下个周日、下个工作日，或者是本月的最后一天。可以使用重载版本的 with 方法，向其传递一个提供了更多定制化选择的 TemporalAdjuster 对象。 123456// 周五LocalDate date1 = LocalDate.of(2021, 2, 5);// 周日，date2 = 2021-02-07LocalDate date2 = date1.with(nextOrSame(DayOfWeek.SUNDAY)); // 这个月的最后一天，date3 = 2021-02-28LocalDate date3 = date2.with(lastDayOfMonth()); 容易发现 TemporalAdjuster 是一个函数式接口。 1234@FunctionalInterfacepublic interface TemporalAdjuster &#123; Temporal adjustInto(Temporal temporal);&#125; 这意味着 TemporalAdjuster 接口的实现需要定义如何将一个 Temporal 对象转换为另一 个 Temporal 对象。可以看成一个 UnaryOperator&lt;Temporal&gt;。 设计一个 NextWorkingDay 类，该类实现了 TemporalAdjuster 接口，能够计算明天的日期，同时过滤掉周六和周日这些节假日。 12.2.2 打印输出及解析日期-时间对象12345678LocalDate date = LocalDate.of(2021, 2, 5);// s1 = 20210205String s1 = date.format(DateTimeFormatter.BASIC_ISO_DATE);// s2 = 2021-02-05String s2 = date.format(DateTimeFormatter.ISO_LOCAL_DATE);// 当然也可以反向解析, parse = 2021-02-05LocalDate parse = LocalDate.parse(\"20210205\", DateTimeFormatter.BASIC_ISO_DATE); 按照某个格式创建 DateTimeFormatter。 123456789DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"dd/MM/yyyy\");LocalDate date6 = LocalDate.of(2021, 2, 5);// format = 05/02/2021String format = date6.format(formatter);// 创建本地化的 `DateTimeFormatter`。DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy MMMM dd\", Locale.CHINA);// format1 = 2021 二月 05String format1 = date6.format(formatter); 12.3 处理不同的时区和历法 没什么重要的内容","categories":[{"name":"Java","slug":"Java","permalink":"http://raymond-zhao.top/categories/Java/"}],"tags":[{"name":"Java8","slug":"Java8","permalink":"http://raymond-zhao.top/tags/Java8/"}]},{"title":"《Java8实战》函数式数据处理","slug":"2021-01-29-Java8InActionPart2","date":"2021-01-29T12:12:59.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2021/01/29/2021-01-29-Java8InActionPart2/","link":"","permalink":"http://raymond-zhao.top/2021/01/29/2021-01-29-Java8InActionPart2/","excerpt":"","text":"4 引入流集合是Java中使用最多的API。几乎每个Java应用程序都会制造和处理集合。集合对于很多编程任务来说都是非常基本的：它们可以把数据分组并加以处 理。尽管集合对于几乎任何一个Java应用都是不可或缺的，但集合操作却远远算不上完美。 很多业务逻辑都涉及类似于数据库的操作。 在处理大量元素时为了提高性能，需要并行处理，并利用多核架构。 但写并行代码比用迭代器还要复杂，而且调试复杂。 4.1 流是什么？流是 Java API 的新成员，它允许以声明性方式处理数据集合(通过查询语句来表达，而不是临时编写一个实现)。可以把它们看成遍历数据集的高级迭代器。此外，流还可以透明地并行处理。 下面两段代码都是用来返回低热量的菜肴名称的， 并按照卡路里排序。分别使用 Java 7 和 Java 8。 Java 7 1234567891011121314151617List&lt;Dish&gt; lowCaloricDishes = new ArrayList&lt;&gt;();for (Dish d : menu) &#123; if (d.getCalories() &lt; 400) &#123; lowCaloricDishes.add(d); &#125;&#125;Collections.sort(lowCaloricDishes, new Comparator&lt;Dish&gt;() &#123; public int compare(Dish d1, Dish d2) &#123; return Integer.compare(d1.getCalories(), d2.getCalories()); &#125;&#125;);List&lt;String&gt; lowCaloricDishesName = new ArrayList&lt;&gt;();for(Dish d: lowCaloricDishes) &#123; lowCaloricDishesName.add(d.getName());&#125; 在上面的代码中，用了一个“垃圾变量” lowCaloricDishes。它唯一的作用就是作为一次性的中间容器。在 Java 8 中，实现的细节被放在它本该归属的库里了。 12345List&lt;String&gt; lowCaloricDishesName = menu.stream() .filter(d -&gt; d.getCalories() &lt; 400) .sorted(comparing(Dish::getCalories)) .map(Dish::getName) .collect(toList()); 为了利用多核架构并行执行这段代码，只需要把 stream() 换成 parallelStream()。 在调用 parallelStream 方法的时候到底发生了什么。用了多少个线程？对性能有多大提升？这些会在第 7 章详细讨论，目前看来上面的代码至少有以下几点好处。 代码是以声明性方式写的。 可以把几个基础操作链接起来，来表达复杂的数据处理流水线，同时保持代码清晰可读。 因为 filter、sorted、map 和 collect 等操作是与具体线程模型无关的高层次构件，所以它们的内部实现可以是单线程的，也可能透明地充分利用多核架构。这意味着用不着为了让某些数据处理任务并行而去操心线程和锁了，Stream API 都做好了！ 新的 Stream API 表达能力非常强。在读完第4、5、6章之后，应该就可以写出像下面这样的代码： 1Map&lt;Dish.Type, List&lt;Dish&gt; dishesByType&gt; = menu.stream().collect(groupingBy(Dish::getType)); 简单来说就是，按照 Map 里面的类别对菜肴进行分组。比如，Map 可能包含下列结果： 123&#123;FISH=[prawns, salmon], OTHER=[french fries, rice, season fruit, pizza], MEAT=[pork, beef, chicken]&#125; 其他库：Guava、Apache和lambdaj 为了给Java程序员提供更好的库操作集合，前人已经做过了很多尝试。比如，Guava就是谷歌创建的一个很流行的库。它提供了multimaps和multisets等额外的容器类。Apache Commons Collections库也提供了类似的功能。最后，本书作者Mario Fusco编写的lambdaj受到函数式编程的启发，也提供了很多声明性操作集合的工具。 在本章剩下的部分和下一章中，会使用这样一个例子：一个 menu，它只是一张菜肴列表。 123456789101112131415161718public class Dish &#123; private final String name; private final boolean vegetarian; private final int calories; private final Type type; public static final List&lt;Dish&gt; menu = Arrays.asList( new Dish(\"pork\", false, 800, Dish.Type.MEAT), new Dish(\"beef\", false, 700, Dish.Type.MEAT), new Dish(\"chicken\", false, 400, Dish.Type.MEAT), new Dish(\"french fries\", true, 530, Dish.Type.OTHER), new Dish(\"rice\", true, 350, Dish.Type.OTHER), new Dish(\"season fruit\", true, 120, Dish.Type.OTHER), new Dish(\"pizza\", true, 550, Dish.Type.OTHER), new Dish(\"prawns\", false, 400, Dish.Type.FISH), new Dish(\"salmon\", false, 450, Dish.Type.FISH));&#125; 4.2 流简介Java 8 中的集合支持一个新的 stream 方法，它会返回一个流。那么，流到底是什么呢？简短的定义就是“从支持数据处理操作的源生成的元素序列”。 元素序列—就像集合一样，流也提供了一个接口，可以访问特定元素类型的一组有序值。因为集合是数据结构，所以它的主要目的是以特定的时间/空间复杂度存储和访问元素(如ArrayList 与 LinkedList)。但流的目的在于表达计算，比如你前面见到的 filter、sorted 和 map。集合讲的是数据，流讲的是计算。 源—流会使用一个提供数据的源，如集合、数组或输入/输出资源。从有序集合生成流时会保留原有的顺序。由列表生成的流，其元素顺序与列表一致。 数据处理操作—流的数据处理功能支持类似于数据库的操作，以及函数式编程语言中的常用操作，如 filter、map、reduce、find、match、sort等。流操作可以顺序执行，也可并行执行。 此外，流操作有两个重要的特点。 流水线：很多流操作本身会返回一个流，这样多个操作就可以链接起来，形成一个大的流水线。流水线的操作可以看做是对数据源进行数据库式查询。 内部迭代：与使用迭代器显式迭代的集合不同，流的迭代操作是在背后进行的。 先来看一个能体现所有这些概念的代码： 12345List&lt;String&gt; threeHighCaloricDishName = menu.stream() // 获得 流 .filter(d -&gt; d.getCalories() &gt; 300) // 建立操作流水线，首先选出高热量的菜肴。 .map(Dish::getName) // 获取菜名 .limit(3) // 只选择前三条 .collect(toList()); // 将结果收集为 List。 除 collect 之外，所有操作都会返回另一个流，它们接成一条流水线，于是就可以看作对源的一个查询。最后，collect 操作开始处理流水线，并返回结果。在调用 collect 之前，没有任何结果产生，实际上根本就没有从 menu 里选择元素。可以这么理解：链中的方法调用都在排队等待，直到调用 collect。 4.3 流与集合粗略地说，集合与流之间的差异就在于什么时候进行计算。集合是一个内存中的数据结构， 它包含数据结构中目前所有的值—集合中的每个元素都得先算出来才能添加到集合中。 相比之下，流则是在概念上固定的数据结构（不能添加或删除元素），其元素则是按需计算的。从另一个角度来说，流就像是一个延迟创建的集合：只有在消费者要求的时候才会计算值。与此相反，集合则是急切创建的。 4.3.1 只能遍历一次和迭代器类似，流只能遍历一次。 1234List&lt;String&gt; title = Arrays.asList(\"Java8\", \"In\", \"Action\");Stream&lt;String&gt; s = title.stream();s.forEach(System.out::println);s.forEach(System.out::println); // java.lang.IllegalStateException：流已被操作或关闭 4.3.2 外部迭代与内部迭代使用 Collection 接口需要用户去做迭代（比如用 for-each），这称为外部迭代。 相反， Streams 库使用内部迭代—它帮你把迭代做了，还把得到的流值存在了某个地方，你只要给出 一个函数说要干什么就可以了。 4.4 流操作java.util.stream.Stream 中的 Stream 接口定义了许多操作。可以分为两大类： filter、map 和 limit 可以连成一条流水线； collect 触发流水线执行并关闭它。 可以连接起来的流操作称为中间操作，关闭流的操作称为终端操作。 4.4.1 中间操作除非流水线上触发一个终端操作，否则中间操作不会执行任何处理。这是因为中间操作一般都可以合并起来，在终端操作时一次性全部处理。 在 4.2 节的代码中，有好几种优化利用了流的延迟性质。 第一，尽管很多菜的热量都高于 300 卡路里，但只选出了前三个。这是因为 limit 操作和一种称为短路的技巧。 第二，尽管 filter 和 map 是两个独立的操作，但它们合并到同一次遍历中了（循环合并）。 4.4.2 终端操作终端操作会从流的流水线生成结果。其结果是任何不是流的值，比如 List、Integer，甚至 void。例如，在下面的流水线中，forEach 是一个返回 void 的终端操作，它会对源中的每道菜应用一个 Lambda。把 System.out.println 传递给 forEach，并要求它打印出由 menu 生成的流中的每一个 Dish。 1menu.stream().forEach(System.out::println); 4.4.3 使用流使用流一般包括三件事： 一个数据源（如集合）来执行一个查询； 一个中间操作链，形成一条流的流水线； 一个终端操作，执行流水线，并能生成结果。 流的流水线背后的理念类似于构建器模式。在构建器模式中有一个调用链用来设置一套配置(对流来说这就是一个中间操作链)，接着是调用 built 方法(对流来说就是终端操作)。 部分中间操作方法。 操作 返回类型 操作参数 函数描述符 filter Stream Predicate T -&gt; boolean map Stream Function&lt;T, R&gt; T -&gt; R limit Stream sorted Stream Comparator (T, T) -&gt; int distinct Stream 终端操作部分方法 操作 目的 forEach 消费流中的每个元素并对其应用 Lambda。这一操作返回 void。 count 返回流中元素的个数。这一操作返回 long。 collect 把流归约成一个集合，比如 List、Map 甚至是 Integer。 4.5 小结 流是“从支持数据处理操作的源生成的一系列元素”。 流利用内部迭代:迭代通过 filter、map、sorted 等操作被抽象掉了。 流操作有两类:中间操作和终端操作。 filter 和 map 等中间操作会返回一个流，并可以链接在一起。可以用它们来设置一条流水线，但并不会生成任何结果。 forEach 和 count 等终端操作会返回一个非流的值，并处理流水线以返回结果。 流中的元素是按需计算的。 5 使用流在接下来将会看到 Stream API 支持的许多操作。这些操作能快速完成复杂的数据查询，如筛选、切片、映射、查找、匹配和归约。还有一些特殊的流数值流、来 自文件和数组等多种来源的流，最后是无限流。 5.1 筛选和切片在本节中，将看到如何选择流中的元素：用谓词筛选，筛选出各不相同的元素，忽略流中的头几个元素，或将流截短至指定长度。 5.1.1 用谓词筛选 filterStreams 接口支持 filter 方法。该操作会接受一个谓词(一个返回 boolean 的函数)作为参数，并返回一个包括所有符合谓词的元素的流。 1List&lt;Dish&gt; vegetarianMenu = menu.stream().filter(Dish::isVegetarian).collect(toList()); 5.1.2 筛选各异的元素 distinctdistinct 方法，它会返回一个元素各异（根据流所生成元素的 hashCode 和 equals 方法实现）的流。 1234List&lt;Integer&gt; numbers = Arrays.asList(1, 2, 1, 3, 3, 2, 4);numbers.stream().filter(i -&gt; i % 2 == 0) .distinct() .forEach(System.out::println); 5.1.3 截短流 limit流支持 limit(n) 方法，该方法会返回一个不超过给定长度的流。所需的长度作为参数传递给 limit。如果流是有序的，则最多会返回前 n 个元素。 1234List&lt;Dish&gt; dishes = menu.stream() .filter(d -&gt; d.getCalories() &gt; 300) .limit(3) .collect(toList()); limit 也可以用在无序流上，比如源是一个 Set 。这种情况下，limit 的结果不会以任何顺序排列。 5.1.4 跳过元素 skipskip(n) 方法，返回一个扔掉了前 n 个元素的流。如果流中元素不足 n 个，则返回一个空流。limit(n) 和 skip(n) 是互补的。 1234List&lt;Dish&gt; dishes = menu.stream() .filter(d -&gt; d.getCalories() &gt; 300) .skip(2) .collect(toList()); 5.2 映射 map一个非常常见的数据处理套路就是从某些对象中选择信息。比如在 SQL 里，可以从表中选择一列。Stream API 也通过 map 和 flatMap 方法提供了类似的工具。 5.2.1 对流中每一个元素应用函数流支持 map 方法，它接受一个函数作为参数。这个函数会被应用到每个元素上，并将其映射成一个新的元素。 123List&lt;String&gt; dishNames = menu.stream() .map(Dish::getName) // 提取每个元素的菜名 .collect(toList()); 5.2.2 流的扁平化 flatMap 对于一张单词表，如何返回一张列表，列出里面各不相同的字符？例如，给定单词列表 [“Hello”,”World”]，返回列表[“H”,”e”,”l”, “o”,”W”,”r”,”d”]。 可以把每个单词映射成一张字符表，然后调用 distinct 来过滤重复的字符。 1234words.stream() .map(word -&gt; word.split(\"\")) .distinct() .collect(toList()); 这个方法的问题在于，传递给 map 方法的 Lambda 为每个单词返回了一个 String[]。因此，map 返回的流实际上是 Stream&lt;String[]&gt; 类型的。而目标结果是用 Stream&lt;String&gt; 来表示一个字符流。 可以用 flatMap 来解决这个问题。 尝试使用 map 和 Arrays.stream() 有一个叫作 Arrays.stream() 的方法可以接受一个数组并产生一个流。 1234words.stream().map(word -&gt; word.split(\"\")) // 将每个单词转换为由其字母构成的数组 .map(Arrays::stream) // 让每个数组变成一个单独的流 .distinct() .collect(toList()); 现在得到的是一个流的列表（更准确地说是 Stream&lt;String&gt;）。先是把每个单词转换成一个字母数组，然后把每个数组变成了一 个独立的流。 使用 flatMap 1234words.stream().map(word -&gt; word.split(\"\")) // 将每个单词转换为由其字母构成的数组 .flapMap(Arrays::stream) // 让各个生成流扁平化为单个流 .distinct() .collect(toList()); 使用 flatMap 方法的效果是，各个数组并不是分别映射成一个流，而是映射成流的内容。所有使用 map(Arrays::stream) 时生成的单个流都被合并起来，即扁平化为一个流。 一言以蔽之，flapMap 方法把一个流中的每个值都换成另一个流，然后把所有的流连接起来成为一个流。 5.3 查找和匹配另一个常见的数据处理套路是看看数据集中的某些元素是否匹配一个给定的属性。Stream API 通过 allMatch、anyMatch、noneMatch、findFirst 和 findAny 方法提供了这样的工具。 5.3.1 检查谓词是否至少匹配一个元素anyMatch 方法可以回答“流中是否有一个元素能匹配给定的谓词”。anyMatch方法返回一个 boolean，因此 是一个终端操作。 123if (menu.stream().anyMatch(Dish::isVegetarian)) &#123; System.out.println(\"The menu is (somewhat) vegetarian friendly!!\");&#125; 5.3.2 检查谓词是否匹配所有元素allMatch与noneMatch正好相反。 anyMatch、allMatch 和 noneMatch 这三个操作都用到了短路，这就是Java中 &amp;&amp; 和 || 运算符短路在流中的版本。 5.3.3 查找元素findAny 方法将返回当前流中的任意元素。流水线将在后台进行优化使其只需走一遍，并在利用短路找到结果时立即结束。 12Optional&lt;Dish&gt; dish = menu.stream().filter(Dish::isVegetarian) .findAny(); Optional&lt;T&gt; 类 java.util.Optional 是一个容器类，代表一个值存在或不存在。下面是 Optional 里面几种可以迫使显式地检查值是否存在或处理值不存在的情形的方法。 isPresent() 将在 Optional 包含值的时候返回 true, 否则返回 false。 ifPresent(Consumer&lt;T&gt; block) 会在值存在的时候执行给定的代码块。接收 T 类型参数，并返回 void 的 Lambda 表达式。 T get() 会在值存在时返回值，否则抛出一个 NoSuchElement 异常。 T orElse(T other) 会在值存在时返回值，否则返回一个默认值。 123menu.stream().filter(Dish::isVegetarian) .findAny() // 返回一个 Optional&lt;Dish&gt; .ifPresent(d -&gt; System.out.println(d.getName())); // 如果包含一个值就打印，否则什么都不做。 5.3.4 查找第一个元素findFirst 工作方式类似于 findAny。 何时使用findFirst和findAny？ 并行。找到第一个元素在并行上限制更多。如果不关心返回的元素是哪个，使用 findAny，因为它在使用并行流时限制较少。 5.4 归约在此之前见到的终端操作都是返回一个 boolean(allMatch之类的)、void (forEach) 或 Optional 对象(findAny等)。或者是使用 collect 来将流中的所有元素组合成一个 List。 所谓归约，在这里就是指使用 reduce 操作来表达更复杂的查询。此类查询需要将流中所有元素反复结合起来，得到一个值，比如一个 Integer。 5.4.1 元素求和没什么好说的，写几个例子算了。 123456789101112// v1int sum = 0;for (int x : numbers) &#123; sum += x;&#125;// v2int sum = numbers.stream().reduce(0, (a, b) -&gt; a + b);// v3int sum = numbers.stream().reduce(0, Integer::sum);// 无初始值的求和，会返回一个 Optional 对象。Optional&lt;Integer&gt; sum = numbers.stream().reduce((a, b) -&gt; (a + b)); 5.4.2 最大值和最小值reduce 接受两个参数： 一个初始值； 一个 Lambda 来把两个流元素结合起来并产生一个新值。 1234// 求最大值Optional&lt;Integer&gt; max = numbers.stream().reduce(Integer::max);// 求最小值，也可以写成 (x, y) -&gt; x &lt; y ? x : yOptional&lt;Integer&gt; min = numbers.stream().reduce(Integer::min); 怎样用 map 和 reduce 方法数一数流中有多少个菜？ 1int count = menu.stream().map(d -&gt; 1).reduce(0, (a, b) -&gt; a + b); map 和 reduce 的连接通常成为 map-reduce 模式，因 Google 用它来进行网络搜索而出名， 因为它很容易并行化。 传递给 reduce 的 Lambda 不能更改状态（如实例变量），而且操作必须满足结合律才可以按任意顺序执行。 ![中间操作和终端操作](/Users/raymond/Library/Application Support/typora-user-images/image-20210130133213822.png) 5.5 付诸实践5.5.1 领域:交易员和交易123456789101112131415161718192021222324@Datapublic class Trader&#123; private String name; private String city;&#125;@Datapublic class Transaction&#123; private Trader trader; private int year; private int value;&#125;Trader raoul = new Trader(\"Raoul\", \"Cambridge\");Trader mario = new Trader(\"Mario\",\"Milan\");Trader alan = new Trader(\"Alan\",\"Cambridge\");Trader brian = new Trader(\"Brian\",\"Cambridge\");List&lt;Transaction&gt; transactions = Arrays.asList( new Transaction(brian, 2011, 300), new Transaction(raoul, 2012, 1000), new Transaction(raoul, 2011, 400), new Transaction(mario, 2012, 710), new Transaction(mario, 2012, 700), new Transaction(alan, 2012, 950)); 找出2011年发生的所有交易，并按交易额排序(从低到高)。 1234List&lt;Transaction&gt; tr2011 = transactions.stream() .filter(transaction -&gt; transaction.getYear() == 2011) .sorted(comparing(Transaction::getValue)) .collect(toList()); 交易员都在哪些不同的城市工作过？ 1234List&lt;String&gt; cities = transactions.stream() .map(transaction -&gt; transaction.getTrader().getCity()) .distinct() .collect(toList()); 查找所有来自于剑桥的交易员，并按姓名排序。 123456Set&lt;String&gt; cities = transactions.stream() .map(Transaction::getTrader) .filter(trader -&gt; trader.getCity().equals(\"Cambridge\")) .distinct() .sorted(comparing(Trader::getName)) .collect(toList()); 返回所有交易员的姓名字符串，按字母顺序排序。 12345String traderStr = transactions.stream() .map(transaction -&gt; transaction.getTrader().getName()) .distinct() .sorted() .reduce(\"\", (n1, n2) -&gt; n1 + n2); 有没有交易员是在米兰工作的? 12boolean milanBased = transactions.stream() .anyMatch(transaction -&gt; transaction.getTrader().getCity().equals(\"Milan\")); 打印生活在剑桥的交易员的所有交易额。 1234transactions.stream() .filter(t -&gt; \"Cambridge\".equals(t.getTrader().getCity())) .map(Transaction::getValue) .forEach(System.out::println); 所有交易中，最高的交易额是多少？ 123Optional&lt;Integer&gt; highestValue = transactions.stream() .map(Transaction::getValue) .reduce(Integer::max); 找到交易额最小的交易。 12Optional&lt;Transaction&gt; smallestTransaction = transactions.stream() .reduce((t1, t2) -&gt; t1.getValue() &lt; t2.getValue() ? t1 : t2); 5.6 数值流使用 reduce 计算菜单的热量： 12int calories = menu.stream().map(Dish::getCalories) .reduce(0, Integer::sum); 上面这段代码的问题是，它有一个暗含的装箱成本。每个 Integer 都必须拆箱成一个原始类型，再进行求和。 Stream API 提供了原始类型流特化，专门支持处理数值流的方法。 5.6.1 原始类型流特化Java 8 引入了三个原始类型特化流接口来解决这个问题：IntStream、DoubleStream 和 LongStream，分别将流中的元素特化为 int、long 和 double，从而避免了暗含的装箱成本。每个接口都带来了进行常用数值归约的新方法，比如对数值流求和的 sum，找到最大元素的 max。 此外还有在必要时再把它们转换回对象流的方法。 特化的原因并不在于流的复杂性，而是装箱造成的复杂性—即类似 int 和 Integer 之间的效率差异。 映射到数值流 将流转换为特化版本的常用方法是 mapToInt、mapToDouble 和 mapToLong。这些方法和前面说的 map 方法的工作方式一样，只是它们返回的是一个特化流，而不是 Stream&lt;T&gt;。 12int calories = menu.stream().mapToInt(Dish::getCalories) .sum(); 这里，mapToInt 会从每道菜中提取热量（用一个 Integer 表示），并返回一个 IntStream (而不是一个 Stream&lt;Integer&gt;)。接下来就可以调用 IntStream 接口中定义的 sum 方法。如果流是空的，sum 默认返回 0。IntStream 还支持其他的方便方法，如 max、min、average 等。 转换会对象流 IntStream 上的操作只能产生原始整数：IntStream 的 map 操作接受的 Lambda 必须接受 int 并返回 int。但如果想利用 int 生成 Dish 对象呢？要把原始流转换成一般流（每个 int 都会装箱成一个 Integer），可以使用 boxed 方法，如下所示： 12IntStream intStream = menu.stream().mapToInt(Dish::getCalories);Stream&lt;Integer&gt; stream = intStream.boxed(); 默认值 OptionalInt 对于三种原始流特化，也分别有一个 Optional 原始类型特化版本：OptionalInt、OptionalDouble 和 OptionalLong。 要找到 IntStream 中的最大元素，可以调用 max 方法，它会返回一个 OptionalInt： 123OptionalInt maxCalories = menu.stream() .mapToInt(Dish::getCalories) .max(); 如果没有最大值，可以显式处理 OptionalInt 去定义一个默认值： 1int max = maxCalories.orElse(1); 5.6.2 数值范围Java 8 引入了两个可以用于 IntStream 和 LongStream 的静态方法，帮助生成范围或者区间内的所有数值： range 和 rangeClosed。这两个方法都是第一个参数接受起始值，第二个参数接受结束值。但 range 是不包含结束值的，而 rangeClosed 则包含结束值。 1IntStream evenNumbers = IntStream.rangeClosed(1, 100) .filter(n -&gt; n % 2 == 0); 5.6.3 生成勾股数流1234Stream&lt;double[]&gt; pythagoreanTriples = IntStream().rangeClosed(1, 100).boxed() .flatMap(a -&gt; IntStream().rangeClosed(a, 100) .mapToObj(b -&gt; new double[]&#123;a, b, Math.sqrt(a * a + b * b)&#125;) .filter(t -&gt; t[2] % 1 == 0)); 5.7 构建流介绍如何从值序列、数组、文件来创建流，甚至由生成函数来创建无限流！ 5.7.1 由值创建流可以使用静态方法 Stream.of，通过显式值创建一个流。它可以接受任意数量的参数。 123Stream&lt;String&gt; stream = Stream.of(\"Java 8 \", \"Lambdas \", \"In \", \"Action\");// 使用 empty 得到一个空流Stream&lt;String&gt; emptyStream = Stream.empty(); 5.7.2 由数组创建流使用静态方法 Arrays.stream 从数组创建一个流。它接受一个数组作为参数。 12int[] numbers = &#123;2, 3, 5, 7, 11, 13&#125;;int sum = Arrays.stream(numbers).sum(); 5.7.3 由文件生成流Java 中用于处理文件等 I/O 操作的 NIO API（非阻塞 I/O）已更新，以便利用 Stream API。java.nio.file.Files 中的很多静态方法都会返回一个流。例如，方法 Files.lines，它会返回一个由指定文件中的各行构成的字符串流。 5.7.4 由函数生成流:创建无限流Stream API 提供了两个静态方法来从函数生成流：Stream.iterate 和 Stream.generate。 这两个操作可以创建无限流：不像从固定集合创建的流那样有固定大小的流。由 iterate 和 generate 产生的流会用给定的函数按需创建值，因此可以无穷无尽地计算下去！一般来说， 应该使用 limit(n) 来对这种流加以限制。 迭代，生成斐波那契数列前 20 个值。 123Stream.iterate(new int[]&#123;0, 1&#125;, t -&gt; new int[]&#123; t[1], t[0] + t[1]&#125;) .limit(20) .forEach(t -&gt; System.out.println(\"(\" + t[0] + \",\" + t[1] +\")\")); 一般来说，在需要依次生成一系列值的时候应该使用 iterate。 生成，generate 不是依次对每个新生成的值应用函数的。它接受一个 Supplier&lt;T&gt; 类型的 Lambda 提供新的值。 12Stream.generate(Math::random).limit(5) .forEach(System.out::println); 5.8 小结 Streams API 可以表达复杂的数据处理查询。 可以使用 filter、distinct、skip 和 limit 对流做筛选和切片。 可以使用 map 和 flatMap 提取或转换流中的元素。 可以使用 findFirst 和 findAny 方法查找流中的元素。 可以使用 allMatch、noneMatch 和 anyMatch 方法让流匹配给定的谓词。 这些方法都利用了短路：找到结果就立即停止计算；没有必要处理整个流。 可以利用 reduce 方法将流中所有的元素迭代合并成一个结果，例如求和或查找最大元素。 filter 和 map 等操作是无状态的，它们并不存储任何状态。reduce 等操作要存储状态才能计算出一个值。 sorted 和 distinct 等操作也要存储状态，因为它们需要把流中的所有元素缓存起来才能返回一个新的流。这种操作称为有状态操作。 流有三种基本的原始类型特化：IntStream、DoubleStream 和 LongStream。它们的操作也有相应的特化 流不仅可以从集合创建，也可从值、数组、文件以及 iterate 与 generate 等特定方法创建。 无限流是没有固定大小的流。 6 用流收集数据Java 8 的流支持两种类型的操作：中间操作(如 filter 或 map)和终端操作(如count、findFirst、forEach和reduce)。 中间操作可以链接起来，将一个流转换为另一个流。这些操作不会消耗流，其目的是建立一个流水线。 与此相反，终端操作会消耗流，以产生一个最终结果，例如返回流中的最大元素。它们通常可以通过优化流水线来缩短计算时间。 6.1 收集器简介1Map&lt;Currency, List&lt;Transaction&gt;&gt; transactionsByCurrencies = transactions.stream().collect(groupingBy(Transaction::getCurrency)); groupingBy 说的是“生成一个 Map，它的键是(货币)桶，值则是桶中那些元素的列表”。 6.1.1 收集器用作高级归约对流调用 collect 方法将对流中的元素触发一个归约操作（由Collector来参数化）。 般来说，Collector 会对元素应用一个转换函数（很多时候是不体现任何效果的恒等转换， 例如toList），并将结果累积在一个数据结构中，从而产生这一过程的最终输出。 Collectors 实用类提供了很多静态工厂方法， 可以方便地创建常见收集器的实例。 1List&lt;Transaction&gt; transactions = transactionStream.collect(Collectors.toList()); 6.1.2 预定义收集器系统中预定义的收集器主要提供三大功能： 将流元素归约和汇总为一个值 元素分组 元素分区 6.2 归约和汇总6.2.1 查找流中的最大值和最小值Collectors.maxBy 和 Collectors.minBy。 6.2.2 汇总Collectors 类专门为汇总提供了一个工厂方法：Collectors.summingInt。它可接受一个把对象映射为求和所需 int 的函数，并返回一个收集；该收集器在传递给普通的 collect 方法后即执行所需要的汇总操作。 1int totalCalories = menu.stream().collect(summingInt(Dish::getCalories)); Collectors.summingLong 和 Collectors.summingDouble 方法的作用完全一样，可以用 于求和字段为 long 或 double 的情况。 但汇总不仅仅是求和；还有 Collectors.averagingInt，连同对应 的averagingLong 和 averagingDouble 可以计算数值的平均数。 很多时候，可能想要得到两个或更多这样的结果，而且希望只需一次操作就可以完成。在这种情况下，可以使用 summarizingInt 工厂方法返回的收集器。 1IntSummaryStatistics menuStatistics = menu.stream().collect(summarizingInt(Dish::getCalories)); 这个收集器会把所有这些信息收集到一个叫作 IntSummaryStatistics 的类里，它提供了方便的取值（getter）方法来访问结果。 1IntSummaryStatistics &#123;count=9, sum=4300, min=120, average=477.777778, max=800&#125; 同样，相应的 summarizingLong 和 summarizingDouble 工厂方法有相关的 LongSummaryStatistics 和 DoubleSummaryStatistics 类型，适用于收集的属性是原始类型 long 或 double 的情况。 6.2.3 连接字符串joining 工厂方法返回的收集器会把对流中每一个对象应用 toString 方法得到的所有字符串连接成一个字符串。 1String shortMenu = menu.stream().map(Dish::getName).collect(joining()); joining 在内部使用了 StringBuilder 来把生成的字符串逐个追加起来。如果 Dish 类有一个 toString 方法来返回菜肴的名称，无需用提取每一道菜名称的函数来对原流做映射就能够得到相同的结果: 1String shortMenu = menu.stream().collect(joining()); joining 工厂方法有一个重载版本可以接受元素之间的分界符。 1String shortMenu = menu.stream().map(Dish::getName).collect(joining(\", \")); 6.2.4 广义归约汇总之前讨论的所有收集器，都是一个可以用 reducing 工厂方法定义的归约过程 的特殊情况。Collectors.reducing 工厂方法是所有这些特殊情况的一般化。 reduce 方法旨在把两个值结合起来生成一个新值，它是一个不可变的归约。与此相反，collect 方法的设计就是要改变容器，从而累积要输出的结果。 1int totalCalories = menu.stream().collect(reducing(0, Dish::getCalories, Integer::sum)); reducing 三个参数：初始值、转换函数、累积函数。 6.3 分组一个常见的数据库操作是根据一个或多个属性对集合中的项目进行分组。 假设要把菜单中的菜按照类型进行分类， 有肉的放一组，有鱼的放一组，其他的都放另一组。用 Collectors.groupingBy 工厂方法返回的收集器可以轻松地完成这项任务。 1Map&lt;Dish.Type, List&lt;Dish&gt;&gt; dishesByType = menu.stream().collect(groupingBy(Dish::getType)); 分组操作的结果是一个 Map，把分组函数返回的值作为映射的键，把流中所有具有这个分类值的项目的列表作为对应的映射值。 6.3.1 多级分组要实现多级分组，可以使用一个由双参数版本的 Collectors.groupingBy 工厂方法创建的收集器，它除了普通的分类函数之外，还可以接受 collector 类型的第二个参数。要进行二级分组的话，可以把一个内层 groupingBy 传递给外层 groupingBy，并定义一个为流中项目分类的二级标准。 123456Map&lt;Dish.Type, Map&lt;CaloricLevel, List&lt;Dish&gt;&gt;&gt; dishesByTypeCaloricLevel = menu.stream().collect(groupingBy(Dish::getType, groupingBy(dish -&gt; &#123; if (dish.getCalories() &lt;= 400) return CaloricLevel.DIET; else if (dish.getCalories() &lt;= 700) return CaloricLevel.NORMAL; else return CaloricLevel.FAT; &#125;) )); 二级分组结果如下 12345&#123; MEAT=&#123;DIET=[chicken], NORMAL=[beef], FAT=[pork]&#125;, FISH=&#123;DIET=[prawns], NORMAL=[salmon]&#125;, OTHER=&#123;DIET=[rice, seasonal fruit], NORMAL=[french fries, pizza]&#125;&#125; 一般来说，把 groupingBy 看作“桶”比较容易明白。第一个 groupingBy 给每个键建立 一个桶。然后再用下游的收集器去收集每个桶中的元素，以此得到 n 级分组。 6.3.2 按子组收集数据从上一节例子得知，可以把第二个 groupingBy 收集器传递给外层收集器来实现多级分组。但进一步说，传递给第一个 groupingBy 的第二个收集器可以是任何类型，而不一定是另一个 groupingBy。例如，要数一数菜单中每类菜有多少个，可以传递 counting 收集器作为 groupingBy 收集器的第二个参数。 1Map&lt;Dish.Type, Long&gt; typesCount = menu.stream().collect(groupingBy(Dish::getType, counting())); 普通的单参数 groupingBy(f)实际上是 groupingBy(f, toList()) 的简便写法。 把收集器的结果转换为另一种类型（Collectors.collectingAndThen） 1Map&lt;Dish.Type, Dish&gt; mostCalorcByType = menu.stream().collect(groupingBy(Dish::getType, collectingAndThen(maxBy(comparingInt(Dish::getCalories)), Optional::get))); Optional::get 操作在这里是安全的，因为 reducing 收集器永远都不会返回 Optional.empty()。 与 groupingBy 联合使用的其他收集器的例子 一般来说，通过 groupingBy 工厂方法的第二个参数传递的收集器将会对分到同一组中的所有流元素执行进一步归约操作。 常常和 groupingBy 联合使用的另一个收集器是 mapping 方法生成的。这个方法接受两个参数：一个函数对流中的元素做变换，另一个则将变换的结果对象收集起来。其目的是在累加之前对每个输入元素应用一个映射函数，这样就可以让接受特定类型元素的收集器适应不同类型的对象。 1234567891011Map&lt;Dish.Type, Set&lt;CaloricLevel&gt;&gt; caloricLevelByType = menu.stream().collect( groupingBy(Dish::getType, mapping( dish -&gt; &#123; if (dish.getCalories() &lt;= 400) return CaloricLevel.DIET; else if (dish.getCalories() &lt;= 700) return CaloricLevel.NORMAL; else return CaloricLevel.FAT; &#125;, toSet() ))); 6.4 分区分区是分组的特殊情况：由一个谓词作为分类函数，它称分区函数。分区函数返回一个布尔值，这意味着得到的分组 Map 的键类型是 Boolean，于是它最多可以分为两组—true 是一组，false 是一组。 12345// 将菜单按是否是素食分开Map&lt;Boolean, List&lt;Dish&gt;&gt; partitiondMenu = menu.stream() .collect(partitioningBy(Dish::isVegetarian));// &#123;false=[pork, beef, chicken, prawns, salmon],// true=[french fries, rice, season fruit, pizza]&#125; 6.4.1 分区的优势分区的好处在于保留了分区函数返回 true 或 false 的两套流元素列表，而避免使用两次 filter 进行筛选。partitioningBy 工厂方法有一个重载版本，可以传递第二个收集器： 1234Map&lt;Boolean, Map&lt;Dish.Type, List&lt;Dish&gt;&gt;&gt; vegetarianByType = menu.stream() .collect(partitioningBy(Dish::isVegetarian, groupingBy(Dish::getType)));// &#123;false=&#123;FISH=[prawns, salmon], MEAT=[pork, beef, chicken]&#125;,// true=&#123;OTHER=[french fries, rice, season fruit, pizza]&#125;&#125; 6.4.2 Collectors类的静态工厂方法 工厂方法 返回类型 用途 toList List 把流中所有项目收集到一个 List toSet Set 把流中所有项目收集到一个 Set，删除重复项 toCollection Collection 把流中所有项目收集到给定的供应源创建的集合 counting Long 计算流中元素的个数 summingInt Integer 对流中项目的一个整数属性求和 averagingInt Double 计算流中项目 Integer 属性的平均值 summarizingInt IntSummaryStatistics 收集关于流中项目 Integer 属性的统计值，例如最大、最小、总和与平均值 joining String 连接对流中每个项目调用 toString 方法所生成的字符串 maxBy Optionnal 一个包裹了流中按照给定比较器选出的最大元素的 Optional，或如果流为空则为 Optional.empty() minBy Optional 一个包裹了流中按照给定比较器选出的最小元素的 Optional，或如果流为空则为 Optional.empty() reducing 归约操作产生的类型 从一个作为累加器的初始值开始，利用 BinaryOperator 与流 中的元素逐个结合，从而将流归约为单个值 collectingAndThen 转换函数返回类型 包裹另一个收集器，对其结果应用转换函数 groupingBy Map&lt;K, List&gt; 根据项目的一个属性的值对流中的项目作问组，并将属性值作 为结果 Map 的键 partitioningBy Map&lt;Boolean, List&gt; 根据对流中每个项目应用谓词的结果来对项目进行分区 6.5 收集器接口Collector 接口的定义，它列出了接口的签名以及声明的五个方法。 12345678910// T 是流中要收集的项目的泛型// A 是累加器的类型，累加器是在收集过程中用于累积部分结果的对象。// R 是收集操作得到的对象(通常但并不一定是集合)的类型。public interface Collector&lt;T, A, R&gt; &#123; Supplier&lt;A&gt; supplier(); BiConsumer&lt;A, T&gt; accumulator(); Function&lt;A, R&gt; finisher(); BinaryOperator&lt;A&gt; combiner(); Set&lt;Characteristics&gt; characteristics();&#125; 如果要实现一个 ToListCollector&lt;T&gt; 类，将 Stream&lt;T&gt; 中的所有元素收集到一个 List&lt;T&gt; 里，它的签名应该是： 1public class ToListCollector&lt;T&gt; implements Collector&lt;T, List&lt;T&gt;, List&lt;T&gt;&gt; 6.5.1 理解Collector接口声明的方法一个个分析 Collector 接口声明的五个方法。 6.5.2 全部融合到一起6.6 开发自定义收集器改善性能6.7 小结 collect 是一个终端操作，它接受的参数是将流中元素累积到汇总结果的各种方式(称为收集器)。 预定义收集器包括将流元素归约和汇总到一个值，例如计算最小值、最大值或平均值。 预定义收集器可以用 groupingBy 对流中元素进行分组，或用 partitioningBy 进行分区。 收集器可以进行高效复合，进行多级分组、分区和归约。 可以实现 Collector 接口中自定义收集器。 7 并行数据处理与性能在 Java 7 之前，并行处理数据集合非常麻烦。 第一，需要明确地把包含数据的数据结分成若干子部分。 第二，要给每个子部分分配一个独立的线程。 第三，需要在恰当的时候对它们进行同步来避免不希望出现的竞争条件，等待所有线程完成，最后把这些部分结果合并起来。 Java 7 引入了一个叫作 分支/合并 的框架，Stream 则在幕后利用到了 Fork/Join 框架。 7.1 并行流并行流就是一个把内容分成多个数据块，并用不同的线程分别处理每个数据块的流，自动把给定操作的工作负荷分配给多核处理器的所有内核。 7.1.1 将顺序流转换为并行流对顺序流调用 parallel 方法： 1234567// 求 1 ~ n 之和public long parallelSum(long n) &#123; return Stream.iterate(1L, i -&gt; i + 1) .limit(n) .parallel() .reduce(0L, Long::sum);&#125; 同一个归纳操作会将各个子流的部分归纳结果合并起来，得到整个原始流的归纳结果。 ![并行归纳操作](/Users/raymond/Library/Application Support/typora-user-images/image-20210202153937167.png) 对顺序流调用 parallel 方法并不意味着流本身有任何实际的变化。它在内部实际上就是设了一个 boolean 标志，表示想让调用 parallel 之后进行的所有操作都并行执行。只需要对并行流调用 sequential 方法就可以把它变成顺序流。 配置并行流使用的线程池 并行流内部使用了默认的 ForkJoinPool，它默认的线程数量就是处理器数量，这个值可以由 Runtime.getRuntime().availableProcessors() 得到的。 可 以 通 过 系 统 属 性 java.util.concurrent.ForkJoinPool.common. parallelism 来改变线程池大小。 这是全局设置，因此将影响代码中所有的并行流。目前还无法专为某个并行流指定这个值。一般而言，让 ForkJoinPool 的大小等于处理器数量。 1System.setProperty(\"java.util.concurrent.ForkJoinPool.common.parallelism\", \"12\"); 7.1.2 测量流性能软件工程可不是靠猜靠想靠说的学科，在优化性能时，应该始终遵循三个黄金规则：测量，测量，再测量。 12345678910111213// 测量对前 n 个自然数求和的函数的性能public long measureSumPerf(Function&lt;Long, Long&gt; adder, long n) &#123; long fastest = Long.MAX_VALUE; for (int i = 0; i &lt; 10; i++) &#123; long start = System.nanoTime(); long sum = adder.apply(n); long duration = (System.nanoTime() - start) / 1000000; System.out.println(\"result: \" + sum); if (duration &lt; fastest) fastest = duration; &#125; return fastest;&#125; iterate 生成的流不易分块，并且生成的是装箱的对象，必须拆箱成数字才能求和，所以效果是反直觉的。高效的求和方法： 12345public static long parallelRangedSum(long n) &#123; return LongStream.rangeClosed(1, n) .parallel() .reduce(0L, Long::sum);&#125; 并行化并不是没有代价的。并行化过程本身需要对流做递归划分，把每个子流的归纳操作分配到不同的线程，然后把这些操作的结果合并成一个值。但在多个内核之间移动数据的代价也可能很大，所以要保证在内核中并行执行工作的时间比在内核之间传输数据的时间长。 7.1.3 正确使用并行流错用并行流而产生错误的首要原因，就是使用的算法改变了某些共享状态。一些在本质上就是顺序执行的代码使用 parallel 是没有意义的。 7.1.4 高效使用并行流 并行流并不总是比顺序流快，在考虑选择顺序流还是并行流时，最重要的考虑就是用适当的基准来检查其性能。 留意装箱。 有些操作本身在并行流上的性能就比顺序流差。特别是 limit 和 findFirst 等依赖于元素顺序的操作，它们在并行流上执行的代价非常大。 要考虑流的操作流水线的总计算成本。 对于较小的数据量，选择并行流几乎从来都不是一个好的决定。并行处理少数几个元素的好处还抵不上并行化造成的额外开销。 要考虑流背后的数据结构是否易于分解。ArrayList 的拆分效率比 LinkedList 高得多，用 range 工厂方法创建的原始类型流也可以快速分解。 流自身的特点，以及流水线中的中间操作修改流的方式，都可能会改变分解过程的性能。 要考虑终端操作中合并步骤的代价是大是小。 7.2 分支/合并框架分支/合并框架的目的是以递归方式将可以并行的任务拆分成更小的任务，然后将每个子任务的结果合并起来生成整体结果。它是 ExecutorService 接口的一个实现，它把子任务分配给线程池（称为 ForkJoinPool）中的工作线程。 7.2.1 使用RecursiveTask要把任务提交到这个池，必须创建 RecursiveTask&lt;R&gt; 的一个子类，其中 R 是并行化任务（以及所有子任务）产生的结果类型，或者如果任务不返回结果，则是 RecursiveAction 类型。要定义 RecursiveTask，需实现抽象方法 compute： 1protected abstract R compute(); 这个方法同时定义了将任务拆分成子任务的逻辑，以及无法再拆分或不方便再拆分时，生成单个子任务结果的逻辑。伪代码类似于： 1234567if (任务足够小或不可分) &#123; 顺序计算该任务&#125; else &#123; 将任务分成两个子任务 递归调用本方法，拆分每个子任务，等待所有子任务完成 合并每个子任务的结果&#125; 接下来使用 RecursiveTask 实现并行对前 n 个自然数求和加法计算器。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class ForkJoinSumCalculator extends RecursiveTask&lt;Long&gt; &#123; // 要求和的数组 private final long[] numbers; // 子任务处理的数组的起始和终止位置 private final int start, end; // 不再将任务分解为子任务的数组大小 private static final long THRESHOLD = 1_0000; private ForkJoinSumCalculator(long[] numbers) &#123; this(numbers, 0, numbers.length); &#125; public ForkJoinSumCalculator(long[] numbers, int start, int end) &#123; this.numbers = numbers; this.start = start; this.end = end; &#125; @Override protected Long compute() &#123; int length = end - start; if (length &lt; THRESHOLD) return computeSequentially(); ForkJoinSumCalculator leftTask = new ForkJoinSumCalculator(numbers, start, start + length / 2); leftTask.fork(); ForkJoinSumCalculator rightTask = new ForkJoinSumCalculator(numbers, start + length / 2, end); Long rightResult = rightTask.compute(); Long leftResult = leftTask.join(); return leftResult + rightResult; &#125; private long computeSequentially() &#123; long sum = 0; for (int i = start; i &lt; end; i++) sum += numbers[i]; return sum; &#125; public static long forkJoinSum(long n) &#123; long[] numbers = LongStream.rangeClosed(1, n).toArray(); ForkJoinSumCalculator task = new ForkJoinSumCalculator(numbers); return new ForkJoinPool().invoke(task); &#125;&#125; 在实际应用时，使用多个 ForkJoinPool 是没有什么意义的。一般来说把它实例化一次，然后把实例保存在静态字段中，使之成为单例，这样就可以在软件中任何部分方便地重用。 7.2.2 使用分支/合并框架的最佳做法 对一个任务调用 join 方法会阻塞调用方，直到该任务做出结果。因此，有必要在两个子任务的计算都开始之后再调用它。 不应该在 RecursiveTask 内部使用 ForkJoinPool 的 invoke 方法。应该始终直接调用 compute 或 fork 方法，只有顺序代码才应该用 invoke 来启动并行计算。 对子任务调用 fork 方法可以把它排进 ForkJoinPool 。 调试使用分支/合并框架的并行计算可能有点棘手。因为调用 compute 的线程并不是概念上的调用方，后者是调用 fork 的那个。 和并行流一样，不应理所当然地认为在多核处理器上使用分支/合并框架就比顺序计算快。一个惯用方法是把输入/输出放在一个子任务里，计算放在另一个子任务里。 7.2.3 工作窃取理想情况下，划分并行任务时， 应该让每个任务都用完全相同的时间完成，让所有的 CPU 内核都同样繁忙。实际中每个子任务所花的时间可能天差地别。 分支/合并框架工程用一种称为工作窃取（work stealing）的技术来解决这个问题。在实际应用中，这些任务差不多被平均分配到 ForkJoinPool 中的所有线程上。每个线程都为分配给它的任务保存一个双向链式队列，每完成一个任务，就会从队列头上取出下一个任务开始执行。 因为某个线程可能早早完成了分配给它的所有任务，也就是它的队列已经空了，而其他的线程还很忙。这个线程不用闲下来，而是随机选一个别的线程，从队 列的尾巴上“偷走”一个任务。这个过程一直继续下去，直到所有的任务都执行完毕，所有的队列都清空。这就是为什么要划成许多小任务而不是少数几个大任务，这有助于更好地在工作线程之间平衡负载。用于在池中的工作线程之间重新分配和平衡任务。 7.3 SpliteratorSpliterator 是 Java 8 中加入的另一个新接口；这个名字代表“可分迭代器”（splitable iterator）。和 Iterator 一样，Spliterator 也用于遍历数据源中的元素，但它是为了并行执行而设计的。在实践中基本上用不到自己开发 Spliterator。 123456public interface Spliterator&lt;T&gt; &#123; boolean tryAdvance(Consumer&lt;? super T&gt; action); Spliterator&lt;T&gt; trySplit(); long estimatedSize(); int characteristics();&#125;","categories":[{"name":"Java","slug":"Java","permalink":"http://raymond-zhao.top/categories/Java/"}],"tags":[{"name":"Java8","slug":"Java8","permalink":"http://raymond-zhao.top/tags/Java8/"}]},{"title":"《Java8实战》基础知识","slug":"2021-01-29-Java8InActionPart1","date":"2021-01-29T06:27:17.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2021/01/29/2021-01-29-Java8InActionPart1/","link":"","permalink":"http://raymond-zhao.top/2021/01/29/2021-01-29-Java8InActionPart1/","excerpt":"","text":"1.为什么要关心 Java 8Java 8 不但对软件有影响，对硬件也有影响：平常用的 CPU 都是多核的—笔记本电脑或台式机上的处理器可能有四个 CPU 甚至更多内核。但是，绝大多数现有的 Java 程序都只使用其中一个内核， 其他三个都闲着，或只是用一小部分的处理能力来运行操作系统或杀毒程序。 在Java 8之前，专家们可能会说必须利用线程才能使用多个内核。但是线程用起来不容易。 Java 8 特点： Stream API 向方法传递代码的技巧 接口中的默认方法 Java 8 提供了一个新的 API(称为“流”，Stream)，它支持许多处理数据的并行操作，其思路和在数据库查询语言中的思路类似—用更高级的方式表达想要的东西，而由“实现”(在这里是Streams库)来选择最佳低级执行机制。可以避免用 synchronized 编写代码，且效率更高。 Java 8 中的主要变化反映了它开始远离常侧重于改变现有值的经典面向对象思想，而向函数式编程领域转变，在大面上考虑做什么被认为是头等大事，并和如何实现区分开来。 编程语言的整个目的就在于操作值，要是按照历史 上编程语言的传统，这些值因此被称为一等值。编程语言中的其他结构也许有助于我们表示值的结构，但在程序执行期间不能传递，因而是二等公民。人们发现，在运行时传递方法能将方法变成一等公民。 谓词(predicate) 在数学上常常用来代表一个类似函数的东西，它接受一个参数值，并返回true或false。 for-each 循环一个个去迭代元素，然后再处理元素。我们把这种 数据迭代的方法称为外部迭代。相反，有了Stream API，你根本用不着操心循环的事情。数据处理完全是在库内部进行的。我们把这种思想叫作内部迭代。 2.通过行为参数化传递代码行为参数化就是可以帮助你处理频繁变更的需求的一种软件开发模式。一言以蔽之，它意味着拿出一个代码块，把它准备好却不去执行它。这个代码块以后可以被程序的其他部分调用，这意味可以推迟这块代码的执行。 2.1 方法参数化 功能需求：根据传入的 Apple 属性，对 List inventory 进行筛选，完成收集后的 List。 12345@Datapublic class Apple &#123; private int weight = 0; private String color = \"\";&#125; 2.2 阶段 1123456789101112131415161718192021222324252627public static List&lt;Apple&gt; filterGreenApples(List&lt;Apple&gt; inventory)&#123; List&lt;Apple&gt; result = new ArrayList&lt;&gt;(); for (Apple apple: inventory)&#123; if (\"green\".equals(apple.getColor())) &#123; result.add(apple); &#125; &#125; return result;&#125;public static List&lt;Apple&gt; filterHeavyApples(List&lt;Apple&gt; inventory)&#123; List&lt;Apple&gt; result = new ArrayList&lt;&gt;(); for (Apple apple: inventory)&#123; if (apple.getWeight() &gt; 150) &#123; result.add(apple); &#125; &#125; return result;&#125;public List&lt;Apple&gt; filterApplesByParams(List&lt;Apple&gt; inventory, String param) &#123; if (\"color\".equals(param)) &#123; return filterGreenApples(inventory); &#125; else if (\"weight\".equals(param)) &#123; return filterHeavyApples(inventory); &#125;&#125; 如果想改变过滤条件，比如把过滤颜色从 “green” 变成 “red” 该怎么办？重新添加一个方法，但是改动的代码只有一两行吗？ 2.3 阶段 21234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495interface ApplePredicate&#123; public boolean test(Apple a);&#125;static class AppleWeightPredicate implements ApplePredicate&#123; public boolean test(Apple apple)&#123; return apple.getWeight() &gt; 150; &#125;&#125;static class AppleColorPredicate implements ApplePredicate&#123; public boolean test(Apple apple)&#123; return \"green\".equals(apple.getColor()); &#125;&#125;static class AppleRedAndHeavyPredicate implements ApplePredicate&#123; public boolean test(Apple apple)&#123; return \"red\".equals(apple.getColor()) &amp;&amp; apple.getWeight() &gt; 150; &#125;&#125;public static List&lt;Apple&gt; filter(List&lt;Apple&gt; inventory, ApplePredicate p)&#123; List&lt;Apple&gt; result = new ArrayList&lt;&gt;(); for(Apple apple : inventory)&#123; if(p.test(apple))&#123; result.add(apple); &#125; &#125; return result;&#125;public static List&lt;Apple&gt; filterGreenApples(List&lt;Apple&gt; inventory)&#123; List&lt;Apple&gt; result = new ArrayList&lt;&gt;(); for(Apple apple: inventory)&#123; if(\"green\".equals(apple.getColor()))&#123; result.add(apple); &#125; &#125; return result;&#125;public static List&lt;Apple&gt; filterApplesByColor(List&lt;Apple&gt; inventory, String color)&#123; List&lt;Apple&gt; result = new ArrayList&lt;&gt;(); for(Apple apple: inventory)&#123; if(apple.getColor().equals(color))&#123; result.add(apple); &#125; &#125; return result;&#125;public static List&lt;Apple&gt; filterApplesByWeight(List&lt;Apple&gt; inventory, int weight)&#123; List&lt;Apple&gt; result = new ArrayList&lt;&gt;(); for(Apple apple: inventory)&#123; if(apple.getWeight() &gt; weight)&#123; result.add(apple); &#125; &#125; return result;&#125;public static void main(String... args) &#123; List&lt;Apple&gt; inventory = Arrays.asList(new Apple(80,\"green\"), new Apple(155, \"green\"), new Apple(120, \"red\")); // [Apple&#123;color='green', weight=80&#125;, Apple&#123;color='green', weight=155&#125;] List&lt;Apple&gt; greenApples = filterApplesByColor(inventory, \"green\"); System.out.println(greenApples); // [Apple&#123;color='red', weight=120&#125;] List&lt;Apple&gt; redApples = filterApplesByColor(inventory, \"red\"); System.out.println(redApples); // [Apple&#123;color='green', weight=80&#125;, Apple&#123;color='green', weight=155&#125;] List&lt;Apple&gt; greenApples2 = filter(inventory, new AppleColorPredicate()); System.out.println(greenApples2); // [Apple&#123;color='green', weight=155&#125;] List&lt;Apple&gt; heavyApples = filter(inventory, new AppleWeightPredicate()); System.out.println(heavyApples); // [] List&lt;Apple&gt; redAndHeavyApples = filter(inventory, new AppleRedAndHeavyPredicate()); System.out.println(redAndHeavyApples); // [Apple&#123;color='red', weight=120&#125;] List&lt;Apple&gt; redApples2 = filter(inventory, new ApplePredicate() &#123; public boolean test(Apple a)&#123; return a.getColor().equals(\"red\"); &#125; &#125;); System.out.println(redApples2);&#125; 2.4 阶段 3123456789101112131415161718192021222324252627282930313233343536373839404142434445public static boolean isGreenApple(Apple apple) &#123; return \"green\".equals(apple.getColor()); &#125;public static boolean isHeavyApple(Apple apple) &#123; return apple.getWeight() &gt; 150;&#125;public static List&lt;Apple&gt; filterApples(List&lt;Apple&gt; inventory, Predicate&lt;Apple&gt; p) &#123; List&lt;Apple&gt; result = new ArrayList&lt;&gt;(); for(Apple apple : inventory)&#123; if(p.test(apple))&#123; result.add(apple); &#125; &#125; return result;&#125;public static void main(String... args) &#123; List&lt;Apple&gt; inventory = Arrays.asList(new Apple(80,\"green\"), new Apple(155, \"green\"), new Apple(120, \"red\")); // [Apple&#123;color='green', weight=80&#125;, Apple&#123;color='green', weight=155&#125;] List&lt;Apple&gt; greenApples = filterApples(inventory, FilteringApples::isGreenApple); System.out.println(greenApples); // [Apple&#123;color='green', weight=155&#125;] List&lt;Apple&gt; heavyApples = filterApples(inventory, FilteringApples::isHeavyApple); System.out.println(heavyApples); // [Apple&#123;color='green', weight=80&#125;, Apple&#123;color='green', weight=155&#125;] List&lt;Apple&gt; greenApples2 = filterApples(inventory, (Apple a) -&gt; \"green\".equals(a.getColor())); System.out.println(greenApples2); // [Apple&#123;color='green', weight=155&#125;] List&lt;Apple&gt; heavyApples2 = filterApples(inventory, (Apple a) -&gt; a.getWeight() &gt; 150); System.out.println(heavyApples2); // [] List&lt;Apple&gt; weirdApples = filterApples(inventory, (Apple a) -&gt; a.getWeight() &lt; 80 || \"brown\".equals(a.getColor())); System.out.println(weirdApples);&#125; 3.Lambda 表达式3.1 Lambda 管中窥豹可以把 Lambda 表达式理解为简洁地表示可传递的匿名函数的一种方式：它没有名称，但有参数列表、函数主体、返回类型，可能还有一个可以抛出的异常列表。 匿名：它不像普通方法那样有一个确切的名称。 函数： Lambda 函数不像方法（Method）那样属于某个特定的类。但和方法一样， Lambda 有参数列表、函数主体、返回类型，还可能有可以抛出的异常列表。 传递： Lambda 表达式可以作为参数传递给方法或存储在变量中。 简洁：无需像匿名类那样写很多模板代码。 普通方法： 123456// 等号右边的就是 匿名函数类。Comparator&lt;Apple&gt; byWeight = new Comparator&lt;Apple&gt;() &#123; public int compareTo(Apple a1, Apple a2) &#123; return a1.getWeight().compareTo(a2.getWeight()); &#125;&#125; Lambda 函数 1Comparator&lt;Apple&gt; byWeight = (Apple a1, Apple a2) -&gt; a.getWeight().compareTo(a2.getWeight()); 需要注意的是 Comparator&lt;T&gt; 是一个接口。 12@FunctionalInterfacepublic interface Comparator&lt;T&gt; &#123; ... &#125; Lambda 表达式有三个部分：参数列表（可以为空，即无参）、箭头、 Lambda 主体（可以无返回值，即 void）。 一些 Lambda 示例 使用案例 Lambda 示例 布尔表达式 (List&lt;String&gt; list) -&gt; list.isEmpty(); 创建对象 () -&gt; new Apple(10); 消费一个对象 (Apple a) -&gt; { System.out.println(a.getWeight()) }; 从一个对象中选择/抽取 (String s) -&gt; s.length(); 组合两个值 (int a, int b) -&gt; a * b; 比较两个对象 (Apple a1, Apple a2) -&gt; a1.getWeight().compareTo(a2.getWeight()) 3.2 Lambda 使用场景在函数式接口上使用 Lambda 表达式。 3.2.1 函数式接口一言以蔽之，函数式接口就是只定义一个抽象方法的接口。比如常见的 Comparator&lt;T&gt;、Runnable、Callable。 1234567891011121314// java.util.Comparatorpublic interface Comparator&lt;T&gt; &#123; int compare(T o1, T o2);&#125;// java.lang.Runnablepublic interface Runnable &#123; void run();&#125;// java.util.concurrent.Callablepublic interface Callable&lt;V&gt; &#123; V call();&#125; 另外，在 Java 8 中已经允许接口中有方法的默认实现，也就是接口可以拥有默认方法（即在类没有对方法进行实现时， 其主体为方法提供默认实现的方法）。即使一个接口有很多默认方法，但是只要接口只定义了一个抽象方法，它就仍然是一个函数式接口。 3.2.2 函数描述符函数式接口的抽象方法的签名基本上就是 Lambda 表达式的签名。这种抽象方法叫作函数描述符。例如，Runnable 接口可以看作一个什么也不接受什么也不返回(void)的函数的签名，因为它只有一个叫作 run 的抽象方法，这个方法什么也不接受，什么也不返回(void)。 Lambda 表达式可以被赋给一个变量，或传递给一个接受函数式接口作为参数的方法，但是这个 Lambda 表达式的签名要和函数式接口的抽象方法一样。 @FunctionalInterface 是怎么回事? 如果查看新的 Java API，会发现函数式接口带有 @FunctionalInterface 的标注，这个标注用于表示该接口会设计成 一个函数式接口。不是必需的，但对于为此设计的接口而言，使用它是比较推荐的，就像是 @Override 标注表示方法被重写了。 3.3 环绕执行模式资源处理（例如处理文件或数据库）时一个常见的模式就是打开一个资源，做一些处理， 然后关闭资源。这个设置和清理阶段总是很类似，并且会围绕着执行处理的那些重要代码。这就是所谓的环绕执行(execute around)模式。 在以下代码中，是从一个文件中读取一行所需的模板代码。 12345public static String processFile() throws IOException &#123; try (BufferedReader br = new BufferedReader(new FileReader(\"data.json\"))) &#123; return br.readLine(); &#125;&#125; 3.3.1 行为参数化上面那段代码是有局限的。只能读文件的第一行，如果想要返回头两行，甚至是返回使用最频繁的词，该怎么办？理想情况下，要重用执行设置和清理的代码，并告诉 processFile方法对文件执行不同的操作。也就是需要把 processFile 的行为参数化。需要一种方法把行为传递给 processFile，以便它可以利用 BufferedReader 执行不同的行为。 需要一个接收 BufferedReader 并返回 String 的 Lambda，下面就是从 BufferedReader 中打印两行的写法： 1String result = processFile((BufferedReader br) -&gt; br.readLine() + br.readLine()); 3.3.2 使用函数式接口来传递行为前面提到过， Lambda 仅可用于上下文是函数式接口的情况。因此需要创建一个能匹配 BufferedReader -&gt; String，还可以抛出 IOException 异常的接口。暂且称此接口为 BufferedReaderProcessor。 1234@FunctionalInterfacepublic interface BufferedReaderProcessor &#123; String process(BufferedReader b) throws IOException;&#125; 接下来就可以把这个接口作为新的 processFile 方法的参数了： 123public static String processFile(BufferedReaderProcessor p) throws IOException &#123; // ...&#125; 3.3.3 执行一个行为任何 BufferedReader -&gt; String 形式的 Lambda 都可以作为参数来传递，因为它们符合 BufferedReaderProcessor 接口中定义的 process 方法的签名。 Lambda 表达式允许直接内联，为函数式接口的抽象方法提供实现，并且将整个表达式作为函数式接口的一个实例。 12345public static String processFile(BufferedReaderProcessor p) throws IOException &#123; try (BufferedReader br = new BufferedReader(new FileReader(\"data.json\"))) &#123; return p.process(br); &#125;&#125; 3.3.4 传递 Lambda接下来就可以通过传递不同的 Lambda 重用 processFile 方法，并以不同的方式处理文件了。 1234// 处理一行String oneLine = processFile((BufferedReader br) -&gt; br.readLine());// 处理两行String twoLines = processFile((BufferedReader br) -&gt; br.readLine() + br.readLine()); 3.4 使用函数式接口函数式接口定义且只定义了一个抽象方法。函数式接口很有用，因为抽象方法的签名可以描述 Lambda 表达式的签名。函数式接口的抽象方法的签名称为函数描述符。 3.4.1 Predicatejava.util.function.Predicate&lt;T&gt; 接口定义了一个名叫 test 的抽象方法，它接受泛型 T 对象，并返回一个 boolean。在需要表示一个涉及类型 T 的布尔表达式时，就可以使用这个接口。 12345678910111213141516@FunctionalInterfacepublic interface Predicate&lt;T&gt;&#123; boolean test(T t);&#125;public &lt;T&gt; List&lt;T&gt; filter(List&lt;T&gt; list, Predicate&lt;T&gt; predicate) &#123; List&lt;T&gt; res = new ArrayList&lt;&gt;(); for (T t : list) &#123; if (predicate.test(t)) res.add(t); &#125; return res;&#125;Predicate&lt;String&gt; nonEmptyStringPredicate = (String s) -&gt; !s.isEmpty();List&lt;String&gt; nonEmpty = filter(listOfStrings, nonEmptyStringPredicate); 3.4.2 Consumerjava.util.function.Consumer&lt;T&gt; 定义了一个名叫 accept 的抽象方法，它接受泛型 T 的对象，没有返回(void)。如果需要访问类型 T 的对象，并对其执行某些操作，可以使用这个接口。 123456789101112@FunctionalInterfacepublic interface Consumer&lt;T&gt;&#123; void accept(T t);&#125;public &lt;T&gt; void forEach(List&lt;T&gt; list, Consumer&lt;T&gt; consumer) &#123; for (T t : list) &#123; c.accept(t); &#125;&#125;forEach(Arrays.asList(1,2,3,4,5), (Integer i) -&gt; System.out.println(i)); 3.4.3 Functionjava.util.function.Function&lt;T, R&gt; 接口定义了一个叫作 apply 的方法，它接受一个泛型 T 的对象，并返回一个泛型 R 的对象。如果需要定义一个 Lambda ，将输入对象的信息映射到输出，可以使用这个接口。 1234567891011121314@FunctionalInterfacepublic interface Function&lt;T, R&gt; &#123; R apply(T t);&#125;public &lt;T, R&gt; List&lt;R&gt; map(List&lt;T&gt; list, Function&lt;T, R&gt; function) &#123; List&lt;R&gt; res = new ArrayList&lt;&gt;(); for (T t : list) &#123; res.add(function.apply(t)); &#125; return res;&#125;List&lt;Integer&gt; res = map(Arrays.asList(\" Lambda s\",\"in\",\"action\"), (String s) -&gt; s.length()); 3.4.4 原始类型特化Java 类型要么是引用类型(比如 Byte、Integer、Object、List)，要么是原始类型(比如 int、double、byte、char)。但是泛型(比如 Consumer&lt;T&gt; 中的 T)只能绑定到引用类型。这是由泛型内部的实现方式造成的。 自动装箱：将原始类型转换为对应的引用类型。 自动拆箱：将引用类型转换为对应的原始类型。 但这在性能方面是要付出代价的。装箱后的值本质上就是把原始类型包裹起来，并保存在堆（Heap）里。因此，装箱后的值需要更多的内存，并需要额外的内存搜索来获取被包裹的原始值。 Java 8 为前面所说的函数式接口带来了一个专门的版本，以便在输入和输出都是原始类型时避免自动装箱的操作。 比如，在下面的代码中，使用 IntPredicate 就避免了对值 1000 进行装箱操作，但要是用 Predicate&lt;Integer&gt; 就会把参数 1000 装箱到一个 Integer 对象中： 123456789public interface IntPredicate &#123; boolean test(int t);&#125;IntPredicate evenNumbers = (int i) -&gt; i % 2 == 0;evenNumbers.test(1000); // true，无装箱操作。Predicate&lt;Integer&gt; oddNumbers = (Integer i) -&gt; i % 2 == 1;oddNumbers.test(1000); // false, 装箱。 一般来说，针对专门的输入参数类型的函数式接口的名称都要加上对应的原始类型前缀，比如 DoublePredicate、IntConsumer、LongBinaryOperator、IntFunction等。Function 接口还有针对输出参数类型的变种：ToIntFunction、IntToDoubleFunction等。 Java 8 中常用的函数式接口。 函数式接口 函数描述符 原始类型特化 占位 占位 占位 任何函数式接口都不允许抛出受检异常(checked exception)。如果需要 Lambda 表达式来抛出异常，有两种办法： 定义一个自己的函数式接口，并声明受检异常。 或者把 Lambda 包在一个 try/catch 块中。 3.5 类型检查、类型推断以及限制 Lambda 表达式时，说它可以为函数式接口生成一个实例。然而， Lambda 表达式本身并不包含它在实现哪个函数式接口的信息。为了全面了解 Lambda 表达式，你应该知 道 Lambda 的实际类型是什么。 3.5.1 类型检查 Lambda 的类型是从使用 Lambda 的上下文推断出来的。上下文中 Lambda 表达式需要的类型称为目标类型。下图概述了下列代码的类型检查过程。 1List&lt;Apple&gt; heavierThan150g = filter(inventory, (Apple a) -&gt; a.getWeight() &gt; 150); 3.5.2 相同 Lambda ，不同函数式接口。有了目标类型的概念，同一个 Lambda 表达式就可以与不同的函数式接口联系起来，只要它们的抽象方法签名能够兼容。比如， 12Callable&lt;Integer&gt; c = () -&gt; 42;PrivilegedAction&lt;Integer&gt; p = () -&gt; 42; 它们都代表着什么也不接受且返回一个泛型 T 的函数。 特殊的 void 兼容规则 如果一个 Lambda 的主体是一个语句表达式， 它就和一个返回 void 的函数描述符兼容(当然需要参数列表也兼容)。 3.5.3 类型推断Java 编译器会从上下文（目标类型）推断出用什么函数式接口来配合 Lambda 表达式，这意味着它也可以推断出适合 Lambda 的签名，因为函数描述符可以通过目标类型来得到。这样做的好处在于，编译器可以了解 Lambda 表达式的参数类型，这样就可以在 Lambda 语法中省去标注参数类型。比如： 1234// 显示参数类型Comparator&lt;Apple&gt; c = (Apple a1, Apple a2) -&gt; a1.getWeight().compareTo(a2.getWeight());// 类型推断，从 Comparator 的泛型类型推断出 a1, a2 的类型。Comparator&lt;Apple&gt; c = (a1, a2) -&gt; a1.getWeight().compareTo(a2.getWeight()); 3.5.4 使用局部变量迄今为止所介绍的所有 Lambda 表达式都只用到了其主体里面的参数。但 Lambda 表达式也允许使用自由变量(不是参数，而是在外层作用域中定义的变量)，就像匿名类一样。 它们被称作捕获 Lambda 。比如： 12int portNumber = 1337;Runnable r = () -&gt; System.out.println(portNumber); Lambda 可以没有限制地捕获实例变量和静态变量。但局部变量必须显式声明为 final， 或事实上是 final。换句话说， Lambda 表达式只能捕获指派给它们的局部变量一次。(注:捕获实例变量可以被看作捕获最终局部变量 this。) 实例变量都存储在堆中，而局部变量则保存在栈上。 闭包 闭包就是一个函数的实例，且它可以无限制地访问那个函数的非本地变量。闭包可以作为参数传递给另一个函数。它也可以访问和修改其作用域之外的变量。现在，Java 8 的 Lambda 和匿名类可以做类似于闭包的事情：它们可以作为参数传递给方法，并且可以访问其作用域之外的变量。但有一个限制：它们不能修改定义 Lambda 的方法的局部变量的内容。这些变量必须是隐式最终的。可以认为 Lambda 是对值封闭，而不是对变量封闭。 3.6 方法引用方法引用允许重复使用现有的方法定义，并像 Lambda 一样传递它们。在一些情况下， 比起使用 Lambda 表达式，它们更易读，也更自然。 1234// Lambda inventory.sort((Apple a1, Apple a2) -&gt; a1.getWeight().compareTo(a2.getWeight()));// Method Referenceinventory.sort(comparing(Apple::getWeight)); 3.6.1 管中窥豹方法引用可以被看作仅仅调用特定方法的 Lambda 的一种快捷写法。它的基本思想是，如果一个 Lambda 代表的只是“直接调用这个方法”，那最好还是用名称来调用它，而不是去描述如何调用它。事实上，方法引用就是根据已有的方法实现来创建 Lambda 表达式。但是，显式地指明方法的名称，代码的可读性会更好。 当需要使用方法引用时，目标引用放在分隔符 :: 前，方法名称放在后面（不需要括号，因为并没有实际调用这个方法）。可以把方法引用看作针对仅仅涉及单一方法的 Lambda 的语法糖。 Lambda 及其等效方法引用的例子。 Lambda 等效的方法引用 (Apple a) -&gt; a.getWeight() Apple::getWeight () -&gt; Thread.currentThread().dumpStack() Thread.currentThread()::dumpStack (str, i) -&gt; str.substring(i) String::substring (String s) -&gt; System.out.println(s) System.out::println 方法引用主要有三类。 指向静态方法的方法引用(例如 Integer 的 parseInt 方法，写作 Integer::parseInt)。 指向任意类型实例方法的方法引用(例如 String 的 length 方法，写作 String::length)。 指向现有对象的实例方法的方法引用(假设有一个局部变量 expensiveTransaction 用于存放 Transaction 类型的对象，它支持实例方法 getValue，那么可以写 expensiveTransaction::getValue)。 依照一些简单的方法，可以将 Lambda 表达式重构为等价的方法引用。 3.6.2 构造函数引用对于一个现有构造函数，可以利用它的名称和关键字 new 来创建它的一个引用：ClassName::new。它的功能与指向静态方法的引用类似。例如，假设有一个构造函数没有参数。 它适合 Supplier 的签名 () -&gt; Apple。可以这样做： 1234// 构造函数引用指向默认的 Apple() 构造函数Supplier&lt;Apple&gt; c1 = Apple::new; // 等价于 Supplier&lt;Apple&gt; c1 = () -&gt; new Apple();// 调用 Supplier 的 get 方法 将产生一个新的 Apple。Apple a1 = c1.get(); 如果构造函数的签名是 Apple(Integer weight)，那么它就适合 Function 接口的签名，于是可以这样写： 123// 等价于 Function&lt;Integer, Apple&gt; c2 = (weight) -&gt; new Apple(weight);Function&lt;Integer, Apple&gt; c2 = Apple::new;Apple a2 = c2.apply(110); 如果构造函数具有两个参数，签名是 Apple(String color, Integer weight)，它适合 BiFunction 接口的签名，于是可以这样写： 123// 等价于 BiFunction&lt;String, Integer, Apple&gt; c3 = (color, weight) -&gt; new Apple(color, weight);BiFunction&lt;String, Integer, Apple&gt; c3 = Apple::new;Apple c3 = c3.apply(\"green\", 110); 3.7 Lambda 和方法引用实战 用不同的排序策略给一个 Apple 列表排序，并需要展示如何把一个原始粗暴的解决方案转变得更为简明。 最后结果： 1inventory.sort(comparing(Apple::getWeight)); 3.8 复合方法可以把多个简单的 Lambda 复合成复杂的表达式。可以让两个谓词之间做一个 or 操作，组合成一个更大的谓词，还可以让一个函数的结果成为另一个函数的输入。 3.8.1 比较器复合可以使用静态方法 Comparator.comparing，根据提取用于比较的键值的 Function 来返回一个 Comparator，如下所示： 123456// 原始Comparator&lt;Apple&gt; c = Comparator.comparing(Apple::getWeight);// 逆序inventory.sort(comparing(Apple::getWeight).reversed());// 比较器链。在按重量比较两个苹果之后，按原产国排序。inventory.sort(comparing(Apple::getWeight).reversed().thenComparing(Apple::getCountry)); 3.8.2 谓词复合谓词接口包括三个方法：negate、and 和 or，可以重用已有的 Predicate 来创建更杂的谓词。 12345Predicate&lt;Apple&gt; notRedApple = redApple.negate(); // 苹果不是红的// 链接两个谓词来生成另一个 Predicate 对象Predicate&lt;Apple&gt; redAndHeavyApple = redApple.and(a -&gt; a.getWeight() &gt; 150);// Predicate&lt;Apple&gt; redAndHeavyAppleOrGreen = redApple.and(a -&gt; a.getWeight() &gt; 150).or(a -&gt; \"green\".equals(a.getColor())); 3.8.3 函数复合可以把 Function 接口所代表的 Lambda 表达式复合起来。Function 接口为此配了 andThen 和 compose 两个默认方法，它们都会返回 Function 的一个实例。 andThen：先对输入应用一个给定函数，再对输出应用另一个函数。意味着 g(f(x))。 compose：先把给定的函数用作 compose 的参数里面给的那个函数，然后再把函数本身用于结果。意味着 f(g(x))。 3.9 数学中的类似思想3.10 小结 Lambda 表达式可以理解为一种匿名函数:它没有名称，但有参数列表、函数主体、返回类型，可能还有一个可以抛出的异常的列表。 Lambda 表达式可以简洁地传递代码。 函数式接口就是仅仅声明了一个抽象方法的接口。 只有在接受函数式接口的地方才可以使用 Lambda 表达式。 Lambda 表达式允许直接内联，为函数式接口的抽象方法提供实现，并且将整个表达式作为函数式接口的一个实例。 Java 8 自带一些常用的函数式接口，放在 java.util.function 包里。 为了避免装箱操作，对 Predicate&lt;T&gt; 和 Function&lt;T, R&gt; 等通用函数式接口的原始类型特化。 Lambda 表达式所需要代表的类型称为目标类型。 方法引用可以重复使用现有的方法实现并直接传递它们。 Comparator、Predicate 和 Function 等函数式接口都有几个可以用来结合 Lambda 表达式的默认方法。","categories":[{"name":"Java","slug":"Java","permalink":"http://raymond-zhao.top/categories/Java/"}],"tags":[{"name":"Java8","slug":"Java8","permalink":"http://raymond-zhao.top/tags/Java8/"}]},{"title":"TCP的运输连接管理","slug":"2020-08-26-Network-TCPANDUDP","date":"2020-08-26T12:19:29.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/08/26/2020-08-26-Network-TCPANDUDP/","link":"","permalink":"http://raymond-zhao.top/2020/08/26/2020-08-26-Network-TCPANDUDP/","excerpt":"","text":"TCP的运输连接管理TCP 是面向连接的协议，运输连接是用来传送 TCP 报文的。TCP 运输连接的建立和释放是每一次面向连接的通信中必不可少的过程。因此，运输连接就有三个阶段，即：连接建立、数据传送和连接释放。运输连接的管理就是使运输连接的建立和释放都能正常地进行。 在TCP连接建立过程中要解决以下三个问题: 要使每一方能够确知对方的存在。 要允许双方协商一些参数(如最大窗口值、是否使用窗口扩大选项和时间戳选项以及服务质量等)。 能够对运输实体资源(如缓存大小、连接表中的项目等)进行分配。 三次握手：建立连接TCP建立连接的过程叫做握手，握手需要在客户和服务器之间交换三个TCP报文段。 假定主机A运行的是TCP客户程序，而B运行TCP服务器程序。最初两端的 TCP 进程都处 CLOSED(关闭)状态。 A主动打开连接，而B被动打开连接。 B的TCP服务器进程创建传输控制块TCB，准备接受客户进程的连接请求，进入监听(Listen)状态。 A的TCP客户端进程创建传输控制块TCB。 A向B请求建立连接时，发出连接请求报文段，首部中同步位 SYN=1，选择初始序号 seq=x。TCP客户进程进入SYN-SENT(同步已发送)状态。 B收到连接请求报文段后，如同意建立连接，则向A发送确认。在确认报文段中携带 SYN=1，ACK=1，ack=x+1，seq=y。服务器进程进入SYN-RCVD(同步收到)状态。 TCP客户进程收到B的确认后，向B发出确认。携带 ACK=1，ack=y+1，seq=x+1。TCP连接已经建立，A 进入 ESTABLISHED (已建立连接)状态。 当 B 收到 A 的确认后，也进入 ESTABLISHED 状态。 SYN报文段不能携带数据，但要消耗一个序号。 确认报文段也不能携带数据，但同样要消耗掉一个序号。也可以把SYN=1，ACK=1拆成两个报文段，这样变成四报文握手，效果一样。 ACK报文段可以携带数据。但如果不携带数据则不消耗序号。 12Q：为什么 A 最后还要发送一次确认呢?A：这主要是为了防止已失效的连接请求报文段突然又传送到了 B，因而产生错误。 “已失效的连接请求报文段”是指，在 A 发出连接请求后，因连接请求报文丢失而未收到确认，于是 A 再重传一次连接请求，并且收到了确认，建立了连接。数据传输完毕后，就释放了连接。 A 共发送了两个连接请求报文段，其中第一个丢失，第二个到达了B，没有“已失效的连接请求报文段”。 现假定出现一种异常情况，即 A 发出的第一个连接请求报文段并没有丢失，而是在某些网络结点长时间滞留了，以致延误到连接释放以后的某个时间才到达B。 这是一个早已失效的报文段，但 B 收到此失效的连接请求报文段后，就误认为是 A 又发出一次新的连接请求。于是就向 A 发出确认报文段，同意建立连接。假定不采用三报文握手，那么只要 B 发出确认，新的连接就建立了。 由于现在 A 并没有发出建立连接的请求，因此不会理睬 B 的确认，也不会向B发送数据。但 B 却以为新的运输连接已经建立了，并一直等待 A 发来数据。B 的许多资源就这样白白浪费了。 采用三报文握手的办法，可以防止上述现象的发生。例如在刚才的异常情况下，A 不会向 B 的确认发出确认。B 由于收不到确认，就知道 A 并没有要求建立连接。 四次分手：断了联系数据传输结束后，通信的双方都可释放连接。 A 和 B 都处于 ESTABLISHED 状态。 A 的应用进程先向其 TCP 发出连接释放报文段，并停止再发送数据，主动关闭 TCP 连接。连接释放报文段首部终止控制位 FIN=1，seq=u，它等于前面已传送过的数据的最后一个字节的序号加 1。A进入 FIN-WAIT-1(终止等待1) 状态，等待 B 的确认。 B 收到连接释放报文段后即发出确认，确认号是 ack=u+1，而这个报文段自己的序号是 v，等于 B 前面已传送过的数据的最后一个字节的序号加 1。 进入 CLOSE-WAIT(关闭等待) 状态。 TCP 服务器进程通知高层应用进程，此时从 A 到 B 这个方向的连接已释放，TCP连接处于 半关闭(half-close) 状态，即 A 已经没有数据要发送了，但 B 若发送数据，A 仍要接收。也就是说，从B到A这个方向的连接并未关闭，这个状态可能会持续一段时间。 A 收到 B 的确认后，进入 FIN-WAIT-2(终止等待2) 状态，等待 B 发出的连接释放报文段。 若 B 已经没有要向A发送的数据，其应用进程就通知 TCP 释放连接。 B 发出的连接释放报文段 FIN=1，seq=w (半关闭状态下 B 可能又发送了一些数据)。 B 还必须重复上次已发送过的确认号 ack=u+1。B进入 LAST-ACK(最后确认) 状态，等待A的确认。 A 收到 B 的连接释放报文段后，对此发出确认。 在确认报文段中，ACK=1，ack=w+1，seq=u+1，然后进入 TIME-WAIT(时间等待) 状态。 此时 TCP连接还没有释放掉，必须经过 2MSL 后，A才进入到 CLOSED状态。 MSL叫做最长报文段寿命(Maximum Segment Lifetime)，RFC793建议设为2分钟。但 TCP 允许不同实现自定义。因此，从A进入到 TIME-WAIT状态后，要经过 2MSL 才能进入到 CLOSED状态，才能开始建立下一个新的连接。 当A撤销相应的传输控制块TCB后，就结束了这次的TCP连接。 B 收到 A 发出的确认后，才进入 CLOSED状态，在 B 撤销 PCB 后，本次 TCP 连接才算结束。 B 结束 TCP 连接的时间要比 A 早一些，因为 A 要等待 2MSL。 1为什么A在TIME-WAIT状态必须等待2MSL的时间? 12345678910111. 为了保证 A 发送的最后一个 ACK 报文段能够到达 B。这个 ACK 报文段有可能丢失，因而使处在 LAST-ACK 状态的 B 收不到对已发送的 FIN+ACK报文段 的确认。B会超时重传这个 FIN+ACK 报文段，而 A 就能在 2MSL 时间内收到这个重传的 FIN+ACK 报文段。接着 A 重传一次确认，重新启动 2MSL 计时器。最后，A 和 B 都正常进入到 CLOSED 状态。如果 A 在 TIME-WAIT 状态不等待一段时间，而是在发送完 ACK 报文段后立即释放连接，那么就无法收到 B 重传的 FIN+ACK 报文段，因而也不会再发送一次确认报文段。这样，B就无法按照正常步骤进入 CLOSED 状态。2. 防止“已失效的连接请求报文段”出现在本连接中。A 在发送完最后一个 ACK 报文段后，再经过时间 2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样就可以使下一个新的连接中不会出现这种旧的连接请求报文段。 TCP与UDP的首部 UDP 的首部 TCP 的首部 TCP 与 UDP 的对比 TCP UDP 用户数据报协议UDP只在IP的数据报服务之上增加了很少一点的功能，这就是复用和分用的功能以及差错检测的功能 面向连接，应用程序在使用TCP协议之前，必须先建立TCP连接。在传送数据完毕后，必须释放已经建立的TCP连接。 无连接的，即发送数据之前不需要建立连接，发送数据之后也不需要断开连接，因此减少了开销和发送数据之前的时延。 提供可靠交付，通过TCP连接传送的数据，无差错、不丢失、不重复，并且按序到达。 尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的连接状态表 面向字节流。TCP中的“流”(stream)指的是流入到进程或从进程流出的字节序列。 UDP是面向报文的 滑动窗口，流量控制，拥塞控制，停止等待协议，连续ARQ协议 没有拥塞控制，因此网络出现的拥塞不会使源主机的发送速率降低。这对实时应用是很重要的。吞吐量只受限于数据生成速率，传输速率以及机器性能 每一条TCP连接只能有两个端点(endpoint)，每一条TCP连接只能是点对点的(一对一) 支持一对一、一对多、多对一和多对多的交互通信。 首部至少20个字节 首部开销小，只有8个字节 TCP提供全双工通信。TCP允许通信双方的应用进程在任何时候都能发送数据。TCP连接的两端都设有发送缓存和接收缓存，用来临时存放双向通信的数据。 提供全双工通信 参考 《计算机网络》第七版，谢希仁。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://raymond-zhao.top/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"http://raymond-zhao.top/tags/TCP/"}]},{"title":"TCP传输为什么是可靠的？","slug":"2020-08-26-Network-WhyTCPReliable","date":"2020-08-26T09:13:58.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/08/26/2020-08-26-Network-WhyTCPReliable/","link":"","permalink":"http://raymond-zhao.top/2020/08/26/2020-08-26-Network-WhyTCPReliable/","excerpt":"","text":"TCP可靠传输的工作原理TCP 发送的报文段是交给 IP层 传送的。但 IP层 只能提供尽最大努力交付，也就是说，TCP 下面的网络所提供的是不可靠的传输。因此，TCP必须采用适当的措施才能使得两个运输层之间的通信变得可靠。 理想的传输信道有以下两个特点： 传输信道不产生差错 不管发送方发送速度多快，接收方总是来得及处理到来的数据。 然鹅，理想很丰满，现实很骨感。实际网络并不具备这两个条件，但是可以使用一些可靠的传输协议，当 出现差错时让发送方重传出现差错的数据 在接收方来不及处理到来的数据时，及时告诉发送方适当降低发送数据的速率。 停止等待协议全双工通信的双方既是发送方也是接收方。以下仅考虑 A 发送数据而 B 接收数据并发送确认。称 A 为发送方，B 为接收方。 这里讨论可靠传输的原理，因此把传送的数据单元都称为分组，而并不考虑数据是在哪一层次上传送的。 “停止等待”就是每发送完一个分组就停止发送，等待对方的确认。在收到确认后再发送下一个分组。 无差错情况 最简单的情况，如 (a) 所示，A 发送完分组 $M_1$ ，发完就暂停发送，等待 B 的确认。B 收到 $M_1$ 后向 A 发送确认，A 收到对 $M_1$ 的确认后才发送下一个分组 $M_2$ 。其他的分组也采用同样的策略发送。 出现差错 图 (b) 是分组在传输中出现差错的情况。 B 收到 $M_1$ 时检测到差错便丢弃 $M_1$，不对 A 作出应答。 $M_1$ 在传输过程中走丢了，B 没有收到过，也不会作出应答。 可靠传输协议：A只要超过了一段时间仍然没有收到确认，就认为刚才发送的分组丢失了，因而重传前面发送过的分组。这就叫做超时重传。 要实现超时重传，要在每发送完一个分组时设置一个超时计时器。如果在超时计时器到期之前收到确认，就撤销已设置的超时计时器。 🧐 需要注意三点： A 在发送完一个分组后，必须暂存已发送的分组的副本(超时重传时是用)，收到相应的确认后才能清除。 分组和确认分组都必须进行编号，这样才能知道是哪一个发送出去的分组收到了确认，而哪一个分组还没有收到确认。 超时计时器设置的重传时间应当比数据在分组传输的平均往返时间更长一些。(其实这个时间的选择很有讲究) 确认丢失和确认迟到 如下图 (a) 所示，B 收到了 $M_1$ 并且也对 A 发出了确认，但是确认在中途丢失了。A 在设定的超时重传时间内没有收到确认，它也无法得知是自己发送的 $M_1$ 出错、丢失、还是 B 的确认丢失了，总之，A 会重传。 A 重传之后，B 收到了重传的分组 $M_1$ ，这时摆在它面前的有两个选择： 直接丢弃 $M_1$ ，不向上层报告(交付)。 继续对 A 发送确认。B 不能认为已经发送给确认就不再崇法确认，毕竟 A 就是因为没有收到确认才重发的，如果不回复确认，A 还会一直重发。 上图 (b) 是另一种可能的情况。B 对 A 发出了确认，但是确认在网络中滞留了很久(可能是迷路了，可能是堵车了，可能是被裹挟了)，此时 A 会收到重复的确认，对重复的确认的处理很简单：收下后就丢弃。B 仍然会收到重复的 $M_1$(超时重传的)，并且同样要丢弃重复的 $M_1$，并重传确认分组。 通常 A 最终总是可以收到对所有发出的分组的确认。如果 A 不断重传分组但总是收不到确认，就说明通信线路太差，不能进行通信。 上述可靠传输协议称为自动重传请求ARQ(Automatic Repeat reQuest)。 意思是重传的请求是自动进行的，接收方不需要请求发送方重传某个出错的分组。 信道利用率 显而易见，每次只传一个分组有点太浪费了，完全没有充分利用信道。 想起 Amdahl 定律，我想我们应该意识到的一点是：现代社会如果想获得最大限度的发展，那就要用尽种种方法去压榨机器的潜能，提高机器的利用率。 为了提高传输效率，发送方可以不使用低效率的停止等待协议，而采用流水线传输。 流水线传输就是发送方可连续发送多个分组，不必每发完一个分组就停顿下来等待对方的确认。这样可使信道上一直有数据不间断地在传送。 当使用流水线传输时，就要使用下面将要介绍的连续ARQ协议和滑动窗口协议。 连续 ARQ 协议下图（a）表示发送方维持的发送窗口，它的意义是：位于发送窗口内的5个分组都可连续发送出去，而不需要等待对方的确认。 向前是指向着时间增大的方向，分组发送是按照分组序号从小到大发送的。 连续 ARQ 协议规定：发送方每收到一个确认，就把发送窗口向前滑动一个分组的位置。 上图（b）表示发送方收到了对第1个分组的确认，于是把发送窗口向前移动一个分组的位置。 接收方采用累积确认的方式。即，接收方不必对收到的分组逐个发送确认，而是在收到几个分组后，对按序到达的最后一个分组发送确认。这表示：到这个分组为止的所有分组都已正确收到了。 累积确认有优点也有缺点 优点：容易实现，即使确认丢失也不必重传。 缺点：不能向发送方反映出接收方已经正确收到的所有分组的信息。 比如，发送方发送了前 5 个分组，而第 3 个分组丢失了。接收方只能对前两个分组发出确认，发送方无法知道后面三个分组的下落，而只好把后面的三个分组都再重传一次。这就叫做 Go-back-N(回退 N 帧)，表示需要再退回来重传 已发送过的 N 个分组。 TCP 可靠传输的实现TCP报文段的首部格式TCP 虽然是面向字节流的，但 TCP 传送的数据单元却是报文段。一个TCP报文段分为首部和数据两部分，而 TCP 的全部功能都体现在它首部中各字段的作用。 TCP 报文段首部的前 20 个字节是固定的，后面有 4n 字节是根据需要而增加的选项(n是整数)。因此TCP首部的最小长度是 20 字节，而 UDP 首部 8 字节。 TCP 首部中的信息有很多，参考图片基本已经可以见字知意了。如果没有，那就要反思一下自己的六级了。 TCP 可靠传输的实现假定数据传输只在一个方向进行，即 A 发送数据，B 给出确认。但是不要忘记 A 与 B 的通信其实是全双工通信。 以字节为单位的滑动窗口TCP的滑动窗口是以字节为单位的。为方便，图中字节编号都取得很小。 假定 A 收到了 B 发来的确认报文段，其中窗口是 20 字节，而确认号是 31 (表明 B 期望收到的下一个序号是 31，而序号 30 为止的数据已经收到了)。根据这两个数据，A就构造出自己的发送窗口。 A 的发送窗口：在没有收到 B 的确认的情下，A 可以连续把窗口内的数据都发送出去。凡是已经发送过的数据，在未收到确认之前都必须暂时保留，以便在超时重传时使用。 接收方会把自己的接收窗口数值放在窗口字段(首部字段)中发送给对方。 因此，A 的发送窗口一定不能超过 B 的接收窗口数值。 发送方的发送窗口大小还要受到当时网络拥塞程度的制约。 发送窗口后沿的后面部分表示已发送且已收到了确认，不需要保留。而发送窗口前沿的前面部分不允许发送，因为接收方都没有为这部分数据保留临时存放的缓存空间。 发送窗口的位置由窗口前沿和后沿的位置共同确定。发送窗口后沿的变化情况 有且仅有两种可能，即不动(没有收到新的确认)和前移(收到了新的确认)，不能走回头路。 从所述可以看出，要描述一个发送窗口的状态需要三个指针：P1、P2和 P3，指针都指向字节的序号。 小于 $P_1$ 的是已发送并已收到确认的部分，而大于 $P_3$ 的是不允许发送的部分。 $P_3–P_1$=A的发送窗口 $P_2–P_1$=已发送但尚未收到确认的字节数 $P_3–P_2$=允许发送但当前尚未发送的字节数(又称为可用窗口或有效窗口) B 的接收窗口：B 只能对按序收到的数据中的最高序号给出确认，因此 B 发送的确认报文段中的确认号仍然是 31。 可能情况：发送窗口内所有的数据都已正确到达 B，B 也早已发出了确认。但所有确认都滞留在网络中。 在没有收到 B 的确认时，A 不能猜测：“或许 B 已经收到了？” 为了保证可靠传输，A 只能认为 B 还没有收到这些数据。于是，A 在经过一段时间后(超时计时器控制)就重传这部分数据，重新设置超时计时器，直到收到 B 的确认为止。 发送方的应用进程把字节流写入 TCP 的发送缓存，接收方的应用进程从 TCP 的接收缓存中读取字节流。 发送缓存用来暂时存放： 发送应用程序传送给发送方 TCP 准备发送的数据 TCP已发送出但尚未收到确认的数据 接收缓存用来暂时存放： 按序到达的、但尚未被接收应用程序读取的数据 未按序到达的数据 根据以上内容，需要注意三点： 虽然 A 的发送窗口是根据 B 的接收窗口设置的，但在同一时刻，A 的发送窗口并不总是和 B 的接收窗口一样大。 TCP 通常对不按序到达的数据是先临时存放在接收窗口中，等到字节流中所缺少的字节收到后，再按序交付上层的应用进程。 TCP 要求接收方必须有累积确认的功能，这样可以减小传输开销。 超时重传时间的选择报文段每重传一次，就把超时重传时间 RTO 增大一些。典型的做法是取新的重传时间为旧的重传时间的 2 倍。当不再发生报文段的重传时，才根据下式计算超时重传时间。$$RTO=RTT_s+4\\times RTT_d\\tag{1}$$$RTT_S$代表加权平均往返时间，$RTT_D$ 是 RTT 的偏差的加权平均值。$$新RTT_d=(1-\\beta)\\times (旧RTT_d)+\\beta \\times |RTT_s-新的RTT样本|\\tag{2}$$$\\beta$ 是小于 1 的系数，推荐值为 0.25。 选择确认SACK若收到的报文段无差错，只是未按序号，中间缺少某些序号的数据，那么能否设法只传送缺少的数据而不重传已经正确到达接收方的数据？ 选择确认(Selective ACK) 就是一种可行的方法。 RFC 2018 规定，如果要使用选择确认SACK，那么在建立TCP连接时，就要在TCP首部的选项中加上“允许SACK”的选项，双方必须事先商定好。 SACK 并没有指明发送方应当怎样响应 SACK，大多数的实现还是重传所有未被确认的数据块。 TCP的流量控制 利用滑动窗口实现流量控制 如果发送方把数据发送得过快，接收方就可能来不及接收，这就会造成数据的丢失。所谓流量控制(flow control)就是让发送方的发送速率不要太快，要让接收方来得及接收。 在上图最下面，B 向 A发送了零窗口的报文段。 如果 B 的接收缓存又有了一些存储空间，于是 B 向 A 发送了 rwnd=400 的报文段。然而这个报文段在传送过程中丢失了。A 一直等待收到 B 发送的非零窗口的通知，而 B 也一直等待A发送的数据。形成 死锁 局面。 为了解决这个问题，TCP 为每一个连接设有一个持续计时器(persistence timer)。只要 TCP 连接的一方收到对方的零窗口通知，就启动持续计时器。 若持续计时器设置的时间到期，就发送一个零窗口探测报文段(仅携带1字节的数据)，而对方就在确认这个探测报文段时给出现在的窗口值。如果窗口仍然是零，那么收到这个报文段的一方就重新设置持续计时器。如果窗口不是零，死锁的僵局就可以打破。 TCP的拥塞控制 拥塞(congestion)：在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。 $$\\sum对资源的需求&gt;可用资源\\tag{3}$$ 要进行拥塞控制，实际上就是寻求打破表达式(3) 的条件。 TCP 进行拥塞控制的算法有四种，即 慢开始(slow-start) 拥塞避免 (congestion avoidance) 快重传(fast retransmit) 快恢复(fast recovery) 仍通之前一样，假定： 数据是单方向传送的，对方只传送确认报文。 接收方总是有足够大的缓存空间，因而发送窗口的大小由网络的拥塞程度 来决定。 慢开始和拥塞避免发送方控制拥塞窗口： 只要网络没有出现拥塞，拥塞窗口就可以再增 大一些，以便把更多的分组发送出去，这样就可以提高网络的利用率。 只要网络出现拥塞或有可能出现拥塞，就必须把拥塞窗口减小一些，以减少注入到网络中的分组数，以便缓解网络出现的拥塞。 发送方确认网络拥塞： 网络发生拥塞时，路由器就要丢弃分组。 因此，只要发送方没有按时收到应当到达的确认报文，也就是说，只要出现了超时，就可以猜想网络可能出现了拥塞。 慢开始算法的思路: 当主机开始发送数据时，由于并不清楚网络的负荷情况，所以如果立即把大量数据字节注入到网络，那么就有可能引起网络发生拥塞。经验证明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是说，由小到大逐渐增大拥塞窗口数值。 慢开始规定，在每收到一个对新的报文段的确认后，可以把拥塞窗口增加最多 一个最大报文段SMSS(Sender Maximum Segment Size)的数值。 使用慢开始算法后，每经过一个传输轮次(transmission round)，拥塞窗口 cwnd 就加倍。(指数增长) 为了防止拥塞窗口 cwnd 增长过大引起网络拥塞，还需要设置一个慢开始门限 ssthresh 状态变量。 当cwnd&lt;ssthresh时，使用慢开始算法。 当cwnd&gt;ssthresh时，停止使用慢开始算法，改用拥塞避免算法。 当cwnd=ssthresh时，既可使用慢开始算法，也可使用拥塞避免算法。 拥塞避免算法的思路是让拥塞窗口cwnd缓慢地增大，即每经过一个往返时间 RTT就把发送方的拥塞窗口 cwnd 加 1。(线性增长) 快重传与快恢复快重传算法：要求接收方不要等待自己发送数据时才进行捎带确认，而是要立即发送确认，即使收到了失序的报文段也要立即发出对已收到的报文段的重复确认。 快重传算法规定，发送方只要一连收到 3 个重复确认，就知道接收方确实没有收到报文段 $M_3$，因而应当立即进行重传(即“快重传”)，这样就不会出现超时，发送方也不就会误认为出现了网络拥塞。 快恢复：是把快恢复开始时的拥塞窗口 cwnd 值再增大一些 (增大3个报文段的长度)，即等于新的ssthresh+3xMS。 拥塞避免流程图 TCP 可靠传输的总结 首先，采用三次握手来建立 TCP 连接，四次握手来释放 TCP 连接，从而保证建立的传输信道可靠。 其次，TCP 采用了停止等待协议与连续 ARQ 协议（回退N，Go-back-N；超时自动重传）来保证数据传输的正确性。 使用滑动窗口协议来保证接方能够及时处理所接收到的数据，进行流量控制。 最后，TCP 使用慢开始、拥塞避免、快重传和快恢复来进行拥塞控制，避免网络拥塞。 也看到网上比较流行的另外一个版本的总结： TCP 通过序列号、超时重传、检验和、流量控制、滑动窗口、拥塞控制实现可靠性。 应用数据被分割成 TCP 认为最适合发送的数据块。 TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 TCP 的接收端会丢弃重复的数据。 超时重传：当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。 校验和：TCP 将保持它首部和数据的检验和，发送的数据包的二进制相加然后取反。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP将丢弃这个报文段和不确认收到此报文段。 流量控制：TCP 连接的每一方都有固定大小的缓冲空间，TCP 的接收端只允许发送端发送接收端缓冲区能接纳的我数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。接收方有即时窗口（滑动窗口），随 ACK 报文发送。（TCP 利用滑动窗口实现流量控制） 滑动窗口：实际中的传输方式。 拥塞控制：当网络拥塞时，减少数据的发送。发送方有拥塞窗口，发送数据前比对接收方发过来的即使窗口，取小慢启动、拥塞避免、拥塞发送、快速恢复。应用数据被分割成TCP认为最适合发送的数据块，TCP 的接收端会丢弃重复的数据。 停止等待协议也是为了 TCP 协议传输稳定可靠，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 参考 《计算机网络》第七版，谢希仁。 GitHub - TCP 如何保证可靠传输","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://raymond-zhao.top/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"http://raymond-zhao.top/tags/TCP/"}]},{"title":"HTTP 与 HTTPS","slug":"2020-08-25-Network-HTTPSAndHTTP","date":"2020-08-24T21:20:20.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/08/25/2020-08-25-Network-HTTPSAndHTTP/","link":"","permalink":"http://raymond-zhao.top/2020/08/25/2020-08-25-Network-HTTPSAndHTTP/","excerpt":"","text":"分层体系OSI 的七层协议体系结构的概念清楚，理论也较完整，但它既复杂又不实用。TCP/IP体系结构则不同，但它现在却得到了非常广泛的应用。 TCP/IP是一个四层的体系结构，它包含应用层、运输层、网际层和网络接口层。 应用层(application layer)：应用层的任务是通过应用进程间的交互来完成特定网络应用。应用层协议定义的是应用进程间通信和交互的规则。在互联网中的应用层协议很多，如域名系统DNS，支持万维网应用的HTTP协议，支持电子邮件的SMTP协议等等。应用层交互的数据单元称为报文(message)。 运输层(transport layer)：运输层的任务就是负责向两台主机中进程之间的通信提供通用的数据传输服务。应用进程利用该服务传送应用层报文。所谓“通用的”，是指并不针对某个特定网络应用，而是多种应用可以使用同一个运输层服务。由于一台主机可同时运行多个进程，因此运输层有复用和分用的功能。复用就是多个应用层进程可同时使用下面运输层的服务，分用和复用相反，是运输层把收到的信息分别交付上面应用层中的相应进程。运输层主要使用以下两种协议: 传输控制协议TCP(Transmission Control Protocol)—提供面向连接的、可靠的数据传输服务，其数据传输的单位是报文段(segment)。 用户数据报协议UDP(User Datagram Protocol)—提供无连接的、尽最大努力(best-effort)的数据传输服务(不保证数据传输的可靠性)，其数据传输的单位是用户数据报。 网络层：网络层负责为分组交换网上的不同主机提供通信服务。在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组或包进行传送。在TCP/IP体系中，由于网络层使用IP协议，因此分组也叫做IP数据报，或简称为数据报。 网络层的另一个任务就是要选择合适的路由，使源主机运输层所传下来的分组，能够通过网络中的路由器找到目的主机。 不要将运输层的“用户数据报UDP”和网络层的“IP数据报”弄混。此外，无论在哪一层传送的数据单元，都可笼统地用“分组”来表示。 数据链路层(data link layer)：两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。在两个相邻结点之间传送数据时，数据链路层将网络层交下来的IP数据报组装成帧(framing)，在两个相邻结点间的链路上传送帧(frame)。每一帧包括数据和必要的控制信息(如同步信息、地址信息、差错控制等)。 在接收数据时，控制信息使接收端能够知道一个帧从哪个比特开始和到哪个比特结束。 控制信息还使接收端能够检测到所收到的帧中有无差错。 物理层(physical layer)：考虑的是怎样在传输媒体上透明地传输数据比特流，而不是指具体的传输媒体。物理层的作用是尽可能屏蔽传输媒体和通信手段的差异，使数据链路层感觉不到这些差异。 请求方法 问题：POST 方法与 PUT 方法的区别? 方法名 作用 GET 向指定的资源发出“显示”请求。 PATCH 用于将局部修改应用到资源。 POST 向指定资源提交数据，请求服务器进行处理。 PUT 向指定资源位置上传其最新内容。 DELETE 请求服务器删除Request-URI所标识的资源。 HEAD 与GET方法一样，都是向服务器发出指定资源的请求。只不过服务器将不传回资源的本文部分。 TRACE 回显服务器收到的请求，主要用于测试或诊断。 OPTIONS 可使服务器传回该资源所支持的所有HTTP请求方法。 CONNECT 通常用于SSL加密服务器的链接 GET 与 POST的区别 比较方面 GET POST 请求主体(Body) 只有URL，无请求主体 有请求主体 响应主体 有 有 后退按钮/刷新 无副作用 数据会被重新提交 书签 可收藏为书签 不可收藏为书签 缓存 可被缓存 不能缓存 编码类型 application/x-www-form-urlencoded application/x-www-form-urlencodedmultipart/form-datatext/plain 历史 参数保留在浏览器历史中 参数不会保存在浏览器历史中 URL长度 有限制，但不同浏览器限制又不同。常说的2KB其实是指IE8，Chrome和Apache为8KB 无限制(Post请求本身不限制长度，但是服务器是会限制的。) 对数据类型的限制 只允许 ASCII 字符 没有限制、也允许二进制数据，Percent Encoding编码 安全性 与 POST 相比，GET 的安全性较差，因为所发送的数据是 URL 的一部分。在发送密码或其他敏感信息时绝不要使用 GET POST 比 GET 更安全，因为参数不会被保存在浏览器历史或 web 服务器日志中。 幂等性 支持 不支持 可见性 数据在 URL 中对所有人都是可见的 数据不会显示在 URL 中 状态码所有HTTP响应的第一行都是状态行，依次是当前HTTP版本号，3位数字组成的状态代码，以及描述状态的短语，彼此由空格分隔。 状态代码的第一个数字代表当前响应的类型： 1xx消息——请求已被服务器接收，继续处理 2xx成功——请求已成功被服务器接收、理解、并接受 200: 正常返回信息 3xx重定向——需要后续操作才能完成这一请求 300 Multiple Choices 301 Moved Permanently: 资源已经被永久移动，通常会发送HTTP Location来重定向到正确的新位置。Google 认为这是将网站从HTTP迁移到HTTPS的最佳方法。 302 Found: 临时重定向。 当 301、302、303 响应状态码返回时，几乎所有的浏览器都会把 POST 改成 GET，并删除请求报文内的主体，之后请求会自动再次 发送。 301、302 标准是禁止将 POST 方法改变成 GET 方法的，但实际使 用时大家都会这么做。 304 Not Modified: 表示客户端发送附带条件的请求时，服务器端允许请求访问资源，但未满足条件的情况。304 状态码返回时，不包含任何响应的主体部分。304 虽然被划分在 3XX 类别中，但是和重定向没有关系。 附带条件的请求是指采用 GET 方法的请求报文中包含 If-Match，If-Modified- Since，If-None-Match，If-Range，If-Unmodified-Since 中任一首部。 4xx请求错误——请求含有词法错误或者无法被执行 401 Unauthorized：请求未经授权，即用户没有权限执行这个请求。 403 Forbidden：服务器收到请求，但是拒绝提供服务 404 Not Found：请求资源不存在。最常见的就是URL错误 5xx服务器错误——服务器在处理某个正确请求时发生错误 500 Internal Server Error：服务器发生不可预期的错误 502 Bad Gateway：作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。 503 Service Unavailable：服务器当前不能处理客户端的请求，一段时间后可能恢复正常。 HTTP的缺点 通信使用明文(不加密)，内容可能会被窃听。 不验证通信方的身份，因此有可能遭遇伪装。 无法证明报文的完整性，所以有可能已遭篡改。 HTTP各版本区别HTTP1.0 短连接：每次发送请求都要重新建立 TCP 连接，即三次握手，数据发送完毕就要关闭连接。 无 host 头域，也就是 http 请求头里的 host。 不允许断点续传，而且不能只传输对象的一部分，要求传输整个对象。 HTTP1.1 引入了持久连接(persistent connectio)，又称长连接。客户端和服务器发现对方一段时间没有活动，可以主动关闭连接。 引入了流水线机制(pipelining)，也就是在同一个TCP连接里面，客户端可以同时发送多个请求。 采用”流模式”（stream）取代”缓存模式”（buffer），采用分块传输编码，产生一块数据，就发送一块。 客户端请求的头信息新增了 Host 字段，用来指定服务器的域名。 新增许多方法：PUT、PATCH、HEAD、 OPTIONS、DELETE。 将 Content-length 字段的作用进行扩充，声明本次回应(Response)的数据长度。 虽然 HTTP1.1 允许复用 TCP 连接，但是同一个 TCP 连接里面，所有的数据通信是按次序进行的。服务器只有处理完一个回应，才会进行下一个回应。要是前面的回应特别慢，后面就会有许多请求排队等着。这称为”队头堵塞”（Head-of-line blocking）。 为了避免这个问题，只有两种方法：一是减少请求数，二是同时多开持久连接。 这产生了很多的网页优化技巧，比如合并脚本和样式表、将图片嵌入CSS代码、域名分片（domain sharding）等等。如果HTTP协议设计得更好一些，这些额外的工作是可以避免的。 HTTP1.x的缺点 HTTP/1.0一次只允许在一个TCP连接上发起一个请求，HTTP/1.1 使用的流水线技术也只能部分处理请求并发，仍然会存在队列头阻塞问题，因此客户端在需要发起多次请求时，通常会采用建立多连接来减少延迟。 单向请求，只能由客户端发起。 请求报文与响应报文首部信息冗余量大。 数据未压缩，导致数据的传输量大。 HTTP2.0 主要基于SPDY协议(Google开发的基于TCP协议的应用层协议) 核心思想是尽量减少TCP连接数。 目标是优化HTTP协议的性能，通过压缩、多路复用和优先级等技术，缩短网页的加载时间并提高安全性。 在不改动HTTP语义、方法、状态码、URI及首部字段的情况下，大幅度提高了 Web 性能。 改用二进制传输，HTTP1.x 使用文本传输。 在应用层与传输层之间增加一个二进制分帧层，在二进制分帧层上，HTTP2.0 会将所有传输的信息分为更小的消息和帧，并采用二进制格式编码。 其中 HTTP1.x 的首部信息会被封装到 Headers帧，而Request Body则封装到 Data帧。 头部(Header)压缩： 使用 HPACK (头部压缩算法)压缩格式对传输的 header 进行编码，减小了 header 的体积。 在两端维护了索引表(Map)，用于记录出现过的header，后面在传输过程中就可以传输已经记录过的 header 的键名，对端收到数据后就可以通过键名找到对应的值。 多路复用：使用多个流(stream)，每个流分帧传输，使得一个 TCP 连接能够处理多个 HTTP 请求，避免 HTTP1.x版本的队头阻塞问题。 服务器端推送Push：服务端可以在客户端某个请求后，主动推送其他资源。 更安全：使用了 TLS 的 拓展ALPN 作为协议升级，除此之外，HTTP2.0 对 TLS 的安全性做了近一步加强，通过黑名单机制禁用了几百种不再安全的加密算法。 HTTP3.0 基于 Google 的 QUIC 协议，利用UDP实现可靠数据传输。 减少了 TCP 三次握手时间，减少了 TLS 握手时间。 解决了 HTTP 2.0 中前一个 stream 丢包导致后一个 stream 被阻塞的问题。 优化了重传策略，重传包和原包的编号不同，降低后续重传计算的消耗。 连接迁移，不再用 TCP 四元组确定一个连接，而是用一个 64 位随机数来确定这个连接。 更合适的流量控制。 HTTP 与 FTP 关于文件上传： 断点续传 Feature HTTP FTP 侧重点 用来浏览网站、更多的是为终端用户提供文件传输，比如电影、图片、音乐。 用来访问和传输文件，FTP 文件传输常见于批量上传和维护网站。 客户端 通常是浏览器 命令行或特定的图形界面 头部 包含 meta-data，比如最后修改日期、编码方式、服务器名称等。 不支持 数据格式 只支持二进制格式文件 支持ASCII与二进制 流水线 支持流水线，这就意味着客户端可以在上一个请求处理完之前，发出下一个请求，其结果就是多次请求数据之前省掉了部分服务器客户端往返时延。 不支持 动态端口 在双向传输中使用动态端口 使用两个连接，第一个连接用来发送控制指令，当接收或者发送数据的时候，又打开第二个TCP连接。 持久连接(长连接) 可以维护一个单个的连接并使用它进行任意数量的数据传输。 每次有数据的需要时都创建一个新的连接。 压缩算法 提供了在一些压缩算法中客户端和服务器共同协商选择的办法，比如 gzip。 不存在这种算法 对代理的支持 支持代理，这种功能是构建在协议里。 不支持 面向的对象 没有这个概念 面向文件，ftp 可以通过命令列出远程服务器的目录列表。 文件大小 比较适合上传小文件 比较适合上传大文件，不需要将文件全部载入内存中。 什么使 FTP 服务更快 发送数据中没有meta-data，仅传输原始的二进制文件。 没有过度的分块编码。 什么使HTTP服务更快？ 重用已存在的持久连接，从而有更好的TCP表现。 流水线的支持使得从同一个服务器上请求多个文件更快。 自动的压缩机制使得传输的数据更少。 没有命令/应答机制最大限度的减少了往返时延。 HTTPS的工作流程如何验证公钥证书 简易版的HTTPS工作流程 客户端发起HTTPS请求，连接到 Server 的 443 端口。 服务器检测并选择客户端能支持的加密方式 传送证书 服务器将自己的数字证书的部分信息返回给客户端，其中包括公钥，以及支持的SSL 版本等信息。 客户端解析证书 客户端首先会验证公钥是否有效，如果发现异常，则弹出警告。否则那么就生成一个随机值（Premaster Secret），然后用公钥对该随机值进行加密。 传送加密信息 传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。 服务端解密信息 服务端用与公钥配对的私钥解密后，得到客户端传过来的随机值。从客户端生成随机值到服务器解密获取随机值这个过程称为非对称加密。 传输加密后的信息 服务器用刚刚解密得到的随机值加密需要传输的内容，并将加密后的信息发送给客户端。 客户端解密信息 客户端用之前生成的随机值解密服务器传过来的信息，获取实际传输的内容。这个过程称为对称加密。 SSL 协商过程 继续借用《图解HTTP》中的图文。 HTTP连接需要交换3个包，而HTTPS连接需要交换 3(TCP) + 9 个包(SSL/TLS连接)。 这里说的加密过程是指 SSL 的加密，至于 TLS 的加密… 客户端发送 Client Hello 报文开始 SSL 通信。报文中包含客户端支持的 SSL 的指定版本、加密组件列表(所使用的加密算法及密钥长度等)。 服务器确定可进行 SSL 通信时，以 Server Hello 报文作为应答。报文中包含 SSL 版本以及加密组件(Cipher Suite)。服务器的加密组件内容是从接收到的客户端加密组件内筛选出来的。 服务器继续发送 Certificate 报文。报文中包含公钥证书。 服务器继续发送 ServerHelloDone 报文。通知客户端第一阶段的 SSL 握手协商部分结束。 客户端发送 Client Key Exchange 报文作为回应。报文中包含通信加密中一种被称为 Pre-master secret的随机密码串。这个报文已用第 3 步中的公钥进行加密。 客户端继续发送 Change Cipher Spec 报文。该报文会提示服务器，在此报文之后的通信都将会采用 Premaster secret秘钥加密。 客户端发送 Finished 报文。该报文包含连接至今，全部报文的整体校验值。此次握手协商能否成功，取决于服务器能否正确解密该报文。 服务器同样发送 Change Cipher Spec 报文。 服务器继续发送 Finished 报文。 服务器与客户端交换完 Finished 报文之后，SSL 连接建立成功，通信将会受到 SSL 的保护。从此处开始进行应用层协议的通信，即发送 HTTP 请求。 应用层协议通信，即发送 HTTP 响应。 由客户端断开连接。发送 close_notify 报文，之后发送 TCP FIN 报文来关闭与 TCP 的通信。 下面是对整个流程的图解。图中说明了从仅使用服务器端的公开密钥证书(服务器证书)建立 HTTPS 通信的整个过程。 SSL 与 TLS SSL(Secure Sockets Layer安全套接层) TLS (Transport Layer Security 传输层安全性协议) TLS 与 SSL 的差异 版本号：TLS 记录格式与 SSL 记录格式相同，但版本号的值不同，TLS1.0 使用的版本号为 SSLv3.1。 报文鉴别码：SSLv3.0 和 TLS 的 MAC 算法及 MAC 计算的范围不同。TLS 使用 HMAC 算法。SSLv3.0使用了相似的算法，两者差别在于 SSLv3.0 中，填充字节与密钥之间采用的是连接运算，而 HMAC 算法采用的是异或运算。 伪随机函数：TLS 使用了称为 PRF 的伪随机函数来将密钥扩展成数据块。 报警代码：TLS 在支持几乎所有的SSLv3.0报警代码的基础上，补充定义了很多报警代码，如解密失败（decryption_failed）、记录溢出（record_overflow）、未知CA（unknown_ca）、拒绝访问（access_denied）等。 密文族和客户证书：SSLv3.0 和 TLS 存在少量差别，TLS 不支持 Fortezza 密钥交换、加密算法和客户证书。 certificate_verify 和 finished 消息：SSLv3.0 和 TLS 在用 certificate_verify 和 finished 消息计算 MD5 和 SHA-1 散列码时，计算的输入有少许差别，但安全性相当。 加密计算：TLS 与 SSLv3.0 在计算主密值(master secret)时采用的方式不同。 填充：用户数据加密之前需要增加的填充字节。 在 SSL 中，填充后的数据长度要达到密文块长度的最小整数倍。 在 TLS 中，填充后的数据长度可以是密文块长度的任意整数倍（但填充的最大长度为255字节），这种方式可以防止基于对报文长度进行分析的攻击。 HTTP与HTTPS的区别 方面 HTTP HTTPS 开发目的 发布和接收 HTML 页面浏览器和网站服务器之间传递信息 提供对网站服务器的身份认证保护交换数据的隐私与完整性。 连接端口 80 443 安全协议 无 SSL/TLS 加密方式 无 对称加密、非对称加密、哈希算法、数字签名 安全性 明文传输、相对较低 加密传输、相对较高 建立连接 TCP三次握手交换3个包 TCP3个包+SSL9个包 响应速度 相对较快 相对较慢(比HTTP慢2到100倍) 所需费用 免费 可能需要购买证书 Cookie 与 Session由于 HTTP 协议是无状态的协议，每一个 HTTP 请求都是互相独立的，服务器分辨不出两次请求是否来自同一个浏览器或同一位用户。 所以服务端需要记录用户的状态时，就需要用某种机制来识具体的用户，这个机制就是 Session。 Cookie 简介 浏览器第一次连接服务器时 Cookie 是不存在的 服务器响应客户端时，会在 Header 中设置 Set-Cookie，服务器收到 Cookie 后存在本地。每一个 Cookie 都是 name-value 对。 此后浏览器再连接服务器，都会包含上 Cookie，服务器可以用它来关联特定的请求。 Cookie里有什么？ Name and data. 数据大小通常被浏览器所限定，一般小于 4KB。 一个服务器可以通过不同的 Name 定义多个 Cookie，但是浏览器限制了每一个服务器可以拥有的 Cookie 数量(50个左右)。 当前 Cookie 的 Domain(作用域)信息：Server、port(可选)、URL 前缀(可选)。Cookie 只会被包含在匹配的作用域的请求之中。 过期时间：浏览器可以删除老旧的 Cookies。 工作流程 由服务器发给客户端的特殊信息，以文本的形式存放在客户端 客户端再次请求的时候，会把 Cookie 回发 服务器收到后，会解析 Cookie 生成与客户端相对应的内容 Session简介 Cookie 是服务器用来实现 Session 的一种方式 Cookie 中包含了 Session 的表示，通常被称为 SessionID。 Session 的管理 可以储存在内存里 或者储存在磁盘中的文件中 或者存在数据库中 简单工作流程 保存在服务器上的信息 解析客户端请求并操作 SessionId ，按需保存状态信息。 实现方式 使用 Cookie 来实现 当客户端禁用 Cookie 时，可以使用 URL 回写来实现 Cookie 和 Session 的区别 对比角度 Cookie Session 存放位置 位于客户端的一段文本、包含用户信息，是 Session 的一种实现方式。 位于服务器的文件，包含用户信息。在 Tomcat 中使用 StandardSession 实现，使用到的数据结构是 ConcurrentHashMap。 生存时间 可以手动设定 当关闭浏览器时就消失 容量大小 一般最大为 4KB 理论上可以存储随意多的信息，唯一的限制是一个脚本一次最大能占用的内存，默认是128MB 依赖性 Cookie 不依赖于 Session，Cookie 是 Session 的一种实现方式。 Session 依赖于 Cookie，当 Cookie 禁用时，可以通过 URL 回写来实现。 安全性 相对较低 相对较高 参考 《计算机网络》第七版，谢希仁。 《图解HTTP》上野 宣、于均良 译。 掘金 - Cookie 和 Session 关系和区别 简书 - 看完就彻底懂了session和cookie Stanford University - CS 142: Web Applications (Fall 2010)","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://raymond-zhao.top/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://raymond-zhao.top/tags/HTTP/"},{"name":"HTTPS","slug":"HTTPS","permalink":"http://raymond-zhao.top/tags/HTTPS/"}]},{"title":"volatile底层实现原理","slug":"2020-08-19-volatile","date":"2020-08-20T15:33:42.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/08/20/2020-08-19-volatile/","link":"","permalink":"http://raymond-zhao.top/2020/08/20/2020-08-19-volatile/","excerpt":"","text":"volatile底层实现原理《Java并发编程的艺术》这本书的每一节基本上都是面试中的高频考点或者是重难点，实体书在书桌上摆了许久竟然都忽视掉了，现在必须要把这本书整理成电子版的。 volatile的应用volatile是轻量级的 synchronized，它在多处理器开发中保证了共享变量的“可见性”。volatile 变量不会引起线程上下文的切换和调度。 可见性：当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。 volatile的定义与实现原理在继续深入原理之前，需要先了解几个 CPU 术语。 对于 volatile 是如何保证可见性的，需要查看 JIT 编译器生成的汇编指令来查看对 volatile 进行写操作时，CPU会做什么事情。 1instance = new Singleton(); // instance是volatile变量 转换成汇编代码 120x01a3de1d: movb $0×0,0×1104800(%esi);0x01a3de24: lock addl $0×0,(%esp); 被 volatile 变量修饰的共享变量进行写操作的时候会多出第二行汇编代码，其中 lock是关键。Lock 前缀的指令在多核处理器下会引发了两件事情： 将当前处理器缓存行的数据写回到系统内存。 这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。 volatile的两条实现原则： Lock前缀指令会引起处理器缓存回写到内存。 一个处理器的缓存回写到内存会导致其他处理器的缓存无效。处理器使用嗅探技术保证它的内部缓存、系统内存和其他处理器的缓存的数据在总线上保持一致。 指令序列的重排序类型 编译器优化的重排序：编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序：如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序： 从Java源代码到最终实际执行的指令序列，会分别经历下面3种重排序： 上述的1属于编译器重排序，2和3属于处理器重排序。 对于编译器重排序：JMM的编译器重排序规则会禁止特定类型的编译器重排序(不是所有的编译器重排序都要禁止)。 对于处理器重排序：JMM的处理器重排序规则会要求Java编译器在生成指令序列时，插入特定类型的内存屏障(Memory Barriers，Intel称之为 Memory Fence)指令，通过内存屏障指令来禁止特定类型的处理器重排序。 屏障类型 指令示例 说明 LoadLoad Barriers Load1;LoadLoad;Load2 确保 Load1 数据的装载先于 Load2 及所有后续装载指令的装载 StoreStore Barriers Store1;StoreStore;Store2 确保 Store1数据对其他处理器可见（刷新到内存）先于 Store2 及所有后续存储指令的存储 LoadStore Barriers Load1;LoadStore;Store2 确保Load1数据装载先于 Store2 及所有后续的存储指令刷新到内存 StoreLoad Barriers Store1;StoreLoad;Load2 确保 Store1 数据对其他处理器变得可见（指刷新到内存）先于 Load2 及所有后续装载指令的装载。StoreLoad Barriers 会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令。 StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他3个屏障的效果。 volatile的特性 可见性：对一个volatile变量的读，总是能看到(任意线程)对这个volatile变量最后的写入。 原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种复合操作不 具有原子性。 从内存语义的角度来说，volatile的写-读与锁的释放-获取有相同的内存效果；volatile写和锁的释放有相同的内存语义；volatile读与锁的获取有相同的内存语义。 volatile的内存语义 volatile写：JMM会把该线程对应的本地内存中的共享变量值刷新到主内存。实质上是线程A向接下来将要读这个volatile变量的某个线程发出了(其对共享变量所做修改的)消息。 volatile读：JMM会把该线程对应的本地内存置为无效，然后再从主内存中读取共享变量。实质上是线程B接收了之前某个线程发出的(在写这个volatile变量之前对共享变量所做修改的)消息。 线程A写一个volatile变量，随后线程B读这个volatile变量，实质上是线程A通过主内存向线程B发送消息。 volatile内存语义的实现重排序分为编译器重排序和处理器重排序。为了实现volatile内存语义，JMM 会分别限制这两种类型的重排序类型。 从中可以看出： 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后。 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前。 当第一个操作是volatile写，第二个操作是volatile读时，不能重排序。 为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。 在每个volatile写操作的前面插入一个StoreStore屏障 在每个volatile写操作的后面插入一个StoreLoad屏障 在每个volatile读操作的后面插入一个LoadLoad屏障 在每个volatile读操作的后面插入一个LoadStore屏障 由于volatile仅仅保证对单个volatile变量的读/写具有原子性，而锁的互斥执行的特性可以确保对整个临界区代码的执行具有原子性。在功能上，锁比volatile更强大；在可伸缩性和执行性能上，volatile更有优势。","categories":[{"name":"多线程与并发","slug":"多线程与并发","permalink":"http://raymond-zhao.top/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"volatile","slug":"volatile","permalink":"http://raymond-zhao.top/tags/volatile/"}]},{"title":"JVM-垃圾收集器与内存分配策略","slug":"2020-08-19-JVM-GC","date":"2020-08-19T05:58:52.000Z","updated":"2023-03-16T06:22:32.000Z","comments":true,"path":"2020/08/19/2020-08-19-JVM-GC/","link":"","permalink":"http://raymond-zhao.top/2020/08/19/2020-08-19-JVM-GC/","excerpt":"","text":"垃圾收集器与内存分配策略如何判断对象已死？引用计数法在对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加一；当引用失效时，计数器值就减一；任何时刻计数器为零的对象就是不可能再被使用的。 引用计数难以解决对象之间相互循环引用的问题，如 a.child = b; b.child = a;。 可达性分析算法通过一系列称为“GC Roots”的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过程所走过的路径称为“引用链”(Reference Chain)，如果某个对象到 GC Roots 间没有任何引用链相连，或者用图论的话来说就是从 GC Roots 到这个对象不可达时，则证明此对象是不可能再被使用的。 可作为 GC Roots 的对象包括： 在虚拟机栈(栈帧中的本地变量表)中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等。 在方法区中类静态属性引用的对象，譬如 Java 类的引用类型静态变量。 在方法区中常量引用的对象，譬如字符串常量池（String Table）里的引用。 在本地方法栈中 JNI（即通常所说的 Native 方法）引用的对象。 Java 虚拟机内部的引用，如基本数据类型对应的 Class 对象，一些常驻的异常对象(比如 NPE、OOM)等，还有系统类加载器。 所有被同步锁(synchronized 关键字)持有的对象 反映 Java 虚拟机内部情况的 JMXBean、JVM TI 中注册的回调、本地代码缓存等。 并发时的可达性分析可达性分析算法理论上要求全过程都基于一个能保障一致性的快照中才能够进行分析，这意味着必须全程冻结用户线程的运行。在根节点枚举时，由于 GC Roots 相比整个 Java 堆中全部的对象毕竟还算是极少数，且在各种优化技巧（如 OopMap）的加持下，它带来的停顿已经是非常短暂且相对固定（不随堆容量而增长）的了。 为什么必须在一个能保障一致性的快照上才能进行对象图的遍历？引入三色标记（Tri-color Marking）作为工具来辅助推导，把遍历对象图过程中遇到的对象，按照“是否访问过”这个条件标记成以下三种颜色： 白色：表示对象尚未被垃圾收集器访问过，显然在可达性分析刚刚开始的阶段，所有的对象都是白色的，若在分析结束的阶段，仍然是白色的对象，即代表不可达。 黑色：表示对象已经被垃圾收集器访问过，且这个对象的所有引用都已经扫描过。黑色的对象代表已经扫描过，它是安全存活的，如果有其他对象引用指向了黑色对象，无须重新扫描一遍。黑色对象不可能直接（不经过灰色对象）指向某个白色对象。 灰色：表示对象已经被垃圾收集器访问过，但这个对象上至少存在一个引用还没有被扫描过。 如果用户线程与收集器并发工作呢？收集器在对象图上标记颜色，同时用户线程在修改引用关系—即修改对象图的结构，这样可能出现两种后果。 把原本已消亡的对象错误标记为存活（浮动垃圾，可清理） 把原本存活的对象错误标记为已消亡 Wilson 于 1994 年在理论上证明了，当且仅当以下两个条件同时满足时，会产生“对象消失”的问题，即原本应该是黑色的对象被误标为白色： 赋值器插入了一条或多条从黑色对象到白色对象的新引用； 赋值器删除了全部从灰色对象到该白色对象的直接或间接引用。 显然，要解决并发扫描时的对象消失问题，只需破坏二者之一即可。由此分别产生了两种解决方案：增量更新（Incremental Update）和原始快照（Snapshot At The Beginning, SATB）。 增量更新：破坏第一个条件。当黑色对象插入新的指向白色对象的引用关系时， 将新插入的引用记录下来 在并发扫描结束之后，再以这些记录过的引用关系中的黑色对象为根，重新扫描一次。 可以理解为，黑色对象一旦新插入了指向白色对象的引用之后，它就变回灰色对象了(白加黑会变灰)。 原始快照：破坏第二个条件。当灰色对象要删除指向白色对象的引用关系时， 将要删除的引用记录下来 在并发扫描结束之后，再以这些记录过的引用关系中的灰色对象为根，重新扫描 一次。 可以理解为，无论引用关系删除与否，都会按照刚刚开始扫描那一刻的对象图快照来进行搜索。 无论是对引用关系记录插入还是删除，虚拟机的记录操作都是通过写屏障实现的。 CMS 基于增量更新实现并发标记，G1、Shenandoah 基于原始快照实现。 四大引用无论是通过引用计数算法判断对象的引用数量，还是通过可达性分析算法判断对象是否引用链可达，判定对象是否存活都和“引用”离不开关系。 引用分为强引用（Strongly Reference）、软引用（Soft Reference）、弱引用（Weak Reference）和虚引用（Phantom Reference）4 种，这 4 种引用强度依次逐渐减弱。 强引用(Strong Reference) 最普遍的引用：Object obj = new Object(); 抛出 OutOfMemoryError 终止程序也不会回收具有强引用的对象 通过将对象设置为 null 来软化引用，使其被回收。 软引用(Soft Reference) 对象处在有用但非必须的状态 只有当内存空间不足时，GC 才会回收该引用的对象的内存。 可以用来实现高速缓存 弱引用(Weak Reference) 非必须的对象，比软引用更弱一些。 GC 时会被回收 被回收的概率也不大，因为 GC 线程优先级比较低。 适用于引用偶尔被使用且不影响垃圾收集的对象 虚引用(Phantom Reference) 不会决定对象的生命周期 任何时候都可能被 GC 回收 跟踪对象被垃圾收集器回收的活动，起哨兵的作用。 必须和引用队列 ReferenceQueue 联合使用 引用类型 被垃圾回收时间 用途 生存时间 强引用 从来不会 对象的一般状态 JVM停止运行时终止 软引用 内存不足时 对象缓存 内存不足时终止 弱引用 垃圾回收时 对象缓存 GC 运行后终止 虚引用 不确定 标记、哨兵 不确定 引用队列(Reference Queue) 无实际存储结构，存储逻辑依赖于内部节点之间的关系来表达。 存储关联的且被 GC 的软引用，弱引用以及虚引用。 To be, or not to be.即使在可达性分析算法中判定为不可达的对象，也不是“非死不可”的，这时候它们暂时还处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历两次标记过程： 如果对象在进行可达性分析后发现没有与 GC Roots 相连接的引用链，那它将会被第一次标记。 随后进行一次筛选，筛选的条件是此对象是否有必要执行 finalize() 方法。假如对象没有覆盖 finalize() 方法，或者 finalize() 方法已经被虚拟机调用过，那么虚拟机将这两种情况都视为“没有必要执行”。 如果这个对象被判定为确有必要执行 finalize() 方法，那么该对象将会被放置在一个名为 F-Queue 的队列之中，并在稍后由一条由虚拟机自动建立的、低调度优先级的 Finalizer 线程去执行它们的 finalize() 方法。 finalize() 方法是对象逃脱死亡命运的最后一次机会，稍后收集器将对 F-Queue 中的对象进行第二次小规模的标记，如果对象要在 finalize() 中成功拯救自己 — 只要重新与引用链上的任何一个对象建立关联即可，譬如把自己（this关键字）赋值给某个类变量或者对象的成员变量，那在第二次标记时它将被移出“即将回收”的集合，如果对象这时候还没有逃脱，那基本上它就真的要被回收了。 123456789101112131415161718192021222324252627282930313233343536373839public class FinalizeDemo &#123; public static FinalizeDemo SAVE_HOOK = null; public void isAlive() &#123; System.out.println(\"I am still alive\"); &#125; @Override protected void finalize() throws Throwable &#123; super.finalize(); System.out.println(\"finalize method called\"); FinalizeDemo.SAVE_HOOK = this; &#125; public static void main(String[] args) throws InterruptedException &#123; SAVE_HOOK = new FinalizeDemo(); // 第一次拯救自己 SAVE_HOOK = null; System.gc(); // Finalizer 方法优先级很低 等待 0.5s 保证其被调用 Thread.sleep(500); if (SAVE_HOOK != null) &#123; SAVE_HOOK.isAlive(); &#125; else &#123; System.out.println(\"Holy crap! I am dead inside.\"); &#125; // 下面这段代码与上面的完全相同，但是这次自救却失败了 SAVE_HOOK = null; System.gc(); // Finalizer 方法优先级很低 等待 0.5s Thread.sleep(500); if (SAVE_HOOK != null) &#123; SAVE_HOOK.isAlive(); &#125; else &#123; System.out.println(\"Holy crap! I am dead inside.\"); &#125; &#125;&#125; 1234// 执行结果// finalize method called// I am still alive// Holy crap! I am dead inside. 任何一个对象的 finalize() 方法都只会被系统自动调用一次，如果对象面临下一次回收，它的 finalize() 方法不会被再次执行，因此第二段代码的自救行动失败了。 回收方法区方法区的垃圾收集主要回收两部分内容: 废弃的常量和不再使用的类型。 判定一个常量是否“废弃”相对简单，而要判定一个类型是否属于“不再被使用的类”的条件就需要同时满足下面三个条件: 该类所有的实例都已经被回收，也就是Java堆中不存在该类及其任何派生子类的实例。 加载该类的类加载器已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 垃圾收集算法从如何判定对象消亡的角度出发，垃圾收集算法可以划分为“引用计数式垃圾收集”(Reference Counting GC)和“追踪式垃圾收集”(Tracing GC)两大类，这两类也常被称作“直接垃圾收集”和“间接垃圾收集”。 分代收集理论分代收集名为理论，实质是一套符合大多数程序运行实际情况的经验法则，它建立在两个分代假说之上: 弱分代假说(Weak Generational Hypothesis): 绝大多数对象都是朝生夕灭的。 强分代假说(Strong Generational Hypothesis): 熬过越多次垃圾收集过程的对象就越难以消亡。 在分代收集理论中，一般至少会把Java堆划分为新生代(Young Generation)和老年代(Old Generation)两个区域。顾名思义，在新生代中，每次垃圾收集时都发现有大批对象死去，而每次回收后存活的少量对象，将会逐步晋升到老年代中存放。 很容易发现分代收集并非只是简单划分一下内存区域那么容易，它至少存在一个明显的困难:对象不是孤立的，对象之间会存在跨代引用。 假如要现在进行一次只局限于新生代区域内的收集(Minor GC)，但新生代中的对象是完全有可能被老年代所引用的，为了找出该区域中的存活对象，不得不在固定的GC Roots之外，再额外遍历整个老年代中所有对象来确保可达性分析结果的正确性，反之亦然。 遍历整个老年代所有对象的方案虽然理论上可行，但会为内存回收带来很大的性能负担。为了解决这个问题，添加第三条经验法则: 跨代引用假说(Intergenerational Reference Hypothesis): 跨代引用相对于同代引用来说仅占极少数。 这其实是可根据前两条假说逻辑推理得出的隐含推论: 存在互相引用关系的两个对象，是应该倾向于同生共死的。举个🌰，如果某个新生代对象存在跨代引用，由于老年代对象难以消亡，该引用会使得新生代对象在收集时同样难以消亡，进而在年龄增长之后晋升到老年代中，这时跨代引用也随即被消除了。 依据这条假说，就不应再为了少量的跨代引用去扫描整个老年代，也不必浪费空间专门记录每一个对象是否存在及存在哪些跨代引用，只需在新生代上建立一个全局的数据结构(该结构被称为“记忆集”，Remembered Set)。 记忆集：把老年代划分成若干小块，标识出老年代的哪一块内存会存在跨代引用。当发生Minor GC时，只有包含了跨代引用的小块内存里的对象才会被加入到GC Roots进行扫描 部分收集(Partial GC): 指目标不是完整收集整个Java堆的垃圾收集，其中又分为: 新生代收集(Minor GC/Young GC):指目标只是新生代的垃圾收集。 老年代收集(Major GC/Old GC):指目标只是老年代的垃圾收集。目前只有CMS收集器会有单独收集老年代的行为。 混合收集(Mixed GC):指目标是收集整个新生代以及部分老年代的垃圾收集。目前只有G1收集器会有这种行为。 整堆收集(Full GC): 收集整个Java堆和方法区的垃圾收集。 标记-清除算法首先标记出所有需要回收的对象，在标记完成后，统一回收掉所有被标记的对象，也可以反过来，标记存活的对象，统一回收所有未被标记的对象。标记过程就是判定对象否属于垃圾的过程。 主要缺点有两个: 执行效率不稳定，如果Java堆中包含大量对象，而且其中大部分是需要被回收的，这时必须进行大量标记和清除的动作，导致标记和清除两个过程的执行效率都随对象数量增长而降低。 内存空间的碎片化问题，标记、清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致当以后在程序运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 标记-复制算法它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 如果内存中多数对象都是存活的，这种算法将会产生大量的内存间复制的开销，但对于多数对象都是可回收的情况，算法需要复制的就是占少数的存活对象，而且每次都是针对整个半区进行内存回收，分配内存时也就不用考虑空间碎片的复杂情况，只要移动堆顶指针，按顺序分配即可。 这种复制回收算法的代价是将可用内存缩小为了原来的一半，空间浪费太多。 还有一种做法是把新生代分为一块较大的Eden空间和两块较小的 Survivor空间，每次分配内存只使用Eden和其中一块Survivor。默认Eden和Survivor的大小比例是8∶1。 发生垃圾收集时，将Eden和Survivor中仍然存活的对象一次性复制到另外一块Survivor空间上，然后直接清理掉Eden和已用过的那块Survivor空间。 当Survivor空间不足以容纳一次Minor GC之后存活的对象时，就需要依赖其他内存区域(实际上大多就是老年代)进行分配担保(Handle Promotion)。 标记-整理算法标记-复制算法在对象存活率较高时就要进行较多的复制操作，效率将会降低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不选用标记-复制算法。 “标记-整理”(Mark-Compact)算法中的标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存。 标记-清除算法与标记-整理算法的本质差异在于前者是一种非移动式的回收算法，而后者是移动式的。 如果移动存活对象，尤其是在老年代这种每次回收都有大量对象存活区域，移动存活对象并更新所有引用这些对象的地方将会是一种极为负重的操作，而且这种对象移动操作必须全程暂停用户应用程序才能进行，也就是“Stop The World”。 如果跟标记-清除算法那样完全不考虑移动和整理存活对象的话，弥散于堆中的存活对象导致的空间碎片化问题就只能依赖更为复杂的内存分配器和内存访问器来解决。 基于以上两点，是否移动对象都存在弊端，移动则内存回收时会更复杂，不移动则内存分配时会更复杂。 吞吐量的实质是赋值器(Mutator，可以理解为使用垃圾收集的用户程序)与收集器的效率总和。 经典垃圾收集器下图展示了七种作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用，图中收集器所处的区域，则表示它是属于新生代收集器抑或是老年代收集器。 Serial收集器一个单线程工作的收集器，它的“单线程”的意义并不仅仅是说明它只会使用一个处理器或一条收集线程去完成垃圾收集工作，更重要的是强调在它进行垃圾收集时，必须暂停其他所有工作线程，直到它收集结束。即 Stop-The-World 。 它依然是HotSpot虚拟机运行在客户端模式下的默认新生代收集器。 ParNew收集器ParNew收集器实质上是Serial收集器的多线程并行版本，除了同时使用多条线程进行垃圾收集之外，其余的行为包括Serial收集器可用的所有控制参数都与Serial收集器完全一致。例如： -XX:SurvivorRatio、-XX:PretenureSizeThreshold、-XX:HandlePromotionFailure 收集算法 Stop The World 对象分配规则 回收策略 目前只有它能与CMS收集器配合工作 并行(Parallel): 并行描述的是多条垃圾收集器线程之间的关系，说明同一时间有多条这样的线程在协同工作，通常默认此时用户线程是处于等待状态。 并发(Concurrent): 并发描述的是垃圾收集器线程与用户线程之间的关系，说明同一时间垃圾收集器线程与用户线程都在运行。 Parallel Scavenge收集器Parallel Scavenge收集器也是一款新生代收集器，它同样是基于标记-复制算法实现的收集器，也是能够并行收集的多线程收集器。 它的关注点与其他收集器不同， CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间 Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量(Throughput)。所谓吞吐量就是处理器用于运行用户代码的时间与处理器总消耗时间的比值，即 $$吞吐量 = \\frac{运行用户代码时间}{运行用户代码时间+运行垃圾收集时间}​$$Parallel Scavenge收集器提供了两个参数用于精确控制吞吐量，分别是 控制最大垃圾收集停顿时间的-XX:MaxGCPauseMillis参数，参数范围为大于 0。 直接设置吞吐量大小的-XX:GCTimeRatio参数，参数范围为 (0,100)。 除上述两个参数之外，Parallel Scavenge收集器还有一个参数-XX:+UseAdaptiveSizePolicy 。 当这个参数被激活之后，就不需要人工指定新生代的大小(-Xmn)、Eden与Survivor区的比例(-XX:SurvivorRatio)、晋升老年代对象大小(-XX:PretenureSizeThreshold)等细节参数，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。 Serial Old收集器Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用标记-整理算法。如果在服务端模式下，它也可能有两种用途: 一种是在JDK 5以及之前的版本中与Parallel Scavenge收集器搭配使用。 另外一种就是作为CMS收集器发生失败时的后备预案，在并发收集发生Concurrent Mode Failure时使用。 Parallel Old收集器Parallel Old是Parallel Scavenge收集器的老年代版本，支持多线程并发收集，基于标记-整理算法实现。 直到Parallel Old收集器出现后，“吞吐量优先”收集器终于有了比较名副其实的搭配组合，在注重吞吐量或者处理器资源较为稀缺的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器这个组 合。 CMS收集器CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。 CMS收集器是基于标记-清除算法实现的，它的运作过程分为四个步骤，包括: 初始标记(CMS initial mark)：标记GC Roots能直接关联到的对象。 并发标记(CMS concurrent mark)：从GC Roots的直接关联对象开始遍历整个对象图，耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行。 重新标记(CMS remark)：为了修正并发标记期间，因用户程序继续运行而导致标记产生变动的那部分对象。停顿时间通常会比初始标记阶段稍长，但远比并发标记阶段的时间短。 并发清除(CMS concurrent sweep)：清理删除掉标记阶段判断的已经死亡的对象，由于不需要移动存活对象，所以这个阶段也可以与用户线程同时并发。 其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。 在整个过程中耗时最长的并发标记和并发清除阶段中，垃圾收集器线程都可以与用户线程一起工作，所以从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。 CMS收集器是 HotSpot虚拟机追求低停顿的第一次成功尝试，但是它还远达不到完美的程度，至少有以下三个明显的缺点: CMS收集器对处理器资源非常敏感，CMS 默认启动的回收线程数是(处理器核心数量 +3)/4。 无法处理“浮动垃圾”(FloatingGarbage)，有可能出现“Concurrent Mode Failure”失败进而导致另一次完全“Stop The World”的Full GC的产生。 适当调高参数-XX:CMSInitiatingOccupancyFraction的值来提高CMS的触发百分比，降低内存回收频率，获取更好的性能。这个参数设置得太高将会很容易导致大量的并发失败产生，性能反而降低。 基于“标记-清除”算法实现的收集器，收集结束时可能会有大量空间碎片产生，将会给大对象分配带来很大麻烦，往往会出现老年代还有很多剩余空间，但就是无法找到足够大的连续空间来分配当前对象，而不得不提前触发一次Full GC的情况。 为了解决这个问题， CMS收集器提供了一个-XX:+UseCMSCompactAtFullCollection开关参数(默认是开启的，JDK 9开始废弃)，用于在CMS收集器不得不进行Full GC时开启内存碎片的合并整理过程，由于这个内存整理必须移动存活对象，(在Shenandoah和ZGC出现前)是无法并发的。这样空间碎片问题是解决了，但停顿时间又会变长。 参数-XX:CMSFullGCsBeforeCompaction(JDK9开始废弃)的作用是要求CMS收集器在执行过若干次(数量由参数值决定)不整理空间的Full GC之后，下一次进入Full GC前会先进行碎片整理(默认值为0，表示每次进入FullGC时都进行碎片整理)。 *浮动垃圾: * 在CMS的并发标记和并发清理阶段，用户线程仍在继续运行的，程序在运行自然就还会伴随有新的垃圾对象不断产生，但这一部分垃圾对象是出现在标记过程结束以后，CMS无法在当次收集中处理掉它们，只好下一次垃圾收集时再清理掉。 G1收集器Garbage First(简称G1)收集器 设计思路：面向局部收集 内存布局：基于Region的内存布局 在G1收集器出现之前的所有其他收集器，包括CMS在内，垃圾收集的目标范围要么是整个新生代(Minor GC)，要么就是整个老年代(Major GC)，再要么就是整个Java堆(Full GC)。 G1 面向堆内存任何部分组成回收集(Collection Set)来进行回收，衡量标准不再是它属于哪个分代，而是哪块内存中存放的垃圾数量最多，回收收益最大，这就是 G1 的 Mixed GC 模式。 G1不再坚持固定大小以及固定数量的分代区域划分，而是把连续的Java堆划分为多个大小相等的独立区域(Region)，每一个Region都可以根据需要，扮演新生代的Eden空间、Survivor空间，或者老年代空间。收集器能够对扮演不同角色的 Region 采用不同的策略去处理。 Region中还有一类特殊的Humongous区域，专门用来存储大对象。 G1 认为只要大小超过了一个Region容量一半的对象即可判定为大对象。每个Region的大小可以通过参数-XX:G1HeapRegionSize设定，取值范围为1MB~32MB，且应为2的N次幂。对于那些超过了整个Region容量的超级大对象，将会被存放在N个连续的 Humongous Region 之中，G1 的大多数行为都把 Humongous Region 作为老年代的一部分来进行看待，如下图所示。 仍然有新生代与老年代概念之分，但是新生代与老年代不再是固定不变的了，它们都是一系列区域(不需要连续)的动态集合。 不再坚持回收区域大小。 G1 收集器之所以能建立可预测的停顿时间模型，是因为它将 Region 作为单次回收的最小单元，即每次收集到的内存空间都是 Region 大小的整数倍，这样可以有计划地避免在整个 Java 堆中进行全区域的垃圾收集。因为不再是回收整个老年代或者整个新生代，所以可以比较容易停下。 更具体的思路是让G1收集器去跟踪各个 Region 里面的垃圾堆积的“价值”大小，价值即回收所获得的空间大小以及回收所需时间的经验值，然后在后台维护一个优先级列表，每次根据用户设定允许的收集停顿时间(使用参数-XX:MaxGCPauseMillis指定，默认值是200毫秒)，优先处理回收价值收益最大的那些 Region。这种使用Region划分内存空间，以及具有优先级的区域回收方式，保证了G1收集器在有限的时间内获取尽可能高的收集效率。 在并发标记阶段如何保证收集线程与用户线程互不干扰地运行? CMS收集器采用增量更新算法 G1收集器则是通过原始快照(SATB)算法 如果不去计算用户线程运行过程中的动作(如使用写屏障维护记忆集的操作)，G1 收集器的运作过程大致可划分为以下四个步骤: 初始标记(Initial Marking): 标记一下GC Roots能直接关联到的对象，并且修改TAMS指针的值，让下一阶段用户线程并发运行时，能正确地在可用的Region中分配新对象。这个阶段需要停顿线程，但耗时很短，而且是借用进行Minor GC的时候同步完成的，所以G1收集器在这个阶段实际并没有额外的停顿。 并发标记(Concurrent Marking): 从GC Roots开始对堆中对象进行可达性分析，递归扫描整个堆里的对象图，找出要回收的对象，耗时较长，但可与用户程序并发执行。当对象图扫描完成以后，还要重新处理SATB记录下的在并发时有引用变动的对象。 最终标记(Final Marking): 对用户线程做另一个短暂的暂停，用于处理并发标记阶段结束后仍遗留下来的少量的SATB记录(参见书本3.4.6节)。 筛选回收(Live Data Counting and Evacuation): 更新 Region 的统计数据，对各个Region的回收价值和成本进行排序； 根据用户所期望的停顿时间制定回收计划，自由选择任意多个Region构成回收集； 把决定回收的那部分 Region 中的存活对象复制到空的 Region 中(复制)； 清理掉整个旧 Region 的全部空间； 以上操作涉及存活对象的移动，必须暂停用户线程，由多条收集器线程并行完成的。 从上面可以看出，G1 整体上基于标记-整理算法实现； 局部(两个Region之间)角度看是基于标记-复制算法实现。 因为没有采用标记-清除算法，所以 G1 运作期间不会产生内存空间碎片，垃圾收集完成之后能提供规整的可用内存。有利于程序长时间运行，在程序需要为大对象分配内存时，因为连续内存空间较多，所以发生 Full GC 的概率也较小。 TAMS(Top at Mark Start)指针：有两个，把 Region 中的一部分空间划分出来用于并发回收过程中的新对象分配，并发回收时新分配的对象地址都必须要在这两个指针位置以上。 总结 收集器名称 收集算法 关注点 分代 Serial 标记-复制 新生代 ParNew 标记-复制 新生代 Parallel Scavenge 标记-复制 吞吐量 新生代 Serial Old 标记-整理 老年代 Parallel Old 标记-整理 吞吐量 老年代 CMS 标记-清除 最短停顿时间 老年代 G1 整体上标记-整理，局部上标记-复制。 最短停顿时间 不再区分新生代与老年代 低延迟收集器下图浅色阶段表示必须挂起用户线程，深色表示收集器线程与用户线程是并发工作的。 从图中可以看出： CMS和G1之前的全部收集器，其工作的所有步骤都会产生“Stop The World”式的停顿。 CMS和G1分别使用增量更新和原始快照技术，实现了标记阶段的并发。(3.4.6节：并发的可达性分析) Shenandoah 和 ZGC，几乎整个工作过程全部都是并发的，只有初始标记、最终标记这些阶段有短暂的停顿。 Shenandoah收集器Shenandoah 与 G1 的区别： 支持并发的整理算法不同。 Shenandoah 没有专门的新生代 Region 或者老年代 Region 的存在，不存在分代。 放弃了记忆集，改用连接矩阵。 Shenandoah 工作的九个过程： 初始标记(Initial Marking)：标记与GC Roots直接关联的对象，仍需 STW，但停顿时间与堆大小无关，只与GC Roots的数量相关。 并发标记(Concurrent Marking)：遍历对象图，标记出全部可达的对象。与用户线程并发，用时取决于堆中存活对象的数量以及对象图的结构复杂程度。 最终标记(Final Marking)：处理剩余的SATB扫描，统计出回收价值最高的Region，构造回收集。 并发清理(Concurrent Cleanup)：用于清理那些整个区域内连一个存活对象都没有找到的Region。 并发回收(Concurrent Evacuation)：把回收集里面的存活对象先复制一份到其他未被使用的Region之中。时间取决于回收集的大小。 初始引用更新(Initial Update Reference)：把堆中所有指向旧对象的引用修正到复制后的新地址。 并发引用更新(Concurrent Update Reference)：真正开始进行引用更新操作，时间长短取决于内存中涉及的引用数量的多少。它不再需要沿着对象图来搜索，只需要按照内存物理地址的顺序，线性地搜索出引用类型，把旧值改为新值即可。 最终引用更新(Final Update Reference)：修正存在于 GC Roots 中的引用。 并发清理(Concurrent Cleanup)：回收Region的内存空间，供以后新对象分配使用。 三个最重要阶段：并发标记、并发回收、并发引用更新。 ZGC收集器与Shenandoah关注点相同：在吞吐量影响不太大的前提下，实现在任意堆内存大小下都可以把垃圾收集的停顿时间限制在十毫秒以内的低延迟。 同样基于Region的堆内存布局，在 x64 硬件平台下，ZGC 的 Region 可以有大、中、小三类容量： 小型 Region(Small Region): 容量固定为 2MB，用于放置小于256KB的小对象。 中型Region(Medium Region): 容量固定为32MB，用于放置大于等于256KB但小于4MB的对象。 大型Region(Large Region):容量不固定，可以动态变化，但必须为2MB的整数倍，用于放置 4MB或以上的大对象。 Shenandoah 使用转发指针和读屏障来实现并发整理，ZGC 虽然同样用到了读屏障，但是实现思路完全不同。 接下来的内容待深究。 参考 《深入理解Java虚拟机：JVM高级特性与最佳实践》（第3版） - 周志明","categories":[{"name":"JVM","slug":"JVM","permalink":"http://raymond-zhao.top/categories/JVM/"}],"tags":[{"name":"GC","slug":"GC","permalink":"http://raymond-zhao.top/tags/GC/"}]},{"title":"MySQL-查询性能优化","slug":"2020-08-03-MySQL-OptimizeSelection","date":"2020-08-03T00:35:00.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/08/03/2020-08-03-MySQL-OptimizeSelection/","link":"","permalink":"http://raymond-zhao.top/2020/08/03/2020-08-03-MySQL-OptimizeSelection/","excerpt":"","text":"MySQL-查询性能优化在《高性能MySQL》一书中，本章首先介绍了查询设计的一些基本原则，然后介绍一些更深的查询优化的技巧，也会介绍一些 MySQL 优化器内部机制。 之所以选择以这种方式记录，是为了以后需要时可以通过手机看到，而不用再去找书，再去看大量的内容选择想要的几句话。 查询为什么会变慢？査询中真正重要的是响应时间。如果把查询看作是一个任务，那么它由一系列子任务组成，每个子任务都会消耗一定的时间。如果要优化査询，实际上要优化其子任务，要么消除其中一些子任务，要么减少子任务的执行次数，要么让子任务运行得更快。优化查询的目的就是减少和消除这些操作所花费的时间。 通常来说，查询的生命周期大致可以按照顺序来看： 从客户端，到服务器，然后在服务器上进行解析； 生成执行计划，执行，并返回结果给客户端。 其中“执行”可以认为是整个生命周期中最重要的阶段，这其中包括了大量为了检索数据到存储引擎的调用以及调用后的数据处理，包括排序、分组等。 慢查询基础：优化数据访问查询性能低下最基本的原因是访问的数据太多，但这并不常见。大部分性能低下的查询都可以通过减少访问的数据量的方式进行优化。对于低效的查询，可以通过下面两个步骤来分析： 确认应用程序是否在检索大量超过需要的数据。这通常意味着访向了太多的行，但有时候也可能是访问了太多的列。 确认 MySQL 服务器层是否在分析大量超过需要的数据行。 是否向数据库请求了不需要的数据有些查询会请求超过实际需要的数据，然后又丢弃掉。这样会给 MySQL 服务器带来负担，增加网络开销，消耗服务器的 CPU 和内存资源。 下面是一些典型的案例： 查询不需要的记录：MySQL 并不只是会返回我们需要的数据，而是先返回全部结果集再进行计算，然后再丢弃大量数据。 多表关联时返回全部列：只选择需要使用的表中的列，而不要全部返回。 总是取出全部列：也就是 SELECT * 取出全部列，会让优化器无法完成索引覆盖扫描这类优化，还会消耗服务器资源。 重复查询相同的数据：记得用缓存。 MySQL 是否在扫描额外的记录在确定查询只返回需要的数据后，接下来应该看看查询为了返回结果是否扫描了过多的数据。主要有三个指标，这三个指标会记录在 MySQL 的慢日志中： 响应时间 扫描的行数 返回的行数 在 EXPLAIN 语句中的 type 列反映了访问类型。访问类型有很多种，从全表扫描到索引扫描、范围扫描、唯一索引查询、常数引用等。列出的这些访问速度从慢到快，扫描的行数从小到大。 一般 MySQL 使用如下三种方式应用 WHERE 条件，从好到坏依次为： 在索引中使用 WHERE 条件来过滤不匹配的记录。在存储引擎层完成。 使用索引覆盖扫描（在 Extra 列中出现了 Using index）来返回记录，直接从索引中过滤不需要的记录并返回命中的结果。在 MySQL 服务器层完成的，无须再回表査询记录。 从数据表中返回数据，然后过滤不满足条件的记录（在 Extra 列中出现 Using Where）。在 MySQL 服务器层完成，MySQL 需要先从数据表读出记录然后过滤。 如果发现査询需要扫描大量的数据，但只返回少数的行，那么可以： 使用索引覆盖扫描，把所有需要用的列都放到索引中，这样存储引擎无须回表就可以返回结果。 改变库表结构。例如使用单独的汇总表。 重写这个复杂的查询，让 MySQL 优化器能够以更优化的方式执行这个査询。 重构查询的方式在优化查询时，目标是找到一个更优的方法获得实际需要的结果，而不一定同时需要从 MySQL 获取一模一样的结果。 复杂查询还是简单查询：是否需要将一个复杂查询分成多个简单查询。 切分查询：分而治之，将大查询切分成小查询，每个查询功能完全一样，只完成一小部分，每次返回一小部分查询结果。 分解关联查询：将关联查询分解成单个查询。 查询执行的基础《高性能MySQL》的图有点糊，正在 ProcessOn 开画的时候想起了我还有英文版的《High Performance MySQL》，英文原版的就清楚多了。 根据上图，我们可以看到在执行一条查询时，MySQL 做了些什么： 客户端发送一条查询给服务器。 服务器先查询缓存，命中缓存则直接返回，否则进入下一阶段。 服务器进行 SQL 解析、预处理、再由查询优化器生成对应的执行计划。 MySQL 根据优化器生成的执行计划，调用存储引擎 API 执行查询。 将结果返回给客户端。 MySQL客户端/服务器通信协议是半双工，在任何一个时刻，要么是服务器向客户端发送数据，要么相反，不能同时发生。 查询优化处理查询的生命周期最重要的一步就是将 SQL 转换成执行计划，MySQL 再依照执行计划和存储引擎进行交互。这包括：解析 SQL、预处理、优化 SQL 执行计划。这个过程中任何错误都可能终止查询。 语法解析器与预处理 解析：MySQL 通过关键字将 SQL 语句解析，生成对应的“解析树”。解析器使用 MySQL 语法规则验证和解析查询。 预处理：根据一些 MySQL 规则进一步检查解析树是否合法。如，检查数据表、列是否存在。 查询优化器 当语法树合法时，由优化器将其转化为执行计划。一条查询可以有多种执行方式并且返回相同的结果，优化器的作用就是找出其中最好的那个。优化策略包括两种，一种是静态优化，一种是动态优化。 下面是一些 MySQL 能够处理的优化类型： 重新定义关联表的顺序 将外连接转化成内连接 使用等价变换规则，如( 5 = 5 And a &gt; 5 ==&gt; a &gt; 5) 优化 COUNT()、MIN()、MAX()：要 MIN()，只需查询 B-Tree 索引最左端的记录，MAX() 则是最右端记录。 预估并转化为常数表达式：当 MySQL 检测到表达式可以转为常数时，就会一直把该表达式作为常数进行优化处理。有时候甚至查询也能够转化为常数，如在索引上执行 MIN()，甚至主键或者唯一键查找语句。 覆盖索引扫描：当索引中的列包含查询中所有需要使用的列时，使用索引返回，而无需查询对应的行。 子查询优化 提前终止查询：发现已满足查询需求时，立刻终止查询。如使用 LIMIT 子句或者发不成立条件(id = -1)。 等值传播：如果两列的值通过等式关联，MySQL 把其中一列的 WHERE 条件传递到另一列上。 列表 IN() 的比较：在一些 DBS 中，IN() 完全等同于多个 OR 子句($\\mathcal{O}(n)$)，在 MySQL 中则不然，MySQL 将 IN() 列表中的数据先排序，然后通过二分查找($\\mathcal{O}(\\lg n)$)来确定列表中的值是否满足条件。 $\\dots$ MySQL 如何执行关联查询 MySQL 关联策略很简单：MySQL 对任何关联都执行嵌套循环关联操作，即 MySQL 先在一个表中循环取出单条数据，然后再嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到所有表中匹配的行为止。然后根据各个表匹配的行，返回查询中需要的各个列。MySQL 会尝试在最后一个关联表中找到所有匹配的行，如果最后一个关联无法找到更多的行，MySQL 返回到上一层次关联表(回溯?)，看能否找到更多的匹配记录，以此类推迭代执行。 执行计划 和很多其他关系数据库不同，MySQL 并不会生成査询字节码来执行查询。MySQL 生成查询的一棵指令树，然后通过存储引擎执行完成这棵指令树并返回结果。最终的执行计划包含了重构查询的全部信息。如果对某个查询执行 EPLAIN EXTENDED 后，再执行 SHOW WARNINGS，就可以看到重构出的查询。 任何多表查询都可以使用一棵树表示，例如下图为执行一个四表的关联操作。 上图属于平衡树，但是 MySQL 的查询并不是如此，正如前面所说的嵌套循环加回溯，MySQL 的的执行计划则是一棵左侧深度优先树。 关联查询优化器 通过评估不同顺序时的成本来选择一个代价最小的关联顺序。 查询执行引擎在解析和优化阶段，MySQL 将生成查询对应的执行计划，MySQL 的查询执行引擎根据这个执行计划来完成查询。执行计划是一种数据结构，而不是和很多其他关系型数据库那样的字节码。 为了执行查询，MySQL 只需要重复执行计划中的各个操作，直到完成所有的数据查询。 返回结果即使查询不需要返回结果集给客户端，MySQL 仍然会返回这个查询的一些信息，如该查询影响到的行数。 如果查询可以被缓存，那么 MySQL 在这个阶段也会将结果存放到查询缓存中。 MySQL 将结果集返回客户端是一个增量、逐步返回的过程。当开始生成第一条结果时，MySQL 就可以开始向客户端逐步返回结果集。 这样处理有两个好处： 服务器端无须存储太多的结果，也就不会因为要返回太多结果而消耗太多内存。 让 MySQL 客户端第一时间获得返回的结果。 结果集中的每一行都会以一个满足 MySQL 客户端/服务器通信协议的封包发送，再通过 TCP 协议进行传输，在 TCP 传输的过程中，可能对 MySQL 的封包进行缓存然后批量传输。 优化特定类型的查询优化 COUNT() 查询 COUNT() 有两种非常不同的作用： 它可以统计某个列值的数量，也可以统计行数。在统计列值时要求列值是非空的(不统计 NULL)。如果在 COUNT() 的括号中指定了列或列表达式，统计的就是这个表达式有值的结果数。 统计结果集的行数。当 MySQL 确认括号内的表达式值不可能为空时，其实就是在统计行数。当使用 COUNT(*) 时，通配符 * 实际上实际上并不会扩展成所有列，而是会忽略所有列而直接统计所有行数。 MyISAM 的 COUNT() 函数虽然非常快，但是是有前提条件的，即只有没有任何 WHERE 条件的COUNT(*) 才非常快，因为此时无序实际计算表的行数，如果包含了 WHERE，那速度就没什么过人之处了。 优化关联查询 确保 ON 或者 USING 子句中的列上有索引。在创建索引时候就要考虑到关联的顺序。一般来说，只需要在关联顺序中的第二个表的相应列上创建索引。 确保任何的 GROUP BY 和 ORDER BY 中的表达式只涉及到一个表中的列，这样 MySQL 才有可能使用索引来优化。 当升级 MySQL 的时候注意：关联语法、运算符优先级等其他可能会发生变化的地方。以前普通关联的地方可能会变成笛卡儿积，不同类型的关联可能会生成不同的结果等。 优化 GROUP BY 和 DISTINCT MySQL 优化器会在内部处理时相互转化这两类查询。当无法使用索引时，GROUP BY 使用临时表或文件排序来做分组。 优化 LIMIT 分页 在 LIMIT 1000,20 这样偏移量非常大的查询时，尽可能地使用索引覆盖。 LIMIT 和 OFFSET 的问题，实际上是 OFFSET 的问题，它会导致 MySQL 扫描大量不需要的数据然后再抛弃。可以使用书签记录，或者使用预先计算的汇总表，或者关联到冗余表，冗余表只包含主键列和需要排序的列。 优化 SQL_CALC_FOUND_ROWS 分页时，在 LIMIT 语句中加上 SQL_CALC_FOUND_ROWS 可以获得去掉 LIMIT 以后满足条件的行数，因此可以作为分页的总数。 或者缓存较多数据，再从缓存中获取。 或者考虑 EXPLAIN 中的 rows 列的值作为结果集总数的近似值。 优化 UNION 查询 除非确实需要服务器消除重复行，否则一定要使用 UNION ALL。如果没有 ALL 关键字，MySQL 会给临时表加上 DISTINCT，会对整个临时表的数据做唯一性检查。 静态分析查询 $\\dots$ 使用用户自定义变量 $\\dots$ 慢查询优化 MySQL 中 InnoDB 与 MyISAM 存储引擎实现索引的数据结构是 B-Tree 早都已经是让人耳熟能详的话了。那 B+Tree 的性质对于索引有什么有用之处呢？ 为什么索引长度要尽量短？ 对数据的查找过程通常会伴随着不止一次的 IO，而 IO 次数与 B+Tree 的高度密切相关。如果当前当数据表的数据项为 N，每个磁盘块的数据项的数量为 m，B+Tree高度为 h，则有 $h = \\log_{m+1}N$。当数据量 N 一定时，$m \\uparrow$，则 $h \\downarrow$；而 $m = $ 磁盘块大小 / 数据项大小，磁盘块的大小也就是一个数据页的大小，是固定的。如果数据项占的空间越小，数据项的数量越多，树的高度越低。这就是为什么每个数据项，即索引字段要尽量的小，比如 int 占 $4$ 字节，要比 bigint $8$ 字节少一半。这也是为什么 B+Tree 要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当每个磁盘块只包含 1 个数据项时将会退化成线性表。 最左匹配：在 B+Tree 中叶子结点中的数据都是有序的，在查找时从最左边开始匹配，联合索引中前面的列匹配之后才匹配后面的列。搜索树是左侧深度优先树。 慢查询优化基本步骤观察分析状态观察分析服务器状态 show status; 1234567mysql&gt; show status like &#39;Queries&#39;;+---------------+-------+| Variable_name | Value |+---------------+-------+| Queries | 7 |+---------------+-------+1 row in set (0.00 sec) 定位慢查询 SQL检查是否开启慢查询 123456789101112131415161718192021222324mysql&gt; show variables like &#39;%query%&#39;;+------------------------------+------------------------------------------+| Variable_name | Value |+------------------------------+------------------------------------------+| binlog_rows_query_log_events | OFF || ft_query_expansion_limit | 20 || have_query_cache | YES || long_query_time | 10.000000 || query_alloc_block_size | 8192 || query_cache_limit | 1048576 || query_cache_min_res_unit | 4096 || query_cache_size | 1048576 || query_cache_type | OFF || query_cache_wlock_invalidate | OFF || query_prealloc_size | 8192 || slow_query_log | OFF || slow_query_log_file | &#x2F;usr&#x2F;local&#x2F;var&#x2F;mysql&#x2F;iMacs-iMac-slow.log |+------------------------------+------------------------------------------+13 rows in set (0.01 sec)# 需要注意的是# long_query_time 10.000000# slow_query_log OFF# slow_query_log_file &#x2F;usr&#x2F;local&#x2F;var&#x2F;mysql&#x2F;iMacs-iMac-slow.log 查询慢查询的数量 1234567mysql&gt; show status like &#39;%slow_queries%&#39;;+---------------+-------+| Variable_name | Value |+---------------+-------+| Slow_queries | 0 |+---------------+-------+1 row in set (0.00 sec) 打开慢查询日志 123456789set global slow_query_log &#x3D; on;# 添加索引alter table t_name add index idx_name(name);# MySQL 默认查询并不一定是 聚集索引(主键)， 而是由 MySQL查询优化器来决定，消除尽可能多的重复select count(id) from t_name;# 强行指定查询索引select count(id) from t_name force index (primary); 设置慢查询阈值，超过 1s 即为慢查询(模拟) 1set long_query_time &#x3D; 1; EXPLAIN 查看执行计划12# explain + 查询语句可以 解释查询信息 查询结果关键字段 type、extraexplain select * from emp; 关于 EXPLAIN 结果列的含义，详见：MySQL-Explain 中的列。 效率 type 列中项 解释 $\\rightarrow$ const/system 常量，表只有一行(用于 MyISAM、Memory)。 $\\uparrow$ eq_ref 搜索时使用 PK 或 UNIQUE KEY，常用于多表联查。 $\\uparrow$ ref 根据索引查找一个或多个值。 $\\uparrow$ index_merge 合并索引，使用多个单列索引查找。 $\\uparrow$ range 按索引进行范围查找。 $\\uparrow$ index 按索引全表扫描。 $\\uparrow$ all 所谓的全表扫描。 Extra 列中项(部分) 解释 “Using index” MySQL 将使用覆盖索引，以避免访问表。不要把覆盖索引和 index 访问类型弄混。 “Using where” MySQL 服务器将在存储引擎检索行后再进行过滤。在 WHERE 条件里涉及索引列时，就能被存储引擎检验，不是所有带 WHERE 子句的查询都会显示 “Using where”。 “Using temporary” MySQL 在对查询结果排序时会使用一个临时表。 “Using filesort” MySQL 会对结果使用一个外部索引排序，而不是按索引次序从表里读取行。 “Range checked for each record (index map: N)” 没有好用的索引，新的索引将在联接的每一行上重新估算。N 是显示在 possible_keys 列中索引的位图，并且是冗余的。 SHOW PROFILE 查看执行时间查看是否开启 1234567mysql&gt; show variables like &#39;profiling&#39;;+---------------+-------+| Variable_name | Value |+---------------+-------+| profiling | OFF |+---------------+-------+1 row in set (0.00 sec) 查看当前会话 12mysql&gt; show profiles;Empty set, 1 warning (0.00 sec) 查看某 Query_ID 开销 12mysql&gt; show profile for query xxx;Empty set, 1 warning (0.00 sec) 扩展阅读MySQL索引原理及慢查询优化 参考 《高性能 MySQL》 顺便记录刚刚看到的 ProcessOn 上几个特别骚的图： JVM内存模型完整版 JUC MyBatis-流程 并发编程-Java锁 Spring 源码分析图","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://raymond-zhao.top/categories/MySQL/"}],"tags":[{"name":"慢查询优化","slug":"慢查询优化","permalink":"http://raymond-zhao.top/tags/%E6%85%A2%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/"}]},{"title":"MySQL-Explain中的列","slug":"2020-08-02-MySQL-Explain-Cols","date":"2020-08-02T08:28:42.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/08/02/2020-08-02-MySQL-Explain-Cols/","link":"","permalink":"http://raymond-zhao.top/2020/08/02/2020-08-02-MySQL-Explain-Cols/","excerpt":"","text":"EXPLAIN 中的列123456789101112131415MySQL&gt; explain select 1\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: NULL partitions: NULL type: NULLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: NULL filtered: NULL Extra: No tables used1 row in set, 1 warning (0.00 sec) id 列这一列总是包含一个编号，标识 SELECT 所属的行。如果在语句当中没有子查询或联合，那么只会有唯一的 SELECT，于是每一行在这个列中都将显示一个 $1$。否则，内层的 SELECT 语句一般会顺序编号，对应于其在原始语句中的位置 MySQL 将 SELECT 查询分为简单和复杂类型，复杂类型可分成三大类：简单子査询、所谓的派生表（在 FROM 子句中的子查询）, 以及 UNION 査询。下面是一个简单的子査询: 12345678MySQL&gt; explain select (select 1 from demo.dept limit 1) from demo.dept;+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-------------+| 1 | PRIMARY | dept | NULL | index | NULL | PRIMARY | 4 | NULL | 1 | 100.00 | Using index || 2 | SUBQUERY | dept | NULL | index | NULL | PRIMARY | 4 | NULL | 1 | 100.00 | Using index |+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-------------+2 rows in set, 1 warning (0.01 sec) 12345# 派生表查询MySQL&gt; explain select film.id from (select film.id from demo.film) as der;# UNION 查询MySQL&gt; explain select 1 from union all select 1; select_type 列显示对应行是简单还是复杂 SELECT（如果是后者，那么是三种复杂类型中的哪一种）。SIMPLE 值意味着査询不包括子查询和 UNION。如果查询有任何复杂的子部分，则最外层部分标记为 PRIMARY，其他部分标记如下。 SUBQUERY 包含在 SELECT 列表中的子査询中的 SELECT（换句话说，不在 FROM 子句中）标记为 SUBQUERY. DERIVED 用来表示包含在 FROM 子句的子查询中的 SELECT, MySQL 会递归执行并将结果放到一个临时表中。服务器内部称其“派生表”，因为该临时表是从子查询中派生来的。 UNION 在 UNION 中的第二个和随后的 SELECT 被标记为 UNION。第一个 SELECT 被标记就好像它以部分外查询来执行。这就是之前的例子中在 UNION 中的第一个 SELECT 显示为 PRIMARY 的原因。如果 UNION 被 FROM 子句中的子査询包含，那么它的第一个 SELECT 会被标记为 DERIVED 。 UNION RESULT 用来从 UNION 的匿名临时表检索结果的 SELECT 被标记为 UNION RESULT。 table 列显示了对应行正在访问哪张表，或者表的别名。当 FROM 子句中有子查询或有 UNION 时，table 列会变得复杂许多。 第六章-左侧深度优先树。MySQL 的查询执行计划总是左侧深度优先树。 type 列 效率 type 列中项 解释 $\\rightarrow$ const/system 常量，表只有一行(用于 MyISAM、Memory)。 $\\uparrow$ eq_ref 搜索时使用 PK 或 UNIQUE KEY，常用于多表联查。 $\\uparrow$ ref 根据索引查找一个或多个值。 $\\uparrow$ index_merge 合并索引，使用多个单列索引查找。 $\\uparrow$ range 按索引进行范围查找。 $\\uparrow$ index 按索引全表扫描。 $\\uparrow$ all 所谓的全表扫描。 访问类型，就是 MySQL 决定如何査找表中的行。下面是最重要的访问方法，依次从最差到最优。 ALL 所谓的全表扫描，通常意味着 MySQL 必须扫描整张表，从头到尾，去找到需要的行。 例外：在查询里使用了 LIMIT，或者在 Extra 列中显示“Using distinct/not exists“。 index 跟全表扫描一样，只是 MySQL 扫描表时按索引次序进行而不是行次序。它的主要优点是避免了排序；最大的缺点是要承担按索引次序读取整个表的开销。这通常意味着若是按随机次序访问行，开销将会非常大。 如果在 Extra 列中看到 “Using index“，说明 MySQL 正在使用覆盖索引，它只扫描索引的数据，而不是按索引次序的每一行。它比按索引次序全表扫描的开销要少很多。 range 范围扫描就是一个有限制的索引扫描，它开始于索引里的某一点，返回匹配这个值域的行。比全索引扫描好一些，因为不用遍历全部索引。显而易见的范围扫描是带有 BETWEEN 或在 WHERE 子句里带有$&gt;$ 的查询。 当 MySQL 使用素引去查找一系列值时，例如 IN() 和 0R 列表，也会显示为范围扫描。然而，这两者其实是相当不同的访向类型，在性能上有重要的差异。 此类扫描的开销跟索引类型相同。 ref 一种索引访问(又称索引查找)，它返回所有匹配某个单个值的行。然而，它可能会找到多个符合条件的行，因此，它是查找和扫描的混合体。此类索引访问只有当使用非唯一性索引或者唯一性索引的非唯一性前缀时才会发生。把它叫做 ref 是因为索引要跟某个参考值相比较。这个参考值或者是一个常数，或者是来自多表査询前一个表里的结果值。 ​ ref or null 是 ref 之上的一个变体，它意味着 MySQL 必须在初次査找的结果里进行第二次査找以找出 NULL 条目。 eq_ref 使用这种索引査找，MySQL 知道最多只返回一条符合条件的记录。这种访问方法可以在 MySQL 使用主键或者唯一性索引査找时看到，它会将它们与某个参考值做比较。 const, system 当 MySQL 能对查询的某部分进行优化并将其转换成一个常量时，它就会使用这些访问类型。 NULL 意味着 MySQL 能在优化阶段分解查询语句，在执行阶段甚至用不着再访问表或者索引。例如，从一个索引列里选取最小值。 possible_keys 列显示了查询可以使用哪些索引。 key 列显示了 MySQL 决定采用哪个索引来优化对该表的访问。如果该索引没有出现在 possible_keys 列中，那么 MySQL 选用它是出于另外的原因。例如，它可能选择了一个覆盖素引，即使没有 WHERE 子句。 换言之，possible_keys 揭示了哪一个索引能有助于高效地行査找，而 key 显示的是优化采用哪一个索引可以最小化查询成本。 key_len 列显示了 MySQL 在索引里使用的字节数。 ref 列显示了之前的表在 key 列记录的索引中查找值所用的列或常量。 rows 列MySQL 估计为了找到所需的行需要读取的行数。 filtered 列针对表里某个符合条件(WHERE 子句或者连接条件)的记录数的百分比所做的一个悲观估算。如果跟 rows 列相乘，就能看到 MySQL估算它将和查询计划里前一个表关联的行数。 Extra 列 Extra 列中项(部分) 解释 “Using index” MySQL 将使用覆盖索引，以避免访问表。不要把覆盖索引和 index 访问类型弄混。 “Using where” MySQL 服务器将在存储引擎检索行后再进行过滤。在 WHERE 条件里涉及索引列时，就能被存储引擎检验，不是所有带 WHERE 子句的查询都会显示 “Using where”。 “Using temporary” MySQL 在对查询结果排序时会使用一个临时表。 “Using filesort” MySQL 会对结果使用一个外部索引排序，而不是按索引次序从表里读取行。 “Range checked for each record (index map: N)” 没有好用的索引，新的索引将在联接的每一行上重新估算。N 是显示在 possible_keys 列中索引的位图，并且是冗余的。 参考 《高性能MySQL》","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://raymond-zhao.top/categories/MySQL/"}],"tags":[{"name":"MySQL Explain","slug":"MySQL-Explain","permalink":"http://raymond-zhao.top/tags/MySQL-Explain/"}]},{"title":"MySQL-事务","slug":"2020-08-01-MySQL-TransactionAndLock","date":"2020-08-01T07:12:32.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/08/01/2020-08-01-MySQL-TransactionAndLock/","link":"","permalink":"http://raymond-zhao.top/2020/08/01/2020-08-01-MySQL-TransactionAndLock/","excerpt":"","text":"MySQL-事务与锁事务四大特性 原子性（atomicity): 一个事务必须被视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚，对于一个事务来说，不可能只执行其中的一部分操作，这就是事务的原子性。 一致性（consistency): 数据库总是从一个一致性的状态转换到另外一个一致性的状态。 隔离性（isolation): 一个事务所做的修改在最终提交以前，对其他事务是不可见的。 持久性（durability): 一旦事务提交，则其所做的修改就会永久保存到数据库中。此时即使系统崩溃，修改的数据也不会丢失。 就像锁粒度的升级会增加系统开销一样，这种事务处理过程中额外的安全性，也会需要数据库系统做更多的额外工作。一个实现了 $ACID$ 的数据库，相比没有实现 $ACID$ 的数据库，通常会需要更强的 $CPU$ 处理能力、更大的内存和更多的磁盘空间。 事务隔离级别在 $SQL$ 标准中定义了四种隔离级别，每一种级别都规定了一个事务中所做的修改，哪些在事务内和事务间是可见的，哪些是不可见的。较低级别的隔离通常可以执行更高的并发，系统的开销也更低。 READ UNCOMMITTED（未提交读） 在 READ UNCOMMITTED级别，事务中的修改，即使没有提交，对其他事务也都是可见的。事务可以读取未提交的数据，这也被称为脏读（Dirty Read）。 READ COMMITTED（提交读） 大多数数据库系统的默认隔离级别都是 READ COMMITTED（但 $MySQL$ 是REPEATABLE READ ）。READ COMITTED 满足前面提到的隔离性的简单定义：一个事务开始时，只能“看见”已经提交的事务所做的修改。换句话说，一个事务从开始直到提交之前，所做的任何修改对其他事务都是不可见的。这个级别有时候也叫做不可重复读（nonrepeatable read），因为两次执行同样的查询，可能会得到不一样的结果。 REPEATABLE READ（可重复读）MySQL 的认事务隔离级别 REPEATABLE READ 解决了脏读的问题，保证了在同一个事务中多次读取同样记录的结果是一致的。但是理论上，可重复读隔离级别还是无法解决另外一个幻读（Phantom Read）的问题。 所谓幻读，指的是当某个事务在读取某个范围内的记录时另外一个事务又在该范围内插入了新的记录，当之前的事务再次读取该范围的记录时，会产生幻行（Phantom Row）。InnoDB 和 XtraDB 存储引擎通过多版本并发控制（MVCC, Multiversion Concurrency Control）解决了幻读的问题。 SERIALIZABLE（可串行化） 最高的隔离级别。通过强制事务串行执行，避免了幻读问题。简单来说，SERIALIZABLE 会在读取的每一行数据上都加锁，所以可能导致大量的超时和锁争用的问题。 隔离级别 脏读可能性 不可重复读可能性 幻读可能性 加锁读 READ UNCOMMITTED $YES$ $YES$ $YES$ $NO$ READ COMMITTED $NO$ $YES$ $YES$ $NO$ REPEATABLE READ $NO$ $NO$ $YES$ $NO$ SERIALIZABLE $NO$ $NO$ $NO$ $YES$ 修改事务隔离级别123456789101112131415161718# 查看当前事务隔离级别select @@tx_isolation;# 设置事务隔离级别set session transaction isolation level repeatable read;# 默认 ON 只在当前 session 有效 先关闭进行并发模拟show variables like &#39;autocommit&#39;;# 关闭默认提交set autocommit &#x3D; 0;# 开启一个事务start transaction;# 需要在一个事务中进行的操作select * from t_name;# 手动提交事务commit; 并发访问问题及解决 更新丢失(Lost update)：$MySQL$ 所有事务隔离级别在数据库层面上均可避免。 脏读(Dirty Read)：$READ-COMMITED$ 事务隔离级别以上可避免。 InnoDB可重复读隔离级别下如何避免幻读多版本并发控制 MVCC可以认为 MVCC 是行级锁的一个变种，但是在多数情况下避免了加锁，因此开销较低。虽然 MySQL、Oracle、PostgreSQL 中 MVCC 的实现方式不尽相同，但大都实现了非阻塞的读操作，写操作也只锁定必要的行。 MVCC 的实现，是通过保存数据在某个时间点的快照来实现的(Snapshot 快照图)。也就是说，不管需要执行多长时间，每个事务看到的数据都是一致的。根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。 另外，不同存储引擎的 MVCC 实现是不同的，典型的有乐观并发控制与悲观并发控制两种。下面简单地说一下 InnoDB 中的 MVCC 是如何工作的。 InnoDB 的 MVCC，是通过在每行记录后面保存两个隐藏的列来实现的。一个保存了行的创建时间，一个保存行的过期时间（或删除时间）。存储的并不是实际的时间值，而是系统版本号（system version number）。每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和査询到的每行记录的版本号进行比较。 下面看一下在 REPEATABLE READ 隔离级别下，MVCC 具体是如何操作的。 SELECT： InnoDB 会根据以下两个条件检查每行记录： InnoDB 只査找版本早于当前事务版本的数据行（行的系统版本号 $\\le$ 事务的系统版本号），这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的。 行的删除版本要么未定义，要么大于当前事务版本号。这可以确保事务读取到的行，在事务开始之前未被删除。 只有符合上述两个条件的记录，才能返回作为査询结果。 INSERT InnoDB 为新插人的每一行保存当前系统版本号作为行版本。 DELETE InnoDB 为删除的每一行保存当前系统版本号作为行删除标识。 UPDATE InnoDB 为插入一行新记录，保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为行删除标识。 保存这两个额外系统版本号，使大多数读操作都可以不用加锁。但是每行记录都需要额外的存储空间，需要做更多的行检查工作，以及一些额外的维护工作。 MVCC 只在 REPEATABLE READ 和 READ COMMITTED 两个隔离级别下工作。其他两个隔离级别都和 MVCC 不兼容, 因为 READ UNCOMMITTED 总是读取最新的数据行，而不是符合当前事务版本的数据行。而 SERIALIZABLE 则会对所有读取的行都加锁。 快照读与当前读新建一张表到demo数据库中，作测试用。 1234567CREATE TABLE &#96;dept&#96; ( &#96;id&#96; int(11) NOT NULL AUTO_INCREMENT, &#96;name&#96; varchar(20) DEFAULT NULL, PRIMARY KEY (&#96;id&#96;)) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;12 DEFAULT CHARSET&#x3D;utf8;insert into dept(name) values(&#39;后勤部&#39;); 然后打开两个 session 对话(两个查询窗口)，手动关闭事务自动提交set autocommit = 0;，进行如下操作： 手动时间戳 Transaction-1 Transaction-2 1 set autocommit = 0; set autocommit = 0; 2 start transaction; start transaction; 3 select * from dept; 4 insert into dept(name) values (&#39;研发部&#39;); 5 commit; 6 select * from dept; 7 commit; 8 select * from dept; 执行结果是在时间戳 $3$ 只查到一条数据，时间戳 $6$ 只查到一条数据，而时间戳 $8$ 则查到两条数据。 此时，InnoDB 的默认隔离级别是 R R，在我们没有加锁，没有加索引，没有其他影响因素，并且仅使用 MVCC 的情况下，似乎是没有发生幻读。 上面 Transaction-1 的操作是 读操作(select)，接下来是一个写操作(update)吧。 手动时间戳 Transaction-1 Transaction-2 1 set autocommit = 0; set autocommit = 0; 2 start transaction; start transaction; 3 select * from dept; 4 insert into dept(name) values (&#39;研发部&#39;); 5 commit; 6 update dept set name = &#39;行政部&#39;(没写where) 7 select * from dept; 8 commit; 最后的结果如下图所示，可以看到的是在执行到时间戳 $3$ 时，查询结果为 $3$ 条，但是到时间戳 $7$ 时，事务尚未提交，但是查询到的结果已经有 $4$ 条了，也就是在 Transaction-1 尚未结束前，Transaction-2 已经插入了，并不是 REPEATABLE READ，出现了幻读。 👨‍🏫：MVCC 解决幻读问题了吗？ 👨‍🎓：在一定程度上解决了幻读问题(select)，但是没有彻底解决幻读问题(update)。在 InnoDB 的 RR 级别下，通过 MVCC 虽然让数据变得可重复读，但是读到的数据是历史数据(事务开始前的快照，同一个事务内可重复读。)，而不是数据库最新的数据。读取历史数据的方式叫做快照读，而读取最新版本数据的方式，叫做当前读。 SELECT 快照读 当执行 SELECT 操作时 InnoDB 默认会执行快照读，记录下此次 SELECT 的结果，之后再 SELECT 的则返回此次快照结果，即使其他事务提交了不会影响当前 SELECT 的数据，也就是可重复读。快照的生成时间为当前事务中第一次执行 SELECT 时刻，也就是说假设 A 开启了事务，然后在执行 SELECT 操作前，事务 B 插入了一条数据然后 COMMIT，此时 A 执行 SELECT，结果中将会出现 B 新添加的那条数据。之后即使仍有事务 C、D、E 继续开启事务、提交数据、提交事务，事务 A 的 SELECT 结果不变，因为快照已经生成。 当前读 对于写操作(update、insert、delete)均采用当前读。执行这三个操作时会读取最新的记录，即使是别的事务新提交的数据也可以查询到。比如事务 A 准备要 UPDATE 一条数据，但是事务 B 已经 DELETE 这条数据并且 COMMIT ，此时事务 A 进行 UPDATE 将会失败，在 UPDATE 的时候需要知道最新的数据。正因如此，才导致上面 UPDATE 测试的那种情况。 SELECT 的当前读需要手动加锁： 123select * from table where ? lock in share mode;select * from table where ? for update; 所以 $BB$ 了这么久怎么解决幻读？🙄 最简单的方式，将上面的例子时间戳 $3$ 手动加锁，改为select * from dept for update;，此时 Transaction-2 将会阻塞。 InnoDB RR级别下如何避免幻读 刚刚看到 GitHub 上关于InnoDB 中 RR 隔离级别能否防止幻读？的讨论，很值得一读。 从上面一节可以看到在 RR 级别下没有彻底解决幻读，如果想要彻底解决的话有两种方式： 实现最高事务隔离级别 SERIALIZABLE； MVCC $+$ Next-Key Locks: next-key locks由 record locks(索引加锁) 和 gap locks(间隙锁，每次锁住的不光是需要使用的数据，还会锁住这些数据附近的数据)。 MVCC $+$ Next-Key Locks MySQL 官方 InnoDB Locking 先从上文中翻译几个锁的意思。 Record Locks「Record Locks」: 即行锁，比如select c1 from t where c1 = 10 for update;这条查询将会阻止其他事务对 t.c1 = 10 这条记录的 insert、update、delete。 「Record Locks」总是会锁住带索引的记录，即使一张表上并没有定义任何索引，InnoDB 也会隐式地创建一个聚簇索引(主键)，并且将这个索引用于 record locking. Gap Locks「Gap Locks」用于锁定索引记录之间的间隙，或者锁定(before the first index record)第一条索引记录之前或(after the last index record)最后一条索引记录之后。比如select c1 from t where c1 between 10 and 20 for update;，由于10-20之间的数据被锁定，可以防止其他事务将t.c1 = 15 的记录插入表中。 「Gap Locks」可以包含单个索引值，多个索引值，或者为空。 「Gap Locks」是性能与并发之间的一种权衡与妥协，只能应用在某些事务隔离级别之下，并不是所有事务隔离级别。 对于使用「唯一索引」来锁定唯一行的语句不需要使用 Gap Locks 而只会使用 Record Locks (这并不包括查询条件包含多列索引其中部分列的情况，在这种情况下 Gap Locks 仍会出现)。比如，如果id这列包含唯一索引(unique index)，下面的这条语句只会使用index-record来锁定id = 100的行，即使其他事务可以在id = 100之前的间隙插入数据。 1select * from child where id &#x3D; 100; 如果 id 没有索引或者是非唯一索引，上面这条语句仍然会锁住id = 100的前置区间。 总结一下： 用于锁住Index Record之间的间隙 如果是通过唯一索引来搜索一行记录的时候，不需要使用Gap Locks，此时Gap Locks降级为Record Locks Gap S-Lock与Gap X-Lock是兼容的 Gap Locks只能阻止其他事务在该Gap中插入记录，但无法阻止其他事务获取同一个Gap上的Gap Lock 可以通过将事务隔离级别设置为 READ COMMITTED 禁用Gap Locks。 Next-Key Locks Next-Key Locks = Record Locks + Gap Locks 若在id列上没有索引或者是非唯一索引，并且有id = 10、11、13、20这几条数据，那么可以锁定的区间为 $(-\\infty, 10],(10,11],(11,13],(13,20],(20,+\\infty)$，左开右闭。若执行select * from t where id = 13 for update; 将在id = 13上有一个 X Lock，在$(11,13)$有个Gap Lock id = 13的Next-Key 为$20$，将在id = 20上有一个 X Lock，在$(13,20)$有一个Gap Lock 因此，在id = 13上有一个X Lock，在$(11,20)$上有一个Gap Lock 也可以理解为在id = 13和id = 20上各有一个X Lock，在 $(11,13)$ 和 $(13, 20)$ 上各有一个Gap Lock 在 InnoDB 的默认隔离级别 RR 下，支持 Next-Key Locks。 各种锁测试 [InnoDB – Next-Key Lock](InnoDB – Next-Key Lock) RC、RR级别下的InnoDB的非阻塞读如何实现 其实主要就是上面提到的 MVCC. MySQL 在 RC 隔离级别下是如何实现读不阻塞的？ MySQL数据库事务各隔离级别加锁情况–read committed &amp;&amp; MVCC MySQL 锁锁的分类 按粒度划分：表级锁，行级锁，页级锁。 按读写划分：共享锁，排它锁。 按加锁方式划分：自动锁，显式锁。 按操作划分：$DML$锁，$DDL$锁。 按使用方式划分：乐观锁(通过版本号)，悲观锁。 锁的使用12345678910# MyISAM 的表级锁# read 读锁、共享锁 write 写锁、排它锁lock tables t_name read | write;unlock tables;# InnoDB 的行级锁# 加共享锁select * from t_name where id &#x3D; ? lock in share mode;# 加排它锁select * from t_name where id &#x3D; ? for update; 共享锁与排它锁的兼容性 X(排它锁) S(共享锁) X(排它锁) 冲突 冲突 S(共享锁) 冲突 兼容 隐式和显式锁定InnoDB 采用的是两阶段锁定协议（two- phase locking protocol）。在事务执行过程中，随时都可以执行锁定，锁只有在执行 COMMIT 或者 ROLLBACK 的时候才会释放，并且所有的锁是在同一时刻被释放。InnoDB 会根据隔离级别在需要的时候自动加锁。 参考 《高性能MySQL》第三版 MySQL的可重复读级别能解决幻读吗 - 掘金 InnoDB – Next-Key Lock","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://raymond-zhao.top/categories/MySQL/"}],"tags":[{"name":"MySQL 事务","slug":"MySQL-事务","permalink":"http://raymond-zhao.top/tags/MySQL-%E4%BA%8B%E5%8A%A1/"}]},{"title":"MySQL-索引与慢查询优化","slug":"2020-07-31-MySQL-Index","date":"2020-07-31T02:58:26.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/07/31/2020-07-31-MySQL-Index/","link":"","permalink":"http://raymond-zhao.top/2020/07/31/2020-07-31-MySQL-Index/","excerpt":"","text":"闲聊几句数据库范式目前关系数据库有六种范式：第一范式$(1NF)$、第二范式$(2NF)$、第三范式$(3NF)$、巴斯-科德范式$(BCNF)$、第四范式$(4NF)$和第五范式$(5NF$，又称完美范式)。 第一范式$(1NF)$：所有的域都应该是原子性的，即数据库表的每一列都是不可分割的原子数据项，而不能是集合，数组，记录等非原子数据项。 第二范式$(2NF)$：在$1NF$的基础上，非码属性必须完全依赖于候选码(在$1NF$基础上消除非主属性对主码的部分函数依赖)。 在$2NF$基础上，任何非主属性不依赖于其它非主属性(在$2NF$基础上消除传递依赖)。 在$3NF$基础上，任何非主属性不能对主键子集依赖(在$3NF$基础上消除对主码子集的依赖) 如何设计一个关系型数据库？首先应该考虑关系数据库管理系统($RDBMS$)，在 $RDBMS$ 中又包含两个重要组成部分，分别是： 程序实例：其中又包括存储管理、缓存机制、$SQL$ 解析、日志管理、权限划分、容灾机制、索引管理、锁管理 存储(文件系统)：硬盘等。 索引索引分类 聚簇索引与非聚簇索引 聚簇索引： 表数据是和主键一起存储，主键索引的叶结点存储行数据(包含了主键值)，二级索引的叶结点存储行的主键值。 使用$B+$树作为索引的存储结构，非叶子节点都是索引关键字，但非叶子节点中的关键字中不存储对应记录的具体内容或内容地址。叶子节点上的数据是主键与具体记录(数据内容)。 非聚簇索引： 表数据和索引是分成两部分存储的，主键索引和二级索引存储上没有任何区别。 使用$B+$树作为索引的存储结构，所有的节点都是索引，叶子节点存储的是索引$+$索引对应的记录的地址。 聚簇索引的优缺点 优点： 把相关数据保存在一起，减少$I/O$次数。例如实现电子邮箱时，可以根据用户ID来聚集数据，这样只需要从磁盘读取少数的数据页就能获取某个用户的全部邮件。如果没有聚簇索引，则每封邮件都可能多一次磁盘$I/O$。 数据访问更快。聚簇索引将索引和数据保存在同一个$B+Tree$中，因此从聚簇索引中获取数据通常比在非聚簇索引中查找要快。 使用覆盖索引扫描的查询可以直接使用页节点中的主键值。 缺点： 聚簇数据最大限度地提高了$I/O$密集型应用的性能，但如果数据全部放在内存中，则访问的顺序就没那么重要了，聚簇索引也就没什么优势了。 插入速度严重依赖于插入顺序。按照主键的顺序插入是加载数据到InnoDB表中速度最快的方式。但如果不是按照主键顺序加载数据，那么在加载完成后最好使用optimize table命令重新组织一下表。 更新聚簇索引列的代价很高，因为会强制$InnoDB$将每个被更新的行移动到新的位置。 基于聚簇索引的表插入新行，或者主键被更新导致需要移动行的时候，可能面临”页分裂（page split)“的问题。当行的主键值要求必须将这一行插入到某个已满的页中时，存储引擎会将该页分裂成两个页面来容纳该行，这就是一次分裂操作。页分裂会导致表占用更多的磁盘空间。 聚簇索引可能导致全表扫描变慢，尤其是行比较稀疏，或者由于页分裂导致数据存储不连续的时候。 二级索引(非聚簇索引)所占存储单元更大，因为二级索引的叶子节点包含了引用行的主键列。 二级索引访问需要两次索引查找，而不是一次。即在非索引覆盖的前提下，先到二级索引查询到主键，然后再按主键去查询得到数据。 主键与唯一索引的区别是什么？ 对比项 主键 唯一索引 功能 不仅是索引，还是约束。 是索引的一种 包含性 主键索引一定是唯一索引 唯一索引不一定是主键索引 空值 主键列不允许null 允许为null 外键 可以做外键 不可以 数量 只有一个主键 可以有多个唯一索引 优先级 执行计划优先级高于唯一索引 执行计划优先级低于主键 包含列 主键可以包含多个列，作为联合主键。 包含一列 索引的数据结构 树结构：二叉排序树、红黑树、B-树、B+树、 Hash结构：BitMap 位图索引是神器 缺点仅能满足 =, in 操作，不能使用范围查询 无法被用来避免数据的排序操作 不能利用部分索引键查询 不能避免表扫描遇到大量 Hash 值相同的情况，此时性能并不一定比 B-树 高。 来自灵魂的拷问 索引的最左匹配👩‍🏫：为什么索引是最左匹配，而不是最右匹配，不是中间匹配？ 👨‍🎓：$\\dots$ 最左前缀匹配原则 If the table has a multiple-column index, any leftmost prefix of the index can be used by the optimizer to look up rows. 如果在一个索引中包含多个列，那么这个索引的任何最左前缀均可以被优化器用于查询行。 MySQL Documentation - Multiple-Column Indexes 假如有联合索引 (a, b, c)，那么查询条件走索引的是 (a)、(a, b)、(a, b, c)。 特别注意：这里说的走索引其实是覆盖索引。 覆盖索引：如果一个索引包含（或者说覆盖）所有需要查询的字段的值，成为“覆盖索引”。 在 EXPLAIN 命令的 如果在 Extra 列中看到 “Using index“，说明 MySQL 正在使用覆盖索引。–《高性能 MySQL》 附录 D =&gt; Explain 中的列 =&gt; type 列 =&gt; index。（Page 728） MySQL会一直从左向右匹配直到遇到范围查询(&gt;, &lt;, between, like)就停止匹配，比如 a=3 and b=4 and c&gt;5 and d=6 , 如果建立 (a,b,c,d) 顺序的索引，则 d 是用不到索引的，因为到 c 就已经停止了，如果建立 a,b,d,c 的索引则都可以用到， a,b,d 的顺序可以任意调整。 =, in 可以乱序，比如建立 (a,b,c) 索引，那么 a=1 and b=2 and c=3 与a=1 and c=3 and b=2都可以命中索引，MySQL的查询优化器会帮助优化成索引可以识别的形式。 假如有如下这张表结构： 1234567create table &#96;student&#96; ( &#96;id&#96; int(11) not null auto_increment, &#96;name&#96; varchar(32) default null, &#96;cid&#96; int(11) default null, primary key (&#96;id&#96;), key &#96;name_cid_idx&#96; (&#96;name&#96;, &#96;cid&#96;)) engine &#x3D; InnoDB auto_increment&#x3D;8 default charset&#x3D;utf8; 插入五条数据后表为 id name cid 1 raymond 2 8 raymond 1 9 ramona 3 10 Bezos 4 11 Bill 6 如果执行下面这条查询，可以看到用了联合索引name_cid_idx，这是没有什么疑问的。而且rows = 1，filtered = 100。 12# 查询 1explain select * from student where name &#x3D; &#39;raymond&#39; and cid &#x3D; 2; 但是如果执行了下面两条查询呢？ 12345# 查询 2explain select * from student where cid &#x3D; 2;# 查询 3explain select * from student where cid &#x3D; 2 and name &#x3D; &#39;raymond&#39;; 查询结果分别如下(列名与上面图片相同，只放结果。) 从中可以看到，上面的查询 $2$ 使用了索引扫描(rows = 5)；查询 $3$ 的结果与查询 $1$ 的结果完全相同，说明$MySQL$ 的查询优化器为我们做了调整。 12341,SIMPLE,student,&lt;null&gt;,index,&lt;null&gt;,name_cid_idx,104,&lt;null&gt;,5,20,Using where; Using index1,SIMPLE,student,,ref,name_cid_idx,name_cid_idx,104,\"const,const\",1,100,Using index 那么为什么要强调最左匹配时要进行等值查询，而遇到范围查询时就失效呢？ $MySQL$ 创建符合索引的规则时首先会对符合索引的最左边的列，也就是第一个name字段的数据进行排序，在第一个排序的基础上再对第二个字段cid进行排序。其实就相当于进行了order by name asc, cid asc这样的排序。 所以如果对上面表格中的几条数据按name进行排序的话就会变成下面图片中的样子，既然按name排序，那么cid不再有序。 如果是等值的话，将是有序的。观察$4,5$列同名的raymond，可以看到cid便是有序的。 如果非等值(区间或范围)的话，将是无序的。观察$1,2,3$ 列可以看到，假如查询名字首字母在B-r之间的人，cid已经乱序，而在使用索引时需要顺序查询，但是cid依次为4,6,3，不能再按序查询。 所以，为什么$MySQL$默认是最左匹配而不是最右匹配，为还是没清楚。 密集索引与稀疏索引密集索引文件中的每个搜索码值都对应一个索引值 稀疏索引文件只为索引码的某些值建立索引项 MyISAM InnoDB 若一个主键被定义，则该主键为密集索引 若没有主键被定义，则该表的第一个唯一非空索引作为密集索引 若不满足以上条件，InnoDB内部会生成一个隐藏主键(密集索引) 非主键索引存储相关键位和其对应的主键值，包含两次查找","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://raymond-zhao.top/categories/MySQL/"}],"tags":[{"name":"MySQL 索引","slug":"MySQL-索引","permalink":"http://raymond-zhao.top/tags/MySQL-%E7%B4%A2%E5%BC%95/"}]},{"title":"数据结构-图","slug":"2020-07-21-DS-Graph","date":"2020-07-21T13:41:14.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/07/21/2020-07-21-DS-Graph/","link":"","permalink":"http://raymond-zhao.top/2020/07/21/2020-07-21-DS-Graph/","excerpt":"","text":"图的遍历与力扣中的一些题目前言每次说到”图”这个字，总是会想起一个名为 六度分割 的理论。 六度分隔理论（英语：Six Degrees of Separation）认为世界上任何互不相识的两人，只需要很少的中间人就能够建立起联系。 图的基本知识图是一种数据结构，现在应用的越来越广泛，如非关系型数据库Neo4j，但是目前图应用的最广泛的领域应该还是社交网络，被重多互联网公司用来分析用户间的关系，分析人际网络，朋友圈。 图由顶点($vertex$) 和 边($edge$) 来表示，其中顶点用来表示实体，而边则用来表示关系。 图的分类图可以分为无向图，有向图($Directed\\ Graph$)，以及加权图($Weighted\\ Graph$)三类。 无向图：边之间没有方向，可以假设他们现在没什么特殊的联系，也许就是在Twitter上看到了某人的Tweet。 有向图：边之间有方向，可以假设他们在Twitter上谁谁关注了谁谁。 加权图：边上有权重，可以假设为他们之间已经认识了多久的时间了。 图的表示方式依然记得本科的数据结构课本上提过图有两种表示方式，分别是邻接矩阵与邻接表。 邻接矩阵 邻接矩阵是一个正方形矩阵，其边长等于结点的个数。矩阵的元素通常为数值。在无向图中是关于主对角线对称的。在下图中，矩阵中的元素为 $1$ 则表示两人之间有关系，为 $0$ 则表示没有关系。 用邻接矩阵的方式来表示图的查询效率很高，但是，空间利用率却很低，有时候会变成稀疏矩阵。 邻接表 邻接表是一个列表数组，数组的大小等于图中的顶点数。数组特定索引处的列表表示与该顶点相邻的所有顶点。 用邻接矩阵的方式来表示图，比较难以创建，而且查询效率较低，但是空间利用率相对较高。 利用Java实现图在 $Java$ 里是没有提供图这个数据结构的，通过上面几部分可以知道，图的组成要素为顶点和边，那就手动创建一下吧。 123456789101112public class Vertex &#123; private String label; public Vertex() &#123;&#125; public Vertex(String label) &#123; this.label = label; &#125; // equals() and hashCode() 需要重写&#125; 上面定义的顶点内部属性为 $String$ ，考虑通用性，是应该设置成泛型的。 接下来就选择使用邻接表的方式来表示图。 12345678910public class Graph &#123; private Map&lt;Vertex, List&lt;Vertex&gt;&gt; adjVertices; public Graph() &#123;&#125; public Graph(Map&lt;Vertex, List&lt;Vertex&gt;&gt; adjVertices) &#123; this.adjVertices = adjVertices; &#125;&#125; 添加/删除顶点123456789101112131415/*** 在顶点集合中增加结点。*/public void addVertex(String label) &#123; adjVertices.putIfAbsent(new Vertex(label), new ArrayList&lt;&gt;());&#125;/*** 在顶点集合中删除结点。*/public void removeVertex(String label) &#123; Vertex v = new Vertex(label); adjVertices.values().stream().forEach(e -&gt; e.remove(v)); adjVertices.remove(new Vertex(label));&#125; 添加/删除边1234567891011121314151617181920212223/*** 创建一条新的边并更新相邻的顶点*/public void addEdge(String label1, String label2) &#123; Vertex v1 = new Vertex(label1); Vertex v2 = new Vertex(label2); adjVertices.get(v1).add(v2); adjVertices.get(v2).add(v1);&#125;/*** 删除边*/public void removeEdge(String label1, String label2) &#123; Vertex v1 = new Vertex(label1); Vertex v2 = new Vertex(label2); List&lt;Vertex&gt; eV1 = adjVertices.get(v1); List&lt;Vertex&gt; eV2 = adjVertices.get(v2); if (eV1 != null) eV1.remove(v2); if (eV2 != null) eV2.remove(v1);&#125; 构造图123456789101112131415public Graph createGraph() &#123; Graph graph = new Graph(); graph.addVertex(\"Bob\"); graph.addVertex(\"Alice\"); graph.addVertex(\"Mark\"); graph.addVertex(\"Rob\"); graph.addVertex(\"Maria\"); graph.addEdge(\"Bob\", \"Alice\"); graph.addEdge(\"Bob\", \"Rob\"); graph.addEdge(\"Alice\", \"Mark\"); graph.addEdge(\"Rob\", \"Mark\"); graph.addEdge(\"Alice\", \"Maria\"); graph.addEdge(\"Rob\", \"Maria\"); return graph;&#125; 获取某个顶点的邻接点123public List&lt;Vertex&gt; getAdjVertices(String label) &#123; return adjVertices.get(new Vertex(label));&#125; 图的遍历前面定义好了图的基本结构与方法，是时候开始真正的操作了。 DFS深度优先遍历从任意根顶点开始，并沿每个分支探索尽可能深的顶点，然后再探索同一级别的顶点。 涉及到深度的用栈，涉及到广度的用队列。这应该没什么疑问吧？ 1234567891011121314151617/*** 深度优先遍历*/public Set&lt;String&gt; depthFirstTraversal(Graph graph, String root) &#123; Set&lt;String&gt; visited = new LinkedHashSet&lt;&gt;(); Stack&lt;String&gt; stack = new Stack&lt;&gt;(); stack.push(root); while (!stack.isEmpty()) &#123; String vertex = stack.pop(); if (!visited.contains(vertex)) &#123; visited.add(vertex); for (Vertex v : graph.getAdjVertices(vertex)) stack.push(v.label); &#125; &#125; return visited;&#125; BFS12345678910111213141516public Set&lt;String&gt; breadthFirstTraversal(Graph graph, String root) &#123; Set&lt;String&gt; visited = new LinkedHashSet&lt;&gt;(); Queue&lt;String&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); visited.add(root); while (!queue.isEmpty()) &#123; String vertex = queue.poll(); for (Vertex v : graph.getAdjVertices(vertex)) &#123; if (!visited.contains(v.label)) &#123; visited.add(v.label); queue.add(v.label); &#125; &#125; &#125; return visited;&#125; 求图的强连通分量123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112import java.util.*;/** * @AUTHOR: raymond * @DATETIME: 2020/8/12 08:38 * DESCRIPTION: 寻找图中的最大强连通分量 * 采用邻接表表示 * 算法思想：一个具有最大强连通分量的有向图的逆转图仍然具有相同的强连通分量 **/public class KosarajuGraph &#123; private int V; private LinkedList&lt;Integer&gt;[] adj; public KosarajuGraph(int V) &#123; this.V = V; adj = new LinkedList[V]; for (int i = 0; i &lt; V; ++i) adj[i] = new LinkedList&lt;&gt;(); &#125; /** * 添加边 注意只有有向图才有强连通分量 * @param s 起点 * @param d 终点 */ public void addEdge(int s, int d) &#123; adj[s].add(d); &#125; /** * 深度优先遍历工具类 * @param s 起点 * @param visitedVertices 访问列表 */ public void DFSUtil(int s, boolean[] visitedVertices) &#123; visitedVertices[s] = true; System.out.print(s + \" \"); int n; for (Integer integer : adj[s]) &#123; n = integer; if (!visitedVertices[n]) DFSUtil(n, visitedVertices); &#125; &#125; /** * 逆转图 * @return 方向调转之后的图 */ public KosarajuGraph transpose() &#123; KosarajuGraph g = new KosarajuGraph(V); for (int s = 0; s &lt; V; s++) &#123; for (Integer integer : adj[s]) g.adj[integer].add(s); &#125; return g; &#125; public void fillOrder(int s, boolean[] visitedVertices, Deque&lt;Integer&gt; stack) &#123; visitedVertices[s] = true; for (int n : adj[s]) &#123; if (!visitedVertices[n]) fillOrder(n, visitedVertices, stack); &#125; stack.push(s); &#125; public void printSCC() &#123; Deque&lt;Integer&gt; stack = new ArrayDeque&lt;&gt;(); boolean[] visitedVertices = new boolean[V]; // 1. 初始化访问列表 for (int i = 0; i &lt; V; i++) visitedVertices[i] = false; for (int i = 0; i &lt; V; i++) &#123; if (!visitedVertices[i]) fillOrder(i, visitedVertices, stack); &#125; KosarajuGraph graph = transpose(); for (int i = 0; i &lt; V; i++) visitedVertices[i] = false; while (!stack.isEmpty()) &#123; int s = stack.pop(); if (!visitedVertices[s]) &#123; graph.DFSUtil(s, visitedVertices); System.out.println(); &#125; &#125; &#125; public static void main(String args[]) &#123; KosarajuGraph g = new KosarajuGraph(8); g.addEdge(0, 1); g.addEdge(1, 2); g.addEdge(2, 3); g.addEdge(2, 4); g.addEdge(3, 0); g.addEdge(4, 5); g.addEdge(5, 6); g.addEdge(6, 4); g.addEdge(6, 7); System.out.println(\"Strongly Connected Components:\"); g.printSCC(); &#125;&#125; Kruskal MST123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132import java.util.Arrays;/** * @AUTHOR: raymond * @DATETIME: 2020/8/12 09:08 * DESCRIPTION: Kruskal 算法生成最小生成树 O(e*lge) * 1. Sort all the edges from low weight to high * 2. Take the edge with the lowest weight and add it to the spanning tree. * If adding the edge created a cycle, then reject this edge. * 3. Keep adding edges until we reach all vertices. **/public class KruskalGraph &#123; private int vertices, edges; private Edge[] edge; public KruskalGraph(int v, int e) &#123; this.vertices = v; this.edges = e; this.edge = new Edge[edges]; for (int i = 0; i &lt; e; i++) edge[i] = new Edge(); &#125; public int find(Subset[] subsets, int i) &#123; if (subsets[i].parent != i) subsets[i].parent = find(subsets, subsets[i].parent); return subsets[i].parent; &#125; public void union(Subset[] subsets, int x, int y) &#123; int xRoot = find(subsets, x); int yRoot = find(subsets, x); if (subsets[xRoot].rank &lt; subsets[yRoot].rank) subsets[xRoot].parent = yRoot; else if (subsets[xRoot].rank &gt; subsets[yRoot].rank) subsets[yRoot].parent = xRoot; else &#123; subsets[yRoot].parent = xRoot; subsets[xRoot].rank++; &#125; &#125; /** * Kruskal 具体实现 */ public void kruskal() &#123; Edge[] res = new Edge[vertices]; int e = 0, i = 0; for (i = 0; i &lt; vertices; ++i) res[i] = new Edge(); // 1. 排序 Arrays.sort(edge); Subset[] subsets = new Subset[vertices]; for (i = 0; i &lt; vertices; ++i) subsets[i] = new Subset(); for (int v = 0; v &lt; vertices; ++v) &#123; subsets[v].parent = v; subsets[v].rank = 0; &#125; i = 0; while (e &lt; vertices - 1) &#123; Edge nextEdge = new Edge(); nextEdge = edge[i++]; int x = find(subsets, nextEdge.src); int y = find(subsets, nextEdge.dest); if (x != y) &#123; res[e++] = nextEdge; union(subsets, x, y); &#125; &#125; for (i = 0; i &lt; e; ++i) System.out.println(\"src-\" + res[i].src + \" - \" + \"dest-\" + res[i].dest + \" - \" + \"weight-\" + res[i].weight); &#125; public static void main(String[] args) &#123; int vertices = 6; // Number of vertices int edges = 8; // Number of edges KruskalGraph G = new KruskalGraph(vertices, edges); G.edge[0].src = 0; G.edge[0].dest = 1; G.edge[0].weight = 4; G.edge[1].src = 0; G.edge[1].dest = 2; G.edge[1].weight = 4; G.edge[2].src = 1; G.edge[2].dest = 2; G.edge[2].weight = 2; G.edge[3].src = 2; G.edge[3].dest = 3; G.edge[3].weight = 3; G.edge[4].src = 2; G.edge[4].dest = 5; G.edge[4].weight = 2; G.edge[5].src = 2; G.edge[5].dest = 4; G.edge[5].weight = 4; G.edge[6].src = 3; G.edge[6].dest = 4; G.edge[6].weight = 3; G.edge[7].src = 5; G.edge[7].dest = 4; G.edge[7].weight = 3; G.kruskal(); &#125; // 需要比较边的权重大小 static class Edge implements Comparable&lt;Edge&gt; &#123; int src, dest, weight; @Override public int compareTo(Edge o) &#123; return this.weight - o.weight; &#125; &#125; static class Subset &#123; int parent, rank; &#125;&#125; Prim MST123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import java.util.Arrays;/** * @AUTHOR: raymond * @DATETIME: 2020/8/12 09:33 * DESCRIPTION: Prim 算法生成最小生成树 O(ElgV) * 1. Initialize the minimum spanning tree with a vertex chosen at random. * 2. Find all the edges that connect the tree to new vertices, find the minimum and add it to the tree. * 3. Keep repeating step 2 until we get a minimum spanning tree. **/public class PrimGraph &#123; public void prim(int G[][], int V) &#123; int noEdge; // number of edge // 创建一个数组，用于跟踪已选择的边。已选为 true，否则为 false. boolean[] selected = new boolean[V]; // 初始化 Arrays.fill(selected, false); noEdge = 0; // Step 1: 选择第 0 个结点 selected[0] = true; // 打印边与权重表头 System.out.println(\"Edge : Weight\"); // 最小生成树中的边的数量总是会小于 V - 1 while (noEdge &lt; V - 1) &#123; int min = Integer.MAX_VALUE; int x = 0; // row number int y = 0; // col number for (int i = 0; i &lt; V; i++) &#123; // 对于集合 S 中的每个顶点，找到这个顶点的所有邻接顶点， if (selected[i]) &#123; for (int j = 0; j &lt; V; j++) &#123; // 未被选择并且两点之间存在边 if (!selected[j] &amp;&amp; G[i][j] != 0) &#123; // 比较 Step 1 中选择的顶点与其邻接点之间的距离 if (min &gt; G[i][j]) &#123; min = G[i][j]; x = i; y = j; &#125; &#125; &#125; &#125; &#125; System.out.println(x + \" - \" + y + \" : \" + G[x][y]); // 如果顶点已经在集合 S 中，丢弃。 // 否则选择另一个与 Step 1 选出的顶点最近的顶点 selected[y] = true; noEdge++; &#125; &#125; public static void main(String[] args) &#123; PrimGraph g = new PrimGraph(); // number of vertices in graph int V = 5; // 创建一个 5x5 大小的二维邻接矩阵 int[][] G = &#123; &#123; 0, 9, 75, 0, 0 &#125;, &#123; 9, 0, 95, 19, 42 &#125;, &#123; 75, 95, 0, 51, 66 &#125;, &#123; 0, 19, 51, 0, 31 &#125;, &#123; 0, 42, 66, 31, 0 &#125; &#125;; g.prim(G, V); &#125;&#125; 图的Java库 JGraphT Google Guava Apache Commons Java Universal Network / Graph（JUNG）","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://raymond-zhao.top/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"图","slug":"图","permalink":"http://raymond-zhao.top/tags/%E5%9B%BE/"}]},{"title":"进程、线程与线程池","slug":"2020-07-19-JUC-ProcessAndThread","date":"2020-07-19T14:48:52.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/07/19/2020-07-19-JUC-ProcessAndThread/","link":"","permalink":"http://raymond-zhao.top/2020/07/19/2020-07-19-JUC-ProcessAndThread/","excerpt":"","text":"进程什么是进程 狭义定义：进程是正在运行的程序的实例。 广义定义：进程是一个具有一定独立功能的程序关于某个数据集合的一次运行活动。它是操作系统动态执行的基本单元，在传统的操作系统中，进程既是基本的分配单元，也是基本的执行单元。 进程的概念主要有两点： 进程是一个实体(程序段、相关的数据段、PCB)：每一个进程都有它自己的地址空间，一般情况下，包括文本区域 $text\\ region$、数据区域 $data\\ region$ 和堆栈 $stack\\ region$。文本区域存储处理器执行的代码；数据区域存储变量和进程执行期间使用的动态分配的内存；堆栈区域存储着活动过程调用的指令和本地变量。 进程是一个“执行中的程序”：程序是一个没有生命的实体，只有处理器赋予程序生命时(操作系统执行之)，它才能成为一个活动的实体，我们称其为进程。进程由创建而产生，由调度而执行，由撤销而消亡。 《计算机操作系统》第三版对进程的定义是： 进程是程序的一次执行。 进程是一个程序及其数据在处理机上顺序执行时所发生的活动。 进程是程序在一个数据集合上运行的过程，它是系统进行资源分配和调度的一个独立单位。 进程的状态三态模型 就绪($Ready$)状态 当进程已分配到除 $CPU$ 以外的所有必要资源后，只要再获得 $CPU$，便可立即执行，进程这时的状态称为就绪状态。在一个系统中处于就绪状态的进程可能有多个，通常将它们排成一个队列，称为就绪队列。 执行状态 进程已获得 $CPU$，其程序正在执行。在单处理机系统中，只有一个进程处于执行状态；在多处理机系统中，则有多个进程处于执行状态。 阻塞状态 正在执行的进程由于发生某事件而暂时无法继续执行时，便放弃处理机而处于暂停状态，亦即进程的执行受到阻塞，把这种暂停状态称为阻塞状态，有时也称为等待状态或封锁状态。致使进程阻塞的典型事件有：请求$I/O$，申请缓冲空间等。通常将这种处于阻塞状态的进程也排成一个队列。有的系统则根据阻塞原因的不同而把处于阻塞状态的进程排成多个队列。 五态模型在不少系统中进程只有上述三种状态，但在另一些系统中，又增加了一些新状态，最重要的是挂起状态。引入挂起状态的原因有： 终端用户的请求。当终端用户在自己的程序运行期间发现有可疑问题时，希望暂时使自己的程序静止下来。 父进程请求。有时父进程希望挂起自己的某个子进程，以便考查和修改该子进程，或者协调各子进程间的活动。 负荷调节的需要。当实时系统中的工作负荷较重，已可能影响到对实时任务的控制时，可由系统把一些不重要的进程挂起，以保证系统能正常运行。 操作系统的需要。操作系统有时希望挂起某些进程，以便检查运行中的资源使用情况或进行记账。 在目前实际的系统中，为了管理的需要，还存在着两种比较常见的进程状态，即创建状态和终止状态。 创建状态 创建一个进程一般要通过两个步骤：首先，为一个新进程创建 $PCB$，并填写必要的管理信息；其次，把该进程转入就绪状态并插入就绪队列之中。 终止状态 进程的终止也要通过两个步骤：首先等待操作系统进行善后处理，然后将其 $PCB$ 清零，并将 $PCB$ 空间返还系统。 增加了创建状态和终止状态后，具有挂起状态的进程状态及转换图。 进程同步方式 信号量机制 整型信号量 记录型信号量 $AND$ 型信号量 信号量集 管程机制 管程由四部分组成：① 管程的名称；② 局部于管程内部的共享数据结构说明；③ 对该数据结构进行操作的一组过程；④ 对局部于管程内部的共享数据设置初始值的语句。 进程通信方式 管道/匿名管道(Pipes) ：用于具有亲缘关系的父子进程间或者兄弟进程之间的通信。 有名管道(Names Pipes) : 匿名管道由于没有名字，只能用于亲缘关系的进程间通信。为了克服这个缺点，提出了有名管道。有名管道严格遵循先进先出(first in first out)。有名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。 信号(Signal) ：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生； 消息队列(Message Queuing) ：消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识。管道和消息队列的通信数据都是先进先出的原则。与管道（无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是消息队列存放在内核中，只有在内核重启(即，操作系统重启)或者显示地删除一个消息队列时，该消息队列才会被真正的删除。消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比 FIFO 更有优势。消息队列克服了信号承载信息量少，管道只能承载无格式字 节流以及缓冲区大小受限等缺。 信号量(Semaphores) ：信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于进程间同步。这种通信方式主要用于解决与同步相关的问题并避免竞争条件。 共享内存(Shared memory) ：使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据的更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。可以说这是最有用的进程间通信方式。 套接字(Sockets) : 此方法主要用于在客户端和服务器之间通过网络进行通信。套接字是支持 TCP/IP 的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程。 进程调度算法 先到先服务(FCFS)调度算法 : 从就绪队列中选择一个最先进入该队列的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。 短作业优先(SJF)的调度算法 : 从就绪队列中选出一个估计运行时间最短的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。 时间片轮转调度算法 : 时间片轮转调度是一种最古老，最简单，最公平且使用最广的算法，又称 RR(Round robin)调度。每个进程被分配一个时间段，称作它的时间片，即该进程允许运行的时间。 多级反馈队列调度算法 ：前面介绍的几种进程调度的算法都有一定的局限性。如短进程优先的调度算法，仅照顾了短进程而忽略了长进程 。多级反馈队列调度算法既能使高优先级的作业得到响应又能使短作业（进程）迅速完成。，因而它是目前被公认的一种较好的进程调度算法，UNIX 操作系统采取的便是这种调度算法。 优先级调度 ： 为每个流程分配优先级，首先执行具有最高优先级的进程，依此类推。具有相同优先级的进程以 FCFS 方式执行。可以根据内存要求，时间要求或任何其他资源要求来确定优先级。 线程进程与线程的区别进程是一个独立的运行环境，而线程是在进程中执行的一个任务。他们两个本质的区别是是否单独占有内存地址空间及其它系统资源（比如I/O）： 进程单独占有一定的内存地址空间，所以进程间存在内存隔离，数据是分开的，数据共享复杂但是同步简单，各个进程之间互不干扰；而线程共享所属进程占有的内存地址空间和资源，数据共享简单，但是同步复杂。 进程单独占有一定的内存地址空间，一个进程出现问题不会影响其他进程，不影响主程序的稳定性，可靠性高；一个线程崩溃可能影响整个程序的稳定性，可靠性较低。 进程单独占有一定的内存地址空间，进程的创建和销毁不仅需要保存寄存器和栈信息，还需要资源的分配回收以及页调度，开销较大；线程只需要保存寄存器和栈信息，开销较小。 另外一个重要区别是，进程是操作系统进行资源分配的基本单位，而线程是操作系统进行调度的基本单位，即 $CPU$ 分配时间的单位 。 《深入理解Java虚拟机》第三版中提到，一种名为纤程的轻量级线程正在逐渐成熟。 进程与线程的关系 $Java$ 对操作系统提供的功能进行封装，包括进程与线程。 运行一个程序会产生一个进程，一个进程至少包含一个线程。 每个进程对应一个 $JVM$ 实例，多个线程共享 $JVM$ 里的堆。 $Java$ 采用单线程编程模型，程序会自动创建主线程。 主线程可以创建子线程，原则上要晚于子线程完成执行。 线程同步的方式为使系统中的多线程能有条不紊地运行，在系统中必须提供用于实现线程间同步和通信的机制。为了支持不同频率的交互操作和不同程度的并行性，在多线程 $OS$ 中通常提供多种同步机制，如互斥锁、条件变量、计数信号量以及多读、单写锁等。 互斥锁($mutex$) 互斥锁可以有开锁(unlock)和关锁(lock)两种状态。相应地，可用两条命令(函数)对互斥锁进行操作。其中的关锁lock操作用于将mutex关上，开锁操作unlock则用于打开mutex。 当一个线程需要读/写一个共享数据段时，线程首先应为该数据段所设置的mutex执行关锁命令。 命令首先判别mutex的状态，如果它已处于关锁状态，则试图访问该数据段的线程将被阻塞；而如果mutex是处于开锁状态，则将mutex关上后便去读/写该数据段。 在线程完成对数据的读/写后，必须再发出开锁命令将mutex打开，同时还须将阻塞在该互斥锁上的一个线程唤醒，其它的线程仍被阻塞在等待mutex打开的队列上。 条件变量每一个条件变量通常都与一个互斥锁一起使用，亦即，在创建一个互斥锁时便联系着一个条件变量。单纯的互斥锁用于短期锁定，主要是用来保证对临界区的互斥进入。而条件变量则用于线程的长期等待，直至所等待的资源成为可用的资源。 信号量机制 就是信号量机制。 线程创建 继承Thread类 重写Thread::run； 调用Thread::start。 实现Runnable接口，无返回值。 获取Runnable接口的实现类，作为参数，创建Thread； 执行Thread::start。 实现Callable接口，结合FutureTask使用，有返回值。 以Callable的实现类为参数，创建FutureTask； 将FutureTask作为Thread的参数，创建Thread; 通过Thread::start启动线程； 通过FutureTask::get阻塞获取线程返回值。 利用线程池Executor 创建Callable或Runnable任务，提交到线程池； 通过返回的Future::get获取返回结果。 线程状态 $NEW$：新建，创建后尚未启动。 $RUNNABLE$：运行，包含 $Running$ 和 $Ready$ $BLOCKED$：阻塞，等待获取排它锁 $WAITING$ ：等待，不会被分配 CPU 时间，需要显示被唤醒 $TIMED-WAITING$ ：限期等待，在一定时间后会由系统自动唤醒 $TERMINATED$： 终止，已终止线程的状态，线程已执行结束。 start() 与 run() start() : 通过start()方法来启动的新线程，处于就绪状态，并没有运行，一旦得到 $CPU$ 时间片，就开始执行相应线程的run()方法，这里方法run()称为线程体，它包含了要执行的这个线程的内容，run()方法运行结束，此线程随即终止。start()不能被重复调用。用start()方法来启动线程，真正实现了多线程运行，即无需等待某个线程的run()方法体代码执行完毕就直接继续执行下面的代码，即进行了线程切换。 run() : run() 就和普通的成员方法一样，可以被重复调用。如果直接调用run()方法，并不会启动新线程！程序中依然只有主线程这一个线程，其程序执行路径还是只有一条，还是要顺序执行，还是要等待run()方法体执行完毕后才可继续执行下面的代码，这样就没有达到多线程的目的。 总结：调用 start() 方法会创建一个新的子线程并启动， run() 方法只是 Thread 的一个方法调用，还是在原线程里运行。 如何给run()方法传参 构造函数传参、成员变量传参、回调函数传参。 sleep() 与 wait() sleep() 是 Thread 类的方法， wait() 是 Object 类的方法。 sleep() 方法可以在任何地方使用；wait() 方法只能在 synchronized 方法或 synchronized 块中使用。 Thread.sleep() : 只会让出$CPU$，不会导致锁行为的改变；Object.wait() : 不仅让出 $CPU$，还会释放已经占有的同步资源锁。 notify() 与 notifyAll() 锁池($Entry\\ List$)：假设线程 $A$ 已经拥有了某个对象(注意:不是类)的锁，而其它的线程想要调用这个对象的某个 synchronized 方法(或者 synchronized块)，由于这些线程在进入对象的 synchronized 方法之前必须先获得该对象的锁的拥有权，但是该对象的锁目前正被线程 $A$ 拥有，所以这些线程就进入了该对象的锁池中。 等待池($Wait\\ Set$): 假设一个线程 $A$ 调用了某个对象的 wait() 方法，线程 $A$ 就会释放该对象的锁(因为 wait() 方法必须出现在 synchronized 中，这样自然在执行 wait() 方法之前线程 $A$ 就已经拥有了该对象的锁)，同时线程 $A$ 就进入到了该对象的等待池中，进入到等待池中的线程不会去竞争该对象的锁。如果另外的一个线程调用了相同对象的 notifyAll() 方法，那么处于该对象的等待池中的线程就会全部进入该对象的锁池中，准备争夺锁的拥有权。如果另外的一个线程调用了相同对象的 notify() 方法，那么仅仅有一个处于该对象的等待池中的线程(随机)会进入该对象的锁池。 notify() : 只会随机选取一个处于等待池中的线程进入锁池去竞争获取锁的机会。 notifyAll() : 会让所有处于等待池中的线程全部进入锁池去竞争获取锁的机会。 yield()当调用 Thread.yield() 方法时，会给线程调度器一个当前线程愿意让出 $CPU$ 使用的暗示，但是线程调度器可能会忽略这个暗示。 如何实现处理线程的返回值 主线程等待 使用 Thread::join 方法阻塞当前线程以等待子线程处理完毕 通过 Callable 接口实现：通过 FutureTask&lt;&gt; 或者 Executor 获取 如何中断线程 调用 interrupt() ，通知线程应该中断了 如果线程处于被阻塞状态，那么线程将立即退出被阻塞状态，并抛出一个 InterruptedException 异常。 如果线程处于正常活动状态，那么会将该线程的中断标志设置为 true ，被设置中断标志的线程将继续正常运行，不受影响。 线程池为什么要使用线程池线程池提供了一种限制和管理资源(包括执行一个任务)， 每个线程池还维护一些基本统计信息，例如已完成任务的数量。 降低资源消耗：通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度：当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性：线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 线程池的使用其实非常广泛，比如MySQL和Tomcat为客户端连接准备的线程池。 线程池集合 上图中有三个重要的 Executor 接口 Executor：运行新任务的简单接口，将任务提交和任务执行细节解耦。 ExecutorService：具备管理执行器和任务生命周期的方法，提交任务机制更完善。 ScheduledExecutorService：支持 Future 和定期执行任务 线程池参数1234567891011121314151617public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; // corePoolSize: 核心线程数量 // maximumPoolSize: 线程不够用时能够创建的最大线程数 // keepAliveTime: 存活等待时间 // unit: 时间单位 // workQueue: 任务等待队列 // threadFactory: 默认 Executors.defaultThreadFactory() // handler: 线程池饱和时的拒绝策略 // ...&#125; 阻塞队列BlockingQueue workQueue：阻塞队列，维护着等待执行的Runnable任务对象。 常用的几个阻塞队列： LinkedBlockingQueue：链式阻塞队列，底层数据结构是链表，默认大小是Integer.MAX_VALUE，也可以指定大小。 ArrayBlockingQueue：数组阻塞队列，底层数据结构是数组，需要指定队列的大小。 SynchronousQueue：同步队列，内部容量为0，每个put操作必须等待一个take操作，反之亦然。 DelayQueue：延迟队列，该队列中的元素只有当其指定的延迟时间到了，才能够从队列中获取到该元素 。 线程工厂创建线程的工厂 ，用于批量创建线程，统一在创建线程时设置一些参数，如是否守护线程、线程的优先级等。如果不指定，会新建一个默认的线程工厂。 1234567891011121314static class DefaultThreadFactory implements ThreadFactory &#123; // ... DefaultThreadFactory() &#123; SecurityManager s = System.getSecurityManager(); group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); namePrefix = \"pool-\" + poolNumber.getAndIncrement() + \"-thread-\"; &#125; // ...&#125; 饱和策略线程池饱和时的拒绝策略 AbortPolicy : 直接抛出异常，这是默认策略 CallerRunsPolicy : 用调用者所在的线程来执行任务 DiscardOldestPolicy : 丢弃队列中最靠前的任务，并执行当前任务 DiscardPolicy : 直接丢弃任务 实现 RejectedExecutionHandler 接口的自定义 Handler 线程池工作流程处理任务的核心方法是execute() 1234567891011121314151617181920212223242526// JDK 1.8 public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); // 1.当前线程数小于 corePoolSize, 则调用 addWorker 创建核心线程执行任务. if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; // 2.如果不小于 corePoolSize, 则将任务添加到 workQueue 队列. if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); // 2.1 如果 isRunning 返回 false (状态检查), 则 remove 这个任务, 然后执行拒绝策略. if (!isRunning(recheck) &amp;&amp; remove(command)) reject(command); // 2.2 线程池处于 running 状态, 但是没有线程, 则创建线程. else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; // 3.如果放入 workQueue 失败, 则创建非核心线程执行任务. // 如果这时创建非核心线程失败(当前线程总数不小于 maximumPoolSize 时), 就会执行拒绝策略. else if (!addWorker(command, false)) reject(command);&#125; 新任务提交 execute 执行后的判断 如果运行的线程少于 corePoolSize ，则创建新线程来处理任务，即使线程池中的其它线程是空闲的。 当 workQueue 未满，线程池中的线程数量大于等于 corePoolSize 且小于 maximumPoolSize，将任务加到workQueue中进行等待，等待空闲coreThread来处理任务。 当 workQueue 已满，并且正在运行的线程数量大于等于corePoolSize并且小于maximumPoolSize，才创建新的线程去处理任务。 如果运行的线程数量大于等于 maximumPoolSize ，这时如果 workQueue 已经满了，则通过 Handler 所制定的策略来处理任务。 线程复用当一个线程被创建的时候会被指定一个任务，当执行完这个任务之后线程会自动销毁。但是线程池可以复用线程，也就是线程在执行完任务后不被销毁，继续执行其他的任务。那么，线程池是如何做到线程复用的呢？ 其实，ThreadPoolExecutor在创建线程时，会将线程封装成工作线程worker，并放入工作线程组中，然后这个工作线程worker反复从阻塞队列中取任务执行。 瞥一眼工作线程Worker 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960private final class Worker extends AbstractQueuedSynchronizer implements Runnable &#123; // 这个类不会被序列化, 提供了序列号只是为了祛除 javac 的警告. private static final long serialVersionUID = 6138294804551838833L; /** Thread this worker is running in. Null if factory fails. */ final Thread thread; /** Initial task to run. Possibly null. */ Runnable firstTask; /** 每个线程完成的任务数量 */ volatile long completedTasks; // 根据从线程工厂获得的 first task 和 thread 创建 Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); &#125; /** Delegates main run loop to outer runWorker */ public void run() &#123; runWorker(this); &#125; // 重写继承自 AQS 的方法 // 0 代表解锁状态 1 代表加锁状态 protected boolean isHeldExclusively() &#123; return getState() != 0; &#125; protected boolean tryAcquire(int unused) &#123; if (compareAndSetState(0, 1)) &#123; setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false; &#125; protected boolean tryRelease(int unused) &#123; setExclusiveOwnerThread(null); setState(0); return true; &#125; public void lock() &#123; acquire(1); &#125; public boolean tryLock() &#123; return tryAcquire(1); &#125; public void unlock() &#123; release(1); &#125; public boolean isLocked() &#123; return isHeldExclusively(); &#125; void interruptIfStarted() &#123; Thread t; if (getState() &gt;= 0 &amp;&amp; (t = thread) != null &amp;&amp; !t.isInterrupted()) &#123; try &#123; t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; &#125; &#125;&#125; 详细分析 线程池的状态 RUNNING : 线程池刚创建后的状态，能接受新提交的任务，并且也能处理阻塞队列中的任务。 SHUTDOWN : 调用shutdown()方法后，不再接受新提交的任务，但可以处理存量任务。 STOP : 调用shutdownNow()方法后，不再接受新提交的任务，也不再处理存量任务。中断所有线程，阻塞队列中没有执行完的任务全部丢弃。 TIDYING : 所有的任务都已终止，ctl记录的任务数量为 $0$，接下来会执行terminated()进入$TERMINATED$ 状态。 TERMINATED : terminated() 方法执行完后进入该状态。 线程池的大小如何选定 $CPU$ 密集型：线程数 $=$ 核数或者核数 $ + \\ 1$ $IO$ 密集型：线程数 $=$ $CPU$核数 $\\times$ ($1 +$ 平均等待时间/平均工作时间) 常见的线程池Executors类提供了几个静态方法用来创建线程池，但是在实际项目中并不推荐直接使用，而是采用手动创建的方式。这样能够提醒创建者更加明确实际任务，避免资源耗尽的风险。 newSingleThreadExecutor()123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; 有且仅有一个核心线程(corePoolSize == maximumPoolSize == 1)，使用了LinkedBlockingQueue(容量很大)，所以，不会创建非核心线程，所有任务按照先来先执行的顺序执行。如果这个唯一的线程不空闲，那么新来的任务会存储在任务队列里等待执行。 newCachedThreadPool()12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 运行流程： 将任务提交到线程池； 因为corePoolSize = 0，不必创建核心线程，maximumPoolSize为Integer.MAX_VALUE，即2147483647； 尝试将任务添加到SynchronousQueue队列； 如果入队成功，等待被空闲线程拉取并执行，如果没有空闲线程，那么创建一个非核心线程，然后从队列中拉去任务并执行； 如果队列中已有任务在等待，入队操作将被阻塞。 当需要执行很多短时间的任务时，CacheThreadPool的线程复用率比较高， 会显著地提高性能。而且线程 $60s$ 后会回收，意味着即使没有任务进来，CacheThreadPool并不会占用很多资源。 newFixedThreadPool()12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; corePoolSize == maximumPoolSize，都为传入参数nThreads。所以只能创建核心线程，不能创建非核心线程。因为LinkedBlockingQueue的默认大小为Integer.MAX_VALUE。所以如果核心线程空闲，则交给核心线程处理，否则入队等待。等有核心线程空闲时进行处理。 与CachedThreadPool的区别： corePoolSize == maximumPoolSize，只创建核心线程；而CachedThreadPool的corePoolSize == 0，所以只会创建非核心线程。 getTask()方法获取任务时：线程会阻塞在LinkedBlockingQueue.take()，线程不会被回收；CachedThreadPool会在 $60s$ 后被回收。 由于线程不会被回收，会一直卡在阻塞，所以没有任务的情况下，FixedThreadPool占用资源更多。 都几乎不会触发拒绝策略，但是原理不同。FixedThreadPool是因为阻塞队列可以很大，故几乎不会触发拒绝策略；CachedThreadPool是因为线程池很大，几乎不会导致线程数量大于最大线程数，故几乎不会触发拒绝策略。 newScheduledThreadPool()创建一个定长线程池，支持定时及周期性任务执行。 12345678910public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize);&#125;// ScheduledThreadPoolExecutor():public ScheduledThreadPoolExecutor(int corePoolSize) &#123; super(corePoolSize, Integer.MAX_VALUE, DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS, new DelayedWorkQueue());&#125; newWorkStealingPool()123456public static ExecutorService newWorkStealingPool(int parallelism) &#123; return new ForkJoinPool (parallelism, ForkJoinPool.defaultForkJoinWorkerThreadFactory, null, true);&#125; 内部会构建 ForkJoinPool ，利用 work-stealing 算法，并行地处理任务，不保证处理顺序。 Fork/Join 框架：把大任务分割成若干个小任务并行执行，最终汇总每个小任务结果后得到大任务结果的框架。 扩展阅读： 深入理解 Java 线程池：ThreadPoolExecutor 《计算机操作系统》 第三版 汤子瀛","categories":[{"name":"多线程与并发","slug":"多线程与并发","permalink":"http://raymond-zhao.top/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程与线程池","slug":"线程与线程池","permalink":"http://raymond-zhao.top/tags/%E7%BA%BF%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%E6%B1%A0/"}]},{"title":"CAS与Unsafe源码笔记","slug":"2020-07-16-JUC-CASAndUnsafe","date":"2020-07-16T14:05:38.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/07/16/2020-07-16-JUC-CASAndUnsafe/","link":"","permalink":"http://raymond-zhao.top/2020/07/16/2020-07-16-JUC-CASAndUnsafe/","excerpt":"","text":"CAS与Unsafe源码笔记乐观锁与悲观锁乐观锁总是假设最好的情况，每次去操作数据的时候都认为别人不会修改，所以不会加锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS实现。在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。乐观锁适用于读多写少的应用场景，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。 悲观锁总是假设最坏的情况，每次去操作数据的时候都认为别人会修改，所以每次在拿数据的时候都会先上锁，这样别人想拿这个数据就会阻塞，直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在操作之前先上锁。$Java$中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。悲观锁比较适合多写的场景。 CAS作用原理 $CAS(Compare \\ And \\ Swap)$ 也就是比较并交换的意思，在 $CAS$ 中有三个参数： $V$ : 要更新的变量( $var$ ) $E$ : 期望值( $expected$ ) $U$: 新值( $update$ ) 在 $CAS$ 操作过程中，判断 $V$ 是否等于 $E$，如果等于，将 $V$ 的值设置为 $U$；如果不等，说明已经有其它线程更新了 $V$，则当前线程放弃更新，什么都不做。如果用代码可以表示为： 12345678public boolean compareAndSwap(int value, int expected, int update) &#123; if (value == expect) &#123; value = update; return true; &#125; else &#123; return false; &#125;&#125; AtomicIntegerAtomicInteger是java.util.concurrent.atomic包下的一个原子类，那么AtomicInteger是怎么实现原子性的呢？点开源码看到： 123private static final Unsafe unsafe = Unsafe.getUnsafe(); // Unsafe 类private static final long valueOffset; // 偏移量private volatile int value; // 整型变量 往下随手一翻可以看到很多使用了 $CAS$ 的方法 123456789101112public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update);&#125;public final int getAndAccumulate(int x, IntBinaryOperator accumulatorFunction) &#123; int prev, next; do &#123; prev = get(); next = accumulatorFunction.applyAsInt(prev, x); &#125; while (!compareAndSet(prev, next)); return prev;&#125; Unsafe以AtomicInteger的源码中可以看到一个动作出现了不止一次。 1234567public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update);&#125;public final boolean weakCompareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update);&#125; 这个理的unsafe就是前面所说的Unsafe对象，加上限定名后可以表示为： 1private static final jdk.internal.misc.Unsafe U = jdk.internal.misc.Unsafe.getUnsafe(); 我们拿下面这个变量来分析一下。 1public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5); compareAndSwapInt(x, x, x, x)这个方法是比较内存中的一个整形值是否和我们的期望值var4一样，如果一样则将内存中的这个值更新为var5，参数var1是值所在的对象(AtomicInteger)，参数var2是值在对象中var1的内存偏移量，它们的组合作用是计算出值所在的内存的地址。 看到native这和关键字就应该想到这已经不是java语言编写的库了，通常是C/C++编写的。 在此借用 薛8的一张图。 Unsafe.java 将对象引用、值在对象中的偏移量、期望的值和打算更新的新值传递给Unsafe.cpp 如果值更新成功则返回true给调用者，否则返回false Unsafe.cpp 接收从Unsafe传递过来的对象引用、偏移量、期望值与更新值，根据对象引用和偏移量计算出值的地址，然后将值地址，期望值，更新值传递给$CPU$ 如果更新成功则返回true给Unsafe.java，否则返回false CPU 接收从Unsafe.cpp传递过来的地址、期望值、更新值，执行指令 $cmpxchg(x, addr, e)$，比较地址中的值是否和期望值一样。是则更新，否则什么也不做。在此cmpxchg可以理解为 $(compare\\ and\\ change)$，也就是比较并改变的意思。 将操作结果返回给Unsafe.cpp CAS实现原子操作的代价ABA问题所谓 $ABA$ 问题，也就是说一个值原来是 $A$ ，然后被改成了 $B$ ，最后又被改成了 $A$ 。而 $CAS$ 比较的重点就是它期待的值是不是 $A$ ，如果是就更新。 $A$ ：“二狗，你再也不是当年的那个二狗了。” 二狗：“$A$，你再也不是当时的那个 $A$ 了。” 但是 $CAS$ 本身是检查不出这个变化的。 为了解决这个问题，可以使用版本号机制或者时间戳，也就是加上时间的判断(现在的我们总还不能与时间为敌吧？)，在 $MySQL$ 的写操作常常会加上这个，在 $J.U.C$ 下也提供了相应的实现，也就是java.util.concurrent.atomic下的AtomicStampedReference。 这个类的compareAndSet方法的作用是首先检查当前引用是否等于预期引用，并且检查当前标志是否等于预期标志，如果二者都相等，才使用 $CAS$ 设置为新的值和标志。 123456789101112public boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) &#123; Pair&lt;V&gt; current = pair; return expectedReference == current.reference &amp;&amp; expectedStamp == current.stamp &amp;&amp; ((newReference == current.reference &amp;&amp; newStamp == current.stamp) || casPair(current, Pair.of(newReference, newStamp)));&#125; 循环开销可能会很大$CAS$多与自旋结合。如果自旋 $CAS$ 长时间不成功，会占用大量的 $CPU$ 资源。 解决思路是 让 $JVM$ 支持处理器提供的 $pause$ 指令。$pause$ 指令能让自旋失败时 $CPU$ 睡眠一小段时间再继续自旋，从而使得读操作的频率低很多，为解决内存顺序冲突而导致的 $CPU$ 流水线重排的代价也会小很多。 限制自旋次数。比如重试 $10$ 次失败后暂时放弃，歇歇再来。 只能保证一个共享变量的原子操作CAS的原子操作只能针对一个共享变量，可以用以下两种方法解决。 使用 $JDK 1.5$ 开始就提供的AtomicReference类保证对象之间的原子性，把多个变量放到一个对象里面进行 $CAS$ 操作 使用锁，锁内的临界区代码可以保证只有当前线程能操作。 CAS的应用CAS操作并不会锁住共享变量，是一种非阻塞的同步机制，CAS就是乐观锁的实现。 Java利用CAS的乐观锁、原子性的特性高效解决了多线程的安全性问题，例如 $JDK1.8$ 中的集合类ConcurrentHashMap、关键字volatile、ReentrantLock等。 参考 面试必备之乐观锁与悲观锁 CAS原理分析及ABA问题详解 CAS与原子操作","categories":[{"name":"多线程与并发","slug":"多线程与并发","permalink":"http://raymond-zhao.top/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"CAS","slug":"CAS","permalink":"http://raymond-zhao.top/tags/CAS/"}]},{"title":"LeetCode多线程题目","slug":"2020-07-13-LeetCode-Concurrency","date":"2020-07-13T06:00:09.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/07/13/2020-07-13-LeetCode-Concurrency/","link":"","permalink":"http://raymond-zhao.top/2020/07/13/2020-07-13-LeetCode-Concurrency/","excerpt":"","text":"LeetCode多线程题目1114. 按序打印12345678910111213141516171819202122232425262728我们提供了一个类：public class Foo &#123; public void one() &#123; print(\"one\"); &#125; public void two() &#123; print(\"two\"); &#125; public void three() &#123; print(\"three\"); &#125;&#125;三个不同的线程将会共用一个 Foo 实例。线程 A 将会调用 one() 方法线程 B 将会调用 two() 方法线程 C 将会调用 three() 方法请设计修改程序，以确保 two() 方法在 one() 方法之后被执行，three() 方法在 two() 方法之后被执行。- 示例1输入: [1,2,3]输出: \"onetwothree\"解释: 有三个线程会被异步启动。输入 [1,2,3] 表示线程 A 将会调用 one() 方法，线程 B 将会调用 two() 方法，线程 C 将会调用 three() 方法。正确的输出是 \"onetwothree\"。- 示例2输入: [1,3,2]输出: \"onetwothree\"解释: 输入 [1,3,2] 表示线程 A 将会调用 one() 方法，线程 B 将会调用 three() 方法，线程 C 将会调用 two() 方法。正确的输出是 \"onetwothree\"。 Solution 1 - 原子类1234567891011121314151617181920212223242526272829class Foo &#123; private AtomicInteger firstJobDone = new AtomicInteger(0); private AtomicInteger secondJobDone = new AtomicInteger(0); public Foo() &#123;&#125; public void first(Runnable printFirst) throws InterruptedException &#123; printFirst.run(); // \"first\" // 确保打印 first 的任务完成后进行原子增加 firstJobDone.incrementAndGet(); &#125; public void second(Runnable printSecond) throws InterruptedException &#123; while (firstJobDone.get() != 1) &#123; // 如果 first 任务还未完成则进行等待 &#125; printSecond.run(); // \"second\" // 确保打印 second 的任务完成后进行原子增加 secondJobDone.incrementAndGet(); &#125; public void third(Runnable printThird) throws InterruptedException &#123; while (secondJobDone.get() != 1) &#123; // 如果 second 任务还未完成则进行等待 &#125; printThird.run(); // \"third\" &#125;&#125; Solution 2 - synchronized12345678910111213141516171819202122232425262728293031323334353637class Foo &#123; private boolean firstDone; private boolean secondDone; private Object lock = new Object(); public Foo() &#123;&#125; public void first(Runnable printFirst) throws InterruptedException &#123; synchronized(lock) &#123; // printFirst.run() outputs \"first\". Do not change or remove this line. printFirst.run(); firstDone = true; lock.notifyAll(); &#125; &#125; public void second(Runnable printSecond) throws InterruptedException &#123; synchronized(lock) &#123; while (!firstDone) lock.wait(); // printSecond.run() outputs \"second\". Do not change or remove this line. printSecond.run(); secondDone = true; lock.notifyAll(); &#125; &#125; public void third(Runnable printThird) throws InterruptedException &#123; synchronized(lock) &#123; while (!secondDone) lock.wait(); // printThird.run() outputs \"third\". Do not change or remove this line. printThird.run(); &#125; &#125;&#125; Solution 3 - volatile123456789101112131415161718192021222324252627282930class Foo &#123; private volatile boolean firstJobDone = false; private volatile boolean secondJobDone = false; public Foo() &#123;&#125; public void first(Runnable printFirst) throws InterruptedException &#123; // printFirst.run() outputs \"first\". Do not change or remove this line. printFirst.run(); firstJobDone=true; &#125; public void second(Runnable printSecond) throws InterruptedException &#123; while (!firstJobDone)&#123; // 等待printFirst完成 &#125; // printSecond.run() outputs \"second\". Do not change or remove this line. printSecond.run(); secondJobDone=true; &#125; public void third(Runnable printThird) throws InterruptedException &#123; while (!secondJobDone)&#123; // 等待printSecond完成 &#125; // printThird.run() outputs \"third\". Do not change or remove this line. printThird.run(); &#125;&#125; Solution 4 - 可重入锁12345678910111213141516171819202122232425262728293031323334class Foo &#123; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); private int count = 1; public Foo() &#123;&#125; public void first(Runnable printFirst) throws InterruptedException &#123; lock.lock(); printFirst.run(); count = 2; condition.signalAll(); lock.unlock(); &#125; public void second(Runnable printSecond) throws InterruptedException &#123; lock.lock(); if (count != 2) condition.await(); printSecond.run(); count = 3; condition.signal(); lock.unlock(); &#125; public void third(Runnable printThird) throws InterruptedException &#123; lock.lock(); while (count != 3) condition.await(); printThird.run(); lock.unlock(); &#125;&#125; Solution 4 - CountDownLatch123456789101112131415161718192021222324252627282930313233public class Foo &#123; // 声明两个 CountDownLatch变量 private CountDownLatch countDownLatch1; private CountDownLatch countDownLatch2; public Foo() &#123; // 初始化每个CountDownLatch 的值为1，表示有一个线程执行完后，执行等待的线程 countDownLatch01 = new CountDownLatch(1); countDownLatch02 = new CountDownLatch(1); &#125; public void first(Runnable printFirst) throws InterruptedException &#123; // 当前只有 first 线程没有任何的阻碍，其余两个线程都处于等待阶段 // printFirst.run() outputs \"first\". Do not change or remove this line. printFirst.run(); // 直到 CountDownLatch1 里面计数为 0 才执行因调用该 countDownCatch1.await() 而等待的线程 countDownLatch1.countDown(); &#125; public void second(Runnable printSecond) throws InterruptedException &#123; // 只有 countDownLatch1 为 0 才能通过，否则会一直阻塞 countDownLatch1.await(); // printSecond.run() outputs \"second\". Do not change or remove this line. printSecond.run(); // 直到 CountDownLatch2 里面计数为 0 才执行因调用该 countDownCatch2.await() 而等待的线程 countDownLatch2.countDown(); &#125; public void third(Runnable printThird) throws InterruptedException &#123; // 只有 countDownLatch2 为 0 才能通过，否则会一直阻塞 countDownLatch2.await(); // printThird.run() outputs \"third\". Do not change or remove this line. printThird.run(); &#125;&#125; Solution 5 - Semaphore12345678910111213141516171819202122232425262728293031323334public class Foo03 &#123; // 声明两个Semaphore 变量 private Semaphore spa,spb; public Foo03() &#123; // 初始化 Semaphore 为 0 的原因 // 如果这个Semaphore 为零，如果另一线程调用(acquire)这个Semaphore就会产生阻塞 // 便可以控制 second 和 third 线程的执行 spa = new Semaphore(0); spb = new Semaphore(0); &#125; public void first(Runnable printFirst) throws InterruptedException &#123; // printFirst.run() outputs \"first\". Do not change or remove this line. printFirst.run(); // 只有等first线程释放Semaphore后使Semaphore值为1,另外一个线程才可以调用（acquire） spa.release(); &#125; public void second(Runnable printSecond) throws InterruptedException &#123; //只有spa为1才能执行acquire，如果为0就会产生阻塞 spa.acquire(); // printSecond.run() outputs \"second\". Do not change or remove this line. printSecond.run(); spb.release(); &#125; public void third(Runnable printThird) throws InterruptedException &#123; //只有spb为1才能通过，如果为0就会阻塞 spb.acquire(); // printThird.run() outputs \"third\". Do not change or remove this line. printThird.run(); &#125;&#125; 1115. 交替打印FooBar12345678910111213141516171819202122232425262728我们提供一个类：class FooBar &#123; public void foo() &#123; for (int i = 0; i &lt; n; i++) &#123; print(\"foo\"); &#125; &#125; public void bar() &#123; for (int i = 0; i &lt; n; i++) &#123; print(\"bar\"); &#125; &#125;&#125;两个不同的线程将会共用一个 FooBar 实例。其中一个线程将会调用 foo() 方法，另一个线程将会调用 bar() 方法。请设计修改程序，以确保 \"foobar\" 被输出 n 次。示例 1:输入: n = 1输出: \"foobar\"解释: 这里有两个线程被异步启动。其中一个调用 foo() 方法, 另一个调用 bar() 方法，\"foobar\" 将被输出一次。示例 2:输入: n = 2输出: \"foobarfoobar\"解释: \"foobar\" 将被输出两次。","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"http://raymond-zhao.top/categories/LeetCode/"}],"tags":[{"name":"LeetCode多线程","slug":"LeetCode多线程","permalink":"http://raymond-zhao.top/tags/LeetCode%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"AQS源码笔记","slug":"2020-07-10-Java-AQS","date":"2020-07-10T15:53:42.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/07/10/2020-07-10-Java-AQS/","link":"","permalink":"http://raymond-zhao.top/2020/07/10/2020-07-10-Java-AQS/","excerpt":"","text":"AQS源码笔记AQS全称是 AbstractQueuedSynchronizer，顾名思义，是一个用来构建锁和同步器的框架，它底层用了 CAS来保证操作的原子性，同时利用 FIFO队列实现线程间的锁竞争，将基础的同步相关抽象细节放在 AQS，这也是 ReentrantLock、CountDownLatch以及其他众多同步工具实现同步的底层实现机制。 AQS是抽象类，并不能直接实例化，当需要使用AQS的时候需要继承AQS抽象类并且重写指定的方法，这些重写方法包括线程获取资源和释放资源的方式(如ReentractLock通过分别重写线程获取和释放资源的方式实现了公平锁和非公平锁)，同时子类还需要负责共享变量state的维护，如当state = 0时表示该锁没有被占，大于 $0$ 时候代表该锁被一个或多个线程占领(重入锁)，而队列的维护(获取资源失败入队、线程唤醒、线程的状态等)不需要我们考虑，AQS已经帮我们实现好了。AQS的这种设计模式采用的正是模板方法模式。 继承了AQS的子类的主要任务包括： 通过CAS操作维护共享变量state 重写资源的获取方式tryAquire() 重写资源的释放方式tryRelease() AQS作为J.U.C的工具类，面向的是需要定制锁的创造者，也就是我们可以基于AQS去按需创造自己需要的锁。而如ReentrantLock这样的锁面向的则是锁的使用者。 内部类与成员变量内部类这是等待队列($Wait \\ Queue$)的结点类。这个等待队列是CLH锁队列的一个变种。CLH锁通常用来为自旋锁服务。Doug Lea老先生在源码里的注释： 12345* &lt;pre&gt;* +------+ prev +-----+ +-----+* head | | &lt;---- | | &lt;---- | | tail* +------+ +-----+ +-----+* &lt;/pre&gt; 如果要入队的话，从tail进入，出队的话，从head出，注意这入队出队的操作要保证原子性。可以看到，这个队列是由tail指向head，而不是head指向tail。 建议阅读 Doug Lea 老爷子在AQS中的大段注释。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677static final class Node &#123; /** 结点在共享模式下进行等待的标志 */ static final Node SHARED = new Node(); /** 结点在独占模式下进行等待的标志 */ static final Node EXCLUSIVE = null; /** 等待状态: 线程已取消 */ static final int CANCELLED = 1; /** 等待状态: 后续线程需要释放 */ static final int SIGNAL = -1; /** 等待状态: 线程正在条件队列 */ static final int CONDITION = -2; /** 等待状态: 指示下一个acquireShared应该无条件传播, 仅在共享模式下可用 */ static final int PROPAGATE = -3; /** 当前结点的状态, SIGNAL, CONDITION, CANCELLED, PROPAGATE 代表是条件队列结点 */ /** 为 0, 代表当前结点在 sync 队列中, 阻塞着等待排队获取锁. */ /** 通过 CAS 修改, 如果条件允许的话也可以通过 volatile写 进行修改 */ volatile int waitStatus; /** * 前驱结点, 入队时赋值, 出队时为null(for GC), * 在前驱结点取消后, 会进行短路操作, * 找到一个未取消的结点(这种情况一定存在, 因为head结点不会被cancelled) * 只有在成功 acquire 的时候一个结点才能成为头结点 * 一个 cancelled 的线程永远不会成功 acquire * 而且一个线程只能被自己取消, */ volatile Node prev; /** * 当前线程/结点释放时链接的后继结点 * 入队时赋值, 在绕过前面cancelled的结点时进行调整, 在出队时清空. */ volatile Node next; /** 这个结点锁包装的线程 构造时初始化, 使用后为 null */ volatile Thread thread; /**They are then transferred to the queue to * re-acquire. And because conditions can only be exclusive, * we save a field by using special value to indicate shared * mode. */ /** * 条件队列 condition 中的后继结点, 或者是特殊值 SHARED * 条件队列仅在独占模式下才可访问 */ Node nextWaiter; /** 判断是否共享模式 */ final boolean isShared() &#123; return nextWaiter == SHARED; &#125; /** 获取前驱结点 */ final Node predecessor() throws NullPointerException &#123; Node p = prev; if (p == null) throw new NullPointerException(); else return p; &#125; Node() &#123;&#125; // Used to establish initial head or SHARED marker Node(Thread thread, Node mode) &#123; // Used by addWaiter this.nextWaiter = mode; this.thread = thread; &#125; Node(Thread thread, int waitStatus) &#123; // Used by Condition this.waitStatus = waitStatus; this.thread = thread; &#125;&#125; 成员变量123456789101112131415161718192021/*** 等待队列的头结点. 懒初始化. 除了初始化操作外, 它仅可以通过 setHead() 方法操作* 如果头结点 head 存在的话, 那它的等待状态一定不是 CANCELLED* 可以认为是当前持有锁的结点*/private transient volatile Node head;/*** 等待队列的尾结点, 懒初始化. * 仅通过 enq() 添加新结点时操作.* 其余线程竞争锁失败后将会加入队尾, tail 始终指向队列最后一个结点. */private transient volatile Node tail;/** * 最简单也最重要的变量, 代表当前锁的状态* 0 : 未被使用* 大于 0 : 代表被线程持有. * getState(), setState() 进行操作*/private volatile int state; 小结从上面可以看到，AQS内部数据结构为： 双向链表：同步队列。队列中的每个结点对应一个Node，AQS通过控制链表结点的插入与删除达到同步与阻塞的目的。 单向链表：条件队列。 可以把同步队列和条件队列理解成储存等待状态的线程的队列，另外： 条件队列中的线程并不能直接去获取资源，而要先从条件队列转到同步队列中排队获取 同步队列的唤醒结果是线程去尝试获取锁 条件队列的唤醒结果是把线程从条件队列移到同步队列中 一个线程要么是在同步队列中，要么是在条件队列中，不可能同时存在这两个队列里面。 AQS提供共享模式与独占模式两种模式去获取资源。 当一个线程以共享模式或独占模式去获取资源的时候，如果获取失败则将该线程封装成Node结点(同时将该结点标识为共享模式或独占模式)加入到同步队列的尾部，AQS实时维护着这个同步队列，这个队列以FIFO(先进先出)来管理结点的排队，即资源的转移(获取再释放)的顺序是从头结点开始到尾结点。 独占获取独占模式即一个线程获取到资源后，其他线程不能再对资源进行任何操作，只能阻塞，在等待队列中等待被唤醒获得资源。 acquire()12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 在此过程中： 通过tryAcquire(arg)尝试获取锁，这个方法需要实现类去自定义获取锁的逻辑，获取成功则持有锁，获取失败则执行加入等待队列的逻辑。 获取锁失败后，执行addWaiter(Node.EXCLUSIVE)将 当前线程封装成一个Node结点，加入等待队列尾部。 执行acquireQueued(xx, xx)。该方法用来判断当前结点的前驱结点是否为头结点，尝试获取锁，如果获取成功，则当前结点会成为新的头结点。这也是获取锁的核心逻辑。 tryAcquire()1234567/*** @param arg 传递给和获取锁的方法的参数, 或者是保存在条件等待队列里的结点的值.* 需要继承了 AQS 的子类去实现具体的实现逻辑*/protected boolean tryAcquire(int arg) &#123; throw new UnsupportedOperationException();&#125; addWaiter()12345678910111213141516171819202122/*** 为当前按线程及其模式(共享或独占)创建入队结点* @param mode Node.EXCLUSIVE for exclusive, Node.SHARED for shared*/private Node addWaiter(Node mode) &#123; // 1. 创建基于当前线程的, EXCLUSIVE 类型的结点 Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; // 2. 判断队尾是否为空, 如果不为空则将结点加入队尾. if (pred != null) &#123; node.prev = pred; // 3. 采用CAS插入 // 即使并发情况, 也只有一条线程能操作成功, 其余的进行 enq 方法 if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node;&#125; 在此过程中： 创建基于当前线程的, EXCLUSIVE类型的结点。 CAS将结点插入队尾。 enq()12345678910111213141516171819202122/*** 入队操作, 向队尾插入结点. 如果有必要还要进行初始化* @param node 要插入的结点* @return 插入结点的前驱*/private Node enq(final Node node) &#123; for (;;) &#123; // 自选操作 Node t = tail; if (t == null) &#123; // Must initialize // CAS设置头, 初始化操作. if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; // CAS 插入队尾, 成功才返回; 也就是说失败就自旋直至修改成功为止. node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; 在此过程中： 自旋机制，这是AQS里很重要的一个机制。 如果队尾结点为空，则初始化队列，将头结点设置为空结点，头结点表示正持有锁的线程。 如果队尾tail不为空，则采取CAS操作，将结点插入到队尾，失败则自旋至成功为止。 对比addWaiter()与enq()两个方法可以发现，先在addWaiter()中使用一次CAS进行尝试插入，如果成功皆大欢喜，失败的话则到enq()中进行完整的入队操作，在这个过程中存在自旋。 完整的入队流程： 队列为空：先初始化，将头结点设为空结点，表示当前没有线程持有锁。 队列不为空：将线程结点插入队尾。 acquireQueued()如果前面的过程都执行成功，说明当前结点已经成功入队。 1234567891011121314151617181920212223242526272829/*** 对于已经入队的线程, 独占模式, 不可中断. */final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; // 不可中断 boolean interrupted = false; for (;;) &#123; // 自旋 final Node p = node.predecessor(); // 入队结点的前驱 if (p == head &amp;&amp; tryAcquire(arg)) &#123; // 如果入队结点的前驱石头结点并且尝试获取锁成功的话 // 把当前结点设为头结点, 代表它当前已经获取到锁了. setHead(node); p.next = null; // help GC failed = false; // 成功 return interrupted; &#125; // 获取锁失败, 进入挂起挂起, 可中断. if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) // 如果获取失败, 则取消获取锁. cancelAcquire(node); &#125;&#125; 在此过程中： 判断当前结点的前驱结点是否是头结点head，如果是，则尝试获取锁。 获取锁失败，挂起。 如果当前结点的前驱是头结点head，这个时候head可能已经释放了锁，所以需要tryAcquire(arg) shouldParkAfterFailedAcquire()12345678910111213141516171819202122232425/*** 检查并更新获获取锁失败的结点的状态* 如果线程应该阻塞, 则返回 true*/private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; if (ws == Node.SIGNAL) // 如果 pred 结点为 SIGNAL 状态, 返回true，说明当前结点需要挂起 return true; if (ws &gt; 0) &#123; // 前驱被取消, 结点状态为 CANCELLED, 跳过前驱并重试. do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; /** * waitStatus 必须为 0 或者 PROPAGATE * 这表示我们需要 SIGNAL, 但是暂时不要 park * 调用者需要进行充实来确保它在park前真的获取不到锁了. */ compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; 在此过程中： 判断pred的状态，如果为SIGNAL，则直接返回true进行阻塞。 删除队列中状态为CANCELLED的结点。 如果pred状态为 $0$ 或者 PROPAGATE，则将其设置为SIGNAL，再从acquireQueued方法自旋操作从新循环。 这里需要注意的时候，结点的初始值为 $0$，因此如果获取锁失败，会尝试将结点设置为 $SIGNAL$。 parkAndCheckInterrupt()1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; LockSupport是用来创建锁和其他同步类的基本线程阻塞原语。LockSupport提供 park() 和 unpark() 方法实现阻塞线程和解除线程阻塞。release释放锁方法逻辑会调用LockSupport.unpark() 方法来唤醒后继结点。 cancelAcquire()123456789101112131415161718192021222324252627282930313233343536373839404142/*** 取消正在尝试获取锁的操作*/private void cancelAcquire(Node node) &#123; // Ignore if node doesn't exist if (node == null) return; node.thread = null; // Skip cancelled predecessors Node pred = node.prev; while (pred.waitStatus &gt; 0) node.prev = pred = pred.prev; // predNext 是要取消拼接的结点. 如果不是的话, 下面的CAS将会失败. Node predNext = pred.next; // 可以利用非CAS操作. 在这个原子步骤之后, 其他的结点可以跳过CANCELLED的结点 // 自此, 当前结点就可以被其他线程干扰了. node.waitStatus = Node.CANCELLED; // If we are the tail, remove ourselves. if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; compareAndSetNext(pred, predNext, null); &#125; else &#123; // 如果后继结点需要被通知, 那就通知它, 否则利用传播的方式唤醒需要被唤醒的结点。 int ws; if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123; Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) compareAndSetNext(pred, predNext, next); &#125; else &#123; unparkSuccessor(node); &#125; node.next = node; // help GC &#125;&#125; 获取锁的逻辑 独占释放释放锁123456789101112/*** 如果 tryRelease(arg) 返回 true 的话, 可以被一个或多个未被阻塞的线程所实现.*/public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; // 尝试释放成功的话, 检查该结点是否有后继结点, 有的话则去唤醒 Node h = head; // head: 当前持有锁的结点 if (h != null &amp;&amp; h.waitStatus != 0) // 头结点状态不为 0, 代表有后继结点. unparkSuccessor(h); // return true; &#125; return false;&#125; 在此过程中： tryRelease尝试释放锁，这个方法需要继承了AQS的子类去实现自己的逻辑。 释放成功则执行唤醒后继结点的逻辑。 unparkSuccessor()1234567891011121314151617181920212223242526/*** 如果后继结点存在的话则进行唤醒*/private void unparkSuccessor(Node node) &#123; // 如果ws &lt; 0 尝试去处理等待通知的结点. int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ // 从后向前寻找且 non-cancelled 的后继结点 Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) // 如果结点状态为 non-cancelled, 直接将后继结点唤醒. LockSupport.unpark(s.thread);&#125; 共享获取共享模式下，线程无论是获取资源还是释放资源，都可能会唤醒后继结点。 共享模式资源的获取和独占模式资源的获取流程差不多，就是在获取资源成功后，会唤醒为共享模式的后继结点，然后被唤醒的后继结点也去获取资源。 acquireShared()123456public final void acquireShared(int arg) &#123; // 尝试获取共享锁, 小于 0 表示获取失败 if (tryAcquireShared(arg) &lt; 0) // 执行获取锁失败的逻辑 doAcquireShared(arg);&#125; tryAcquireShared()仍然，获取锁的具体逻辑是由继承了AQS的子类去实现的。 123protected int tryAcquireShared(int arg) &#123; throw new UnsupportedOperationException();&#125; doAcquireShared()12345678910111213141516171819202122232425262728293031323334353637383940/*** 共享模式, 不可中断模式下的获取.*/private void doAcquireShared(int arg) &#123; // 添加共享锁类型结点到队列中 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (;;) // 获取线程的前驱 final Node p = node.predecessor(); if (p == head) &#123; // 如果前驱节点为头结点，则该线程尝试获取资源。 int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; // 获取成功 对后继 SHARED 节点持续唤醒 setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); // Thread.currentThread().interrupt(); failed = false; return; &#125; &#125; // 和独占模式一样 // 调用 shouldParkAfterFailedAcquire, 将该节点的前驱节点 // 的状态设置为 SIGNAL，告诉前驱节点我要去“睡觉”了，当资源排 // 到你的时候，你就通知我一下让我醒来，即节点做进入等待状态的准备。 // 当节点做好了进入等待状态的准备，则调用 parkAndCheckInterrupt // 函数，让该节点进入到等待状态。 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; setHeadAndPropagate()1234567891011121314151617181920212223242526272829303132333435/** * Sets head of queue, and checks if successor may be waiting * in shared mode, if so propagating if either propagate &gt; 0 or * PROPAGATE status was set. * * @param node the node * @param propagate the return value from a tryAcquireShared */private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // Record old head for check below setHead(node); /* * Try to signal next queued node if: * Propagation was indicated by caller, * or was recorded (as h.waitStatus either before * or after setHead) by a previous operation * (note: this uses sign-check of waitStatus because * PROPAGATE status may transition to SIGNAL.) * and * The next node is waiting in shared mode, * or we don't know, because it appears null * * The conservatism in both of these checks may cause * unnecessary wake-ups, but only when there are multiple * racing acquires/releases, so most need signals now or soon * anyway. */ if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; // 如果节点为共享节点，则调用doReleaseShared函数唤醒后继节点。 if (s == null || s.isShared()) doReleaseShared(); &#125;&#125; 共享释放共享模式下资源释放流程和独占模式下资源释放的流程差不多，就是在释放后唤醒后继为共享模式的节点，且唤醒的动作是传播下去的，直到后继节点出现不是共享模式的，这个唤醒的过程和共享模式的获取资源的唤醒过程一样。 releaseShared()1234567public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125; tryReleaseShared()123456/*** 仍然, 被子类实现.*/protected boolean tryReleaseShared(int arg) &#123; throw new UnsupportedOperationException();&#125; doReleaseShared()1234567891011121314151617181920212223242526272829303132333435/*** Release action for shared mode -- signals successor and ensures* propagation. (Note: For exclusive mode, release just amounts* to calling unparkSuccessor of head if it needs signal.)*/private void doReleaseShared() &#123; /* * Ensure that a release propagates, even if there are other * in-progress acquires/releases. This proceeds in the usual * way of trying to unparkSuccessor of head if it needs * signal. But if it does not, status is set to PROPAGATE to * ensure that upon release, propagation continues. * Additionally, we must loop in case a new node is added * while we are doing this. Also, unlike other uses of * unparkSuccessor, we need to know if CAS to reset status * fails, if so rechecking. */ for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; // 如果节点标识后继节点需要唤醒，则调用 unparkSuccessor 方法进行唤醒。 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125;&#125; 共享模式下的逻辑 条件队列条件队列又称等待队列，条件队列的实现是通过ConditionObject的内部类完成的，一开始介绍了同步队列条件队列的区别。 可以把同步队和条件队列理解成储存等待状态的线程的队列，条件队列中的线程并不能直接去获取资源，而要先从条件队列转到同步队列中排队获取，一个线程要么是在同步队列中，要么是在条件队列中，不可能同时存在这两个队列里面。 12345678910111213141516171819202122232425262728293031/* * 使当前线程进入等待状态，直到以下4种情况任意一个发生： * 1.另一个线程调用该对象的signal()，当前线程恰好是被选中的唤醒线程 * 2.另一个线程调用该对象的signalAll() * 3.另一个线程interrupt当前线程（此时会抛出InterruptedException） * 4.虚假唤醒（源自操作系统，发生概率低） * ConditionObject要求调用时该线程已经拿到了其外部AQS类的排它锁（acquire成功） */void await() throws InterruptedException;/* * 与await()相同，但是不会被interrupt唤醒 */void awaitUninterruptibly();/* * 与await()相同，增加了超时时间，超过超时时间也会停止等待 * 三个方法功能相似，其返回值代表剩余的超时时间，或是否超时 */long awaitNanos(long nanosTimeout) throws InterruptedException;boolean await(long time, TimeUnit unit) throws InterruptedException;boolean awaitUntil(Date deadline) throws InterruptedException;/* * 唤醒一个正在等待该条件变量对象的线程 * ConditionObject会选择等待时间最长的线程来唤醒 * ConditionObject要求调用时该线程已经拿到了其外部AQS类的排它锁（acquire成功） */void signal();/* * 唤醒所有正在等待该条件变量对象的线程 * ConditionObject要求调用时该线程已经拿到了其外部AQS类的排它锁（acquire成功） */void signalAll(); 可以看到，其作用与Object原生的wait()/notify()/notifyAll()很相似，但是增加了更多的功能。下面以awaitUninterruptibly()、signal()为例，阐述一下其内部实现。 同步队列和条件队列的关系 线程执行condition.await()方法，将节点从同步队列转移到条件队列中。 线程执行condition.signal()方法，将节点从条件队列中转移到同步队列。 参考 一文带你快速掌握AQS Java并发值AQS源码分析 《Java并发编程的艺术》","categories":[{"name":"多线程与并发","slug":"多线程与并发","permalink":"http://raymond-zhao.top/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"AQS","slug":"AQS","permalink":"http://raymond-zhao.top/tags/AQS/"}]},{"title":"ConcurrentHashMap","slug":"2020-07-09-JUC-ConcurrentHashMap","date":"2020-07-09T04:50:26.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/07/09/2020-07-09-JUC-ConcurrentHashMap/","link":"","permalink":"http://raymond-zhao.top/2020/07/09/2020-07-09-JUC-ConcurrentHashMap/","excerpt":"","text":"ConcurrentHashMapjava.util.HashMap是线程不安全的集合类，如果想要使用线程安全的Map，可以选择使用java.util.Hashtable(不建议)，或者java.util.Collections.synchronizedMap，或者java.util.concurrent.ConcurrentHashMap。本篇文章就是ConcurrentHashMap(JDK1.8)的源码笔记。 继承/实现关系图12public class ConcurrentHashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements ConcurrentMap&lt;K,V&gt;, Serializable &#123; ... &#125; 关键常量与成员变量常量1234567891011private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;private static final int DEFAULT_CAPACITY = 16;private static final float LOAD_FACTOR = 0.75f;static final int TREEIFY_THRESHOLD = 8;static final int UNTREEIFY_THRESHOLD = 6;static final int MIN_TREEIFY_CAPACITY = 64;static final int MOVED = -1; // hash for forwarding nodesstatic final int TREEBIN = -2; // hash for roots of treesstatic final int RESERVED = -3; // hash for transient reservationsstatic final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash 成员变量12345678910111213141516/** bucket桶数组, 在第一次插入时才懒加载 */transient volatile Node&lt;K,V&gt;[] table;/** 在 resize 时使用的桶数组, 只有在 resize 的时候非空 */private transient volatile Node&lt;K,V&gt;[] nextTable;/** * table 初始化与 resize 的标志* 为负数时, table 正在被初始化或者 resize* -1 代表正在初始化* -x: 有 x - 1 个线程正在进行 resize, 即-(1 + 正在进行 resize 的线程数量)* 0: table还未被初始化. * 正数: 下一次进行扩容的容量大小*/private transient volatile int sizeCtl;/** Unsafe 类 用于实现CAS操作 */private static final sun.misc.Unsafe U; 在CHM的实现中可以看到大量的U.compareAndSwapXxxx的方法去操作CHM的一些属性，也就是大量地使用CAS，CAS是乐观锁的一种实现方式，每次操作前都假设不会产生冲突，等到发生冲突的时候就会去自旋重试。 CAS的思想不再多说，问题在于会产生ABA问题，可通过version number机制加以解决。 内部类 Node类，普通的结点。 1234567891011121314151617181920212223242526272829303132333435static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return val; &#125; public final String toString()&#123; return key + \"=\" + val; &#125; public final V setValue(V value) &#123; throw new UnsupportedOperationException(); &#125; public final int hashCode() &#123; return key.hashCode() ^ val.hashCode(); &#125; public final boolean equals(Object o) &#123; Object k, v, u; Map.Entry&lt;?,?&gt; e; return ((o instanceof Map.Entry) &amp;&amp; (k = (e = (Map.Entry&lt;?,?&gt;)o).getKey()) != null &amp;&amp; (v = e.getValue()) != null &amp;&amp; (k == key || k.equals(key)) &amp;&amp; (v == (u = val) || v.equals(u))); &#125; /** * Virtualized support for map.get(); overridden in subclasses. */ Node&lt;K,V&gt; find(int h, Object k) &#123; ... &#125;&#125; TreeNode类，继承于Node类，封装为红黑树的结点，但是没有具体操作，同时被TreeBin再一次封装 1234567891011121314151617181920212223242526/*** Nodes for use in TreeBins*/static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next, TreeNode&lt;K,V&gt; parent) &#123; super(hash, key, val, next); this.parent = parent; &#125; Node&lt;K,V&gt; find(int h, Object k) &#123; return findTreeNode(h, k, null); &#125; /** * Returns the TreeNode (or null if not found) for the given key * starting at given root. */ final TreeNode&lt;K,V&gt; findTreeNode(int h, Object k, Class&lt;?&gt; kc) &#123; ... &#125;&#125; TreeBin，不处理用户的key-value信息，而是包装了TreeNode结点，封装了大量的红黑树操作。在实际的ConcurrentHashMap数组中，存放的是TreeBin对象，而不是TreeNode。 123456789101112131415161718192021222324252627282930313233343536373839404142434445static final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; root; volatile TreeNode&lt;K,V&gt; first; volatile Thread waiter; volatile int lockState; // values for lockState static final int WRITER = 1; // set while holding write lock static final int WAITER = 2; // set when waiting for write lock static final int READER = 4; // increment value for setting read lock static int tieBreakOrder(Object a, Object b) &#123; ... &#125; TreeBin(TreeNode&lt;K,V&gt; b) &#123; ... &#125; private final void lockRoot() &#123; ... &#125; private final void unlockRoot() &#123; lockState = 0; &#125; private final void contendedLock() &#123; ... &#125; final Node&lt;K,V&gt; find(int h, Object k) &#123; ... &#125; final TreeNode&lt;K,V&gt; putTreeVal(int h, K k, V v) &#123; ... &#125; final boolean removeTreeNode(TreeNode&lt;K,V&gt; p) &#123; ... &#125; /** 红黑树操作方法 */ /** ---------------------- */ /** 左旋操作 */ static &lt;K,V&gt; TreeNode&lt;K,V&gt; rotateLeft(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; p) &#123; ... &#125; /** 右旋操作 */ static &lt;K,V&gt; TreeNode&lt;K,V&gt; rotateRight(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; p) &#123; ... &#125; /** 插入后进行树的调整 */ static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceInsertion(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; x) &#123; ... &#125; /** 删除后进行树的调整 */ static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceDeletion(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; x) &#123; ... &#125; /** Unsafe 类 */ private static final sun.misc.Unsafe U; private static final long LOCKSTATE; // 锁状态 static &#123; try &#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = TreeBin.class; LOCKSTATE = U.objectFieldOffset (k.getDeclaredField(\"lockState\")); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125;&#125; ForwardingNode，特殊结点，在扩容时才会出现的特殊结点，key,value,hash全部为null，由nextTable指向新的table数组。 12345678910/** A node inserted at head of bins during transfer operations. */static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125; Node&lt;K,V&gt; find(int h, Object k) &#123; ... &#125;&#125; CAS操作123456789101112131415/** 返回 tab 中索引为 i 处的元素 */static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125;/** 利用 CAS 设置 tab 中索引为 i 处的元素 */static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125;/** 设置 tab 中索引为 i 处的元素为 v */static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; 关键方法构造方法 CHM为我们提供了 $5$ 种构造方法，从中可以看出比HashMap多了一个加入了concurrencyLevel的构造方法，这个代表预先估计的并发量。不同的构造函数计算sizeCtl的方法不一样。 123456789101112131415161718192021222324252627282930313233343536373839/** 返回一个默认容量为 16 的CHM */public ConcurrentHashMap() &#123;&#125;/** 手动设置初始最大容量* 小于 0 则抛出异常* 大于MAXIMUM_CAPACITY/2，设置为MAXIMUM_CAPACITY* 否则利用 tableSizeFor()方法将 initialCapacity * 1.5 + 1 的值转化为 2 的幂的大小* 设置sizeCtl = cap, 即下一次扩容时的容量.*/public ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); this.sizeCtl = cap;&#125;public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.sizeCtl = DEFAULT_CAPACITY; putAll(m);&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor) &#123; this(initialCapacity, loadFactor, 1); // 调用下面的全参构造 1 代表1个线程&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if (!(loadFactor &gt; 0.0f) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); // 初始容量 &lt; 并发线程量 if (initialCapacity &lt; concurrencyLevel) // Use at least as many bins initialCapacity = concurrencyLevel; // as estimated threads long size = (long)(1.0 + (long)initialCapacity / loadFactor); int cap = (size &gt;= (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); this.sizeCtl = cap;&#125; 初始化方法 initTable() 首先检查tab是否为空，如果为空的话则尝试去初始化。 检查sizeCtl是否小于 0，因为sizeCtl 的默认值为0，所以最先进入方法的一个或多个线程会尝试使用CAS去初始化数组，设置成功的话sizeCtl=-1，表示正在初始化，后续的线程将会进入yield状态。 在将sizeCtl设置为 -1 后，会再次检查 tab 是否已被初始化，这是双端检锁的思想，避免tab被重复初始化。 此时将会去设置指定容量的 Node 数组，默认为 16，并将成员变量table指向新创建的 Node 数组。 接着降回去计算阈值，这里使用sc=n - (n &gt;&gt;&gt; 2);，可以保证新的阈值=新容量*0.75。 最后再更新sizeCtl。 123456789101112131415161718192021222324252627282930313233343536373839/*** Initializes table, using the size recorded in sizeCtl.*/private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; // 当 tab == null 或者长度为 0 时, 则一直循环试图初始化 table while ((tab = table) == null || tab.length == 0) &#123; // 1. sizeCtl &lt; 0, 说明有其他线程正在进行初始化或者扩容, 则挂起当前线程 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin // 2. sizeCtl &gt;= 0, 调用 CAS 试图将 sizeCtl 设置为 -1, 表示正在初始化. // 此时如果别的线程也进入了 initTable() 方法, 将会执行上面的 Thread.yield(); else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; // 3. 如果设置 sizeCtl = -1 成功的话 try &#123; // 4. 在此确认 table 是否被初始化 双端检锁的思想 if ((tab = table) == null || tab.length == 0) &#123; // 5. 如果 sc &gt; 0 则 n = sc, 否则 n = 16 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(\"unchecked\") // 6. 创建指定容量大小的 Node 数组 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; // 7. 将 table 指向新创建的 Node 数组 table = tab = nt; // 8. 计算阈值, sc = 0.75n 当 CHM 储存的键值对数量大于这个值时将会扩容 // 8.1 这里的 0.75 等于默认负载因子, // HashMap、Hashtable如果使用传入了负载因子的构造函数的话 // 那么每次扩容, 新阈值=新容量*负载因子 // 8.2 CHM不管使用哪种构造函数初始化, 新阈值=新容量*0.75 sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; Thread.yield()方法，使当前线程由执行状态变为就绪状态，让出CPU时间，在下一个线程执行时候，此线程可能被执行，也可能没被执行。 put(K key, V val) 检查k-v是否为null，是的话则抛出NPE。 根据 key 计算 hash 值。 检查 table 是否已被初始化，如果没有的话将会尝试去初始化，此时允许多个线程去尝试初始化，但是initTable()中的CAS操作保证了只有一个线程能够初始化成功。 利用 (n-1) &amp; hash 计算索引值，然后检查 table 中这个位置的首结点是否为空，为空的话则使用CAS尝试去设置值，设置成功则跳出。 如果首结点不为空，说明发生了碰撞，这个时候将会使用synchronized去加锁头结点，但是不影响table中的其他 Node 的工作。接下来将会去判断加锁的头结点是属于链表结点还是红黑树节点。 如果是链表结点的话， 比较 key 和 hash 是否均相等，如果相等则进行覆盖，如果找到链表尾部 key 都不相等则在尾部进行插入(尾插法)。然后还要检查链表结点数量是否达到了树化阈值，是的话则进行树化。 如果是红黑树节点的话，则调用红黑树的操作进行节点插入，并进行相应地调整。 如果此时进来的其他线程检查到正在扩容操作的话，将会去协助扩容，减少扩容时间。 最后调用addCount(1L, binCount) 方法区近似地估计一下table中的总的元素数量。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495/*** Maps the specified key to the specified value in this table.* Neither the key nor the value can be null.* 也就是说 key-value 均不能为 null*/public V put(K key, V value) &#123; return putVal(key, value, false);&#125;/** Implementation for put and putIfAbsent */final V putVal(K key, V value, boolean onlyIfAbsent) &#123; // 1. 检查 key-value 是否为空 if (key == null || value == null) throw new NullPointerException(); // 2. 计算 hash 索引, return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS(0x7fffffff); // 加 &amp; HASH_BITS(0x7fffffff)是为了保障结果为正数 int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; // 3. 检查 table 是否为空, 这里没加 synchronized // 也就是允许多个线程去尝试初始化 // 但是 initTable() 方法里使用CAS保证了只有一个线程执行初始化 if (tab == null || (n = tab.length) == 0) tab = initTable(); // 4. (n - 1) &amp; hash 这个操作其实就是取模, 跟HashMap一样 // 检查 tab 中索引为 i 处的位置是否为空 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; // 5. 如果为空的话则使用 CAS 的方式尝试去设置值 设置成功则跳出 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; // 6. 如果正在扩容 else if ((fh = f.hash) == MOVED) // 7. tab 等于其他线程帮助扩容后的结果 下面有代码 tab = helpTransfer(tab, f); else &#123; // 8. 出现了碰撞 V oldVal = null; synchronized (f) &#123; // 加锁当前下标的链表, 其他bucket并未加锁, 提升了效率 if (tabAt(tab, i) == f) &#123; // 9. hashcode值此时相等了 // 10. 此时为链表, 在链表中寻找插入点 if (fh &gt;= 0) &#123; // 如果 hashcode &gt;= 0 binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; // 遍历链表 K ek; // 10. 比较待插入点的 key 是不是相等 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; // 11. 如果相等则覆盖 oldVal = e.val; // 12. 如果不是只在空的时候才插入, 并跳出 if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; // 13. 如果找到链表尾部都没找到相同key的, 则插入到尾部. pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; // 14. 如果 f 是红黑树结点, 则插入树结点. else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; // 15. binCount在遍历链表的时候自增, 如果大于阈值则树化 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // 16. 对于table的size的近似估计 // 在多线程环境下没有办法去估计究竟有多少线程做了多少操作 // 产生了什么样的结果. addCount(1L, binCount); return null;&#125; helpTransfer()transfer()方法为CHM扩容的核心方法。在此过程中，CHM支持多线程扩容。扩容主要分为两个步骤： 构建nextTable桶数组，大小为之前的两倍，这个操作在单线程下完成。 将oldTable里面的内容复制到nextTable，这个操作允许多线程操作。可以减少扩容时间。 基本思路与HashMap差不多，主要区别在于CHM多线程加锁synchronized (f)。 1234567891011121314151617181920212223/** 如果 resize 正在进行的话则其他线程帮助一起 resize */final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; // 1. 声明一个新数组, 以及sc, 也就是 sizeCtl Node&lt;K,V&gt;[] nextTab; int sc; // 2. 如果tab不空, 并且是扩容结点 ForwardingNode(上面有这个结点的介绍) if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; int rs = resizeStamp(tab.length); while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; // 3. sc &lt; 0, 说明可能有多个线程在扩容 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; // 4. 调用下面的 transfer 方法 transfer(tab, nextTab); break; &#125; &#125; return nextTab; &#125; return table;&#125; transfer()在此过程中用到的常量或方法 123static final int NCPU = Runtime.getRuntime().availableProcessors();private static final int MIN_TRANSFER_STRIDE = 16;private transient volatile Node&lt;K,V&gt;[] nextTable; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148/*** Moves and/or copies the nodes in each bin to new table. See* above for explanation.* 把旧的桶中的数组中的结点移动或者复制到新的 table 中*/private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; // 1. 根据CPU数量来为每个线程分配\"力所能及\"的bucket(bin)数量, 避免因扩容线程过多反而影响性能 // 主要是计算 stride, 如果CPU核大于1,则stride=n无符号右移3位除以CPU核 // 如果只有1核的话, 那 stride=n, 那么接下来就辛苦你了, 仅有的CPU, 全部都要交给你了. // 如果stride最后小于16的话, 则强制给每个线程分配16个bin(桶) if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range if (nextTab == null) &#123; // initiating // 2. 如果辅助数组为空的话则进行初始化, 且初始化为原数组的n&lt;&lt;1倍,也就是2倍 try &#123; @SuppressWarnings(\"unchecked\") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME // 捕获 OOM sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; // 前进, 继续的意思. boolean finishing = false; // to ensure sweep before committing nextTab for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; // 迁移过程 // 逻辑与HashMap类似, 拿旧数组的容量当做mask(掩码),然后与当前结点的hash值做 &amp; 运算 // 可以得出新结点的新增有效位 // 是 0 的话索引没变，是 1 的话索引变成索引+旧数组容量 // 可以参考HashMap那篇文章分析. synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) &#123; // 如果 forwardingNode hash&gt;=0, 为链表 int runBit = fh &amp; n; // 计算新增的有效位 Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln); // 为 0, 索引不变 setTabAt(nextTab, i + n, hn); // 为 1, i + n setTabAt(tab, i, fwd); advance = true; &#125; else if (f instanceof TreeBin) &#123; // 如果是红黑树结点的话, 进行红黑树相关操作 TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; // 强转 forwardingNode TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; addCount()addCount()的作用是估计table.size，也就是table中存储的键值对数量，在使用put()/remove()方法并且成功执行之后都会调用addCount()。需要注意的是，这个值也只是估计值，因为多线程并发的情况下，没有办法精确统计有多少线程做了多少操作，作用的效果如何。 方法中用到的常量或方法 123456private transient volatile CounterCell[] counterCells; // 计数单元格,当非空时大小为2的幂private static final long BASECOUNT; // Unsafeprivate static final long CELLVALUE; // Unsafe// 主要在没有竞争时使用, 也就是单线程情况下统计的table中的结点// 但是也作为table初始化时的回调, 通过 CAS 更新private transient volatile long baseCount; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778@sun.misc.Contended static final class CounterCell &#123; volatile long value; CounterCell(long x) &#123; value = x; &#125;&#125;/*** Adds to count, and if table is too small and not already* resizing, initiates transfer. If already resizing, helps* perform transfer if work is available. Rechecks occupancy* after a transfer to see if another resize is already needed* because resizings are lagging additions.* 添加count, 如果table太小或者没有正在扩容,则初始化迁移操作.* 如果已经正在扩容, 尽可能去帮助迁移操作.* 在迁移之后再次检查坑位, 看看是否又需要resize, 因为正在扩容这个动作存在滞后(也就是不实时)** @param x the count to add 要添加的统计数量* @param check if &lt;0, don't check resize, if &lt;= 1 only check if uncontended*/private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; // 尝试修改baseCount, 以求达到计数的目的, 如果修改失败, 说明处于竞争情况, 即多线程操作 boolean uncontended = true; // 未处于竞争状态 /** * 如果计算单元格为空, 或者随机取一个数组为空, 或者CAS修改失败 * 表示处于并发竞争中, 则执行fullAddCount(x, uncontended)以死循环插入, 同时返回. * 否则代表这个slot处的变量修改成功, 继续执行. * * 每个线程通过 ThreadLocalRandom.getProbe() &amp; m 寻址找到属于它的CounterCell并计数 * ThreadLocalRandom是线程私有的伪随机数生成器, 每个probe都是不同的. * CounterCell的大小是2的幂, 初始容量为2, 每次扩容为之前的2倍. * 出于性能考虑, 最大容量为机器的CPU数量, CounterCell碰撞冲突是很严重的. */ if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended); // 死循环插入 return; &#125; if (check &lt;= 1) // check if uncontended return; s = sumCount(); &#125; // if &lt; 0, don't check resize, if &lt;= 1 only check if uncontended if (check &gt;= 0) &#123; // 检查是否需要扩容 Node&lt;K,V&gt;[] tab, nt; int n, sc; while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); if (sc &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125;&#125;final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum;&#125; get(Object key) 根据 key 计算 哈希值 检查table是否为空，非空的话检查table中根据key计算出的索引处的头结点也不为空 如果这两个检查中有一个为空，则返回null。 否则的话，检查定位到的头结点的 hash 与 key 是否均相等，相等的话则返回结果。 否则的话，判断是红黑树节点还是链表结点。如果是红黑树节点的话则到红黑树中查找，如果是链表结点的话则遍历链表查找，并返回相应的结果。 如果最后没有查到，则返回 null。 12345678910111213141516171819202122232425public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; // 1. 计算哈希值 int h = spread(key.hashCode()); // 2. 如果table不空 并且 table 长度大于 0 且计算出的下标上的 bucket 不为空 // 说明这个 bucket 存在, 进入到 bucket 中查找 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; if ((eh = e.hash) == h) &#123; // 3. 如果 hashcode 相等 并且 key 相等表示查找到, 返回 val if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; // 3. 哈希地址小于 0, 代表是红黑树的 bucket, 在红黑树中查找 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; // 4. 如果bucket头节点的哈希地址不小于0, 则代表bucket为链表, 遍历链表, 在链表中查找. while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; remove()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** 从map中移除key, 如果key不存在的话什么也不做. */public V remove(Object key) &#123; return replaceNode(key, null, null);&#125;final V replaceNode(Object key, V value, Object cv) &#123; // 1. 计算 hashcode int hash = spread(key.hashCode()); for (Node&lt;K,V&gt;[] tab = table;;) &#123; // 遍历 table Node&lt;K,V&gt; f; int n, i, fh; // 2. 如果tab为空或者key所在的bucket为空, 则什么也不做, 跳出返回. if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) &amp; hash)) == null) break; else if ((fh = f.hash) == MOVED) // 3. 如果正在扩容, 则帮助扩容 tab = helpTransfer(tab, f); else &#123; V oldVal = null; boolean validated = false; synchronized (f) &#123; // 4. 将key所在的bucket加锁, 其他bucket仍未加锁 if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; // 5. 哈希地址大于0, 为链表 validated = true; for (Node&lt;K,V&gt; e = f, pred = null;;) &#123; // 6. 遍历链表 K ek; // 7. 检查key是否相等, 如果相等则进行移除. if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; V ev = e.val; if (cv == null || cv == ev || (ev != null &amp;&amp; cv.equals(ev))) &#123; oldVal = ev; if (value != null) e.val = value; else if (pred != null) pred.next = e.next; else setTabAt(tab, i, e.next); &#125; break; &#125; pred = e; if ((e = e.next) == null) break; &#125; &#125; // 8. 如果是树结点则在红黑树中寻找并删除 else if (f instanceof TreeBin) &#123; validated = true; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; r, p; if ((r = t.root) != null &amp;&amp; (p = r.findTreeNode(hash, key, null)) != null) &#123; V pv = p.val; if (cv == null || cv == pv || (pv != null &amp;&amp; cv.equals(pv))) &#123; oldVal = pv; if (value != null) p.val = value; else if (t.removeTreeNode(p)) setTabAt(tab, i, untreeify(t.first)); &#125; &#125; &#125; &#125; &#125; if (validated) &#123; if (oldVal != null) &#123; if (value == null) addCount(-1L, -1); return oldVal; &#125; break; &#125; &#125; &#125; return null;&#125; 比较 比较方面 HashMap Hashtable ConcurrentHashMap 是否线程安全 否 是 是 线程安全采用的方式 无 采用synchronized类锁，效率低 CAS + synchronized，锁住的只有当前操作的bucket，不影响其他线程对其他bucket的操作，效率高 数据结构 数组+链表+红黑树(链表长度超过8则转红黑树) 数组+链表 数组+链表+红黑树(链表长度超过8则转红黑树) 是否允许null键值 是 否 否 哈希地址算法 (h=key.hashCode())^(h&gt;&gt;&gt;16 key.hashCode() (h=key.hashCode())^(h&gt;&gt;&gt;16)&amp;0x7fffffff 定位算法 (n-1)&amp;hash (hash&amp;0x7fffffff)%n (n-1)&amp;hash 扩容算法 当键值对数量大于阈值，则容量扩容到原来的2倍 当键值对数量大于等于阈值，则容量扩容到原来的2倍+1 当键值对数量大于等于sizeCtl，单线程创建新哈希表，多线程复制bucket到新哈希表，容量扩容到原来的2倍 链表插入 将新节点插入到链表尾部 将新节点插入到链表头部 将新节点插入到链表尾部 继承的类 继承abstractMap抽象类 继承Dictionary抽象类 继承abstractMap抽象类 实现的接口 实现Map接口 实现Map接口 实现ConcurrentMap接口 默认容量大小 16 11 16 默认负载因子 0.75 0.75 0.75 统计size方式 直接返回成员变量size 直接返回成员变量count 遍历CounterCell数组的值进行累加，最后加上baseCount的值即为size","categories":[{"name":"多线程与并发","slug":"多线程与并发","permalink":"http://raymond-zhao.top/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"ConcurrentHashMap","slug":"ConcurrentHashMap","permalink":"http://raymond-zhao.top/tags/ConcurrentHashMap/"}]},{"title":"HashMap","slug":"2020-07-08-Java-HashMap","date":"2020-07-08T11:36:45.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/07/08/2020-07-08-Java-HashMap/","link":"","permalink":"http://raymond-zhao.top/2020/07/08/2020-07-08-Java-HashMap/","excerpt":"","text":"HashMapMap简介Map中可以说是Java面试中最热门的考点了，提起Map能想到什么呢？HashMap,LinkedHashMap,ConcurrentHashMap,ConcurrentSkipListMap,ConcurrentNavigableMap,ThreadLocalMap，这是JDK为我们提供的。Spring中还有getBean(&quot;xxx&quot;)…现在先专注于HashMap吧，以后陆续把这些欠下的知识给补上。 HashMap基础构造函数(4种)12345678910111213141516171819202122232425262728public HashMap() &#123; // 默认容量 16 this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125;public HashMap(int initialCapacity) &#123; // 自定义最大容量 this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;/** 自定义最大容量与负载因子 **/public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125;public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);&#125; 存储无序12345678910public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put(\"Banana\", \"香蕉\"); map.put(\"Orange\", \"橘子\"); map.put(\"Apple\", \"苹果\"); map.entrySet().forEach(System.out::println);&#125;// Apple=苹果// Orange=橘子// Banana=香蕉 从输出结果可以看出，HashMap存储与打印的顺序不是一致的，也就是HashMap存储数据不是按序存储的，如果需要按序存储可以选择LinkedHashMap,TreeMap，其中LinkedHashMap是用双向链表来保持有序的，而TreeMap则是用红黑树保持有序的。 重要变量123456789101112131415161718192021222324252627/** 默认初始容量 = 16, 比如为 2 的幂 **/static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;/** 最大容量 如果没有被构造方法指定的话将会使用这个数值, 必须是 2 的幂并且 &lt;= 1 &lt;&lt; 30 **/static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** 默认负载因子, 也就是HashMap中存储的实际Node数量超过总容量的这个百分比之后将会扩容* 可以等于 1, 也可以大于 1, 只是冲突太多, 降低效率。*/static final float DEFAULT_LOAD_FACTOR = 0.75f;/** 将链表转化为红黑树的阈值. 下面的 bin 指的是 bucket, 如下图中的2开始的一串, 而不是从 0-7 的bucket数组 **//** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon shrinkage. */static final int TREEIFY_THRESHOLD = 8;/** 红黑树转化为链表的阈值 **/static final int UNTREEIFY_THRESHOLD = 6;/** 树化时的最小容量 **//*** The smallest table capacity for which bins may be treeified.* (Otherwise the table is resized if too many nodes in a bin.)* Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts* between resizing and treeification thresholds.*/static final int MIN_TREEIFY_CAPACITY = 64; 关于为什么要把树化阈值设置为 $8$. 可以参考 阿里面试题：为什么Map桶中个数超过8才转为红黑树，但是个人更推荐看HsahMap源码中的最前面的注释 Implementation notes.，刚才读了一下，感觉很有必要专门写几篇读前言注释的文章了，先大致记载一下刚才获取到的对我而言的新知识。 TreeNode所占空间近乎Node两倍，在此就考虑到了空间与时间。 转化成红黑树时，默认按照key的hashCode()结果进行排序，也可以自定义比较器。 在random hashCodes情况下，bucket中的nodes服从 泊松分布，平均情况下为 $$p=\\frac{0.5^k}{k!}e^{-0.5}, k=0,1,2,\\cdots,n$$ $k$ 的意思是在忽略方差的情况下，链表的长度( $Node$ 的个数)，p代表链表长度为 $k$ 时的概率。下面为链表长度出现 $k$ 从 $1$ 到 $8$ 时的概率 12345678910* 0: 0.60653066* 1: 0.30326533* 2: 0.07581633* 3: 0.01263606* 4: 0.00157952* 5: 0.00015795* 6: 0.00001316* 7: 0.00000094* 8: 0.00000006* more: less than 1 in ten million 也就是说，在同一个bucket发生 $8$ 次以上碰撞的概率已经小于千万分之一，所以不应该轻易认为链表转化为红黑树是很常见的事情。在数据量达到千万或者亿级以上的时候(还记得哈希表的MAXIMUM_CAPACITY = 1 &lt;&lt; 30吗？)可能才会偶尔出现，而这个时候哈希表的效率就显得尤为重要。 网上有很多博客都在强调 $O(n)$ 与 $O(\\lg n)$ 的区别，当结点数量为 $8$ 时， 平均情况下：链表查找长度为 $n/2=4$ ，而红黑树的查找长度为 $\\log_2 n = 3$，差别好像并不是很大。 最坏情况下：链表查找长度为 $8$，红黑树为 $3$， 好像已经隐隐接近 $3$ 倍，省下了 $5$ 个查找单位时间，但是这种情况出现的概率呢？千万分之六. 存储结构及冲突解决方案 Java8 之前：数组加链表 Java8 及之后：数组+链表+红黑树 (当链表的长度大于 $8$ 时转化为红黑树，当红黑树的结点个数小于 $6$ 时再转化为链表, 这样做是为了保证在最坏查找情况下 HashMap的查找时间复杂度为 $O(\\lg n)$，而不是 $O(n)$ )。 红黑树复习 其他解决冲突的方法：开放定址法，线性探测再散列、平方探测法、再哈希法、双散列函数法… 内部类从源码可知，HashMap类中有一个实现了 Map.Entry&lt;&gt; 接口的静态类 static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt;，有一个非常重要的字段定义为 transient Node&lt;K,V&gt;[] table; 即哈希桶数组，也就是上图中的Table数组。 1234567891011121314151617181920212223static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + \"=\" + value; &#125; public final V setValue(V newValue) &#123; ... &#125; /** 重写 hashCode 一定要重写 equals */ public final int hashCode() &#123; ... &#125; public final boolean equals(Object o) &#123; ... &#125;&#125; put() 方法1234map.put(\"Banana\", \"香蕉\");- 系统将调用\"Banana\"这个 key 的 hashCode() 方法得到其 hashCode 值 - (每个Java对象都有这个方法, Object 类为所有类的父类, 所以 Object 内的方法和变量子类也拥有.)- 高位运算和取模运算, 来定位该键值对的存储位置, 有时两个 key 会定位到相同的位置，表示发生了 Hash 碰撞。当然Hash算法计算结果越分散均匀, Hash 碰撞的概率就越小, map 的存取效率就会越高. put() 方法的逻辑 如果 HashMap 未被初始化过，则初始化 对 key 求 Hash 值，然后计算下标 如果没有碰撞，则直接放入桶中 如果碰撞了，以链表的方式链接到后边 如果链表长度 &gt;= TREEIFY_THRESHOLD - 1 ，并且 table &gt;= MIN_TREEIFY_CAPACITY 将链表转化为红黑树，否则可能只是扩容。 如果红黑树结点小于 $6$ ，将红黑树转化为链表 如果结点已经存在就替换旧值 如果初始桶装满了(容量 $16 \\times 0.75 = 12$，就需要扩容 resize()(扩容 $2$ 倍) 如何有效减少碰撞？ 扰动函数：促使元素位置分布均匀，减少碰撞几率 使用 final 对象，并采用合适的 equals() 和 hashCode() 方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;/** * Implements Map.put and related methods. * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don't change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none */final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 判断数组是否为空或者为 null 若是则进行扩容并将扩容后的长度赋值给 n if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 根据 key 值计算得到的 hash 值寻找数组索引 i // 如果 tab[i] 为空 则创建并添加新的结点 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; // 如果 tab[i] 不空 说明当前位置有元素 Node&lt;K,V&gt; e; K k; // 判断 tab[i] 的首个元素是否和key一样，如果相同直接覆盖value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果 p 是红黑树的结点 如果是红黑树 则在红黑树中添加键值对 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 不是红黑树，则是普通的链表 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 链表长度大于等于 8 转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st // 树化 treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; // key 已经存在直接覆盖value p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 扩容次数 // 超过最大容量 就扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 确定索引位置1234567891011121314static final int hash(Object key) &#123; //jdk1.8 &amp; jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 无符号右移 16 位 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;static int indexFor(int h, int length) &#123; //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的 return h &amp; (length-1); //第三步 取模运算 // 这个方法非常巧妙，它通过h &amp; (table.length -1)来得到该对象的保存位 // 而HashMap底层数组的长度总是 2 的n次方(见下方)，这是HashMap在速度上的优化 // 当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length // 但是&amp;比%具有更高的效率&#125; HashMap怎么保证底层数组的长度总是 $2^n$ ？试一试就知道了。 123456789static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 这里的Hash算法本质上就是三步：取key的hashCode值、高位运算、取模运算。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高 $16$ 位异或低 $16$ 位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组 table.length 比较小的时候，也能保证考虑高低位都参与到Hash的计算中，同时不会有太大的开销。 扩容 resize()在理解Hash和扩容流程之前，我们得先了解下 HashMap 的几个字段。从 HashMap 的默认构造函数源码可知，构造函数就是对下面几个字段进行初始化，源码如下： 123456789101112131415public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; int threshold; // 所能容纳的 key-value 阈值 // 如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值； // 相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 final float loadFactor; // 负载因子 int modCount; // 用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。 int size; // 实际存在的键值对数量 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 默认初始容量 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 默认负载因子 static final int TREEIFY_THRESHOLD = 8; // 树化门槛 static final int UNTREEIFY_THRESHOLD = 6; // 反树化 即转化为链表门槛&#125; 扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，将小容量数组中的元素复制到大容量数组里，并释放小容量数组。 分析下 resize 的源码，鉴于JDK1.8融入了红黑树，较复杂，为了便于理解仍然使用JDK1.7的代码，好理解一些，本质上区别不大，具体区别后文再说。 此部分内容大量参考- 美团技术团队知乎文章 12345678910111213 1 void resize(int newCapacity) &#123; //传入新的容量 2 Entry[] oldTable = table; //引用扩容前的Entry数组 3 int oldCapacity = oldTable.length; 4 if (oldCapacity == MAXIMUM_CAPACITY) &#123; //扩容前的数组大小如果已经达到最大(2^30)了 5 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 6 return; 7 &#125; 8 9 Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组10 transfer(newTable); //！！将数据转移到新的Entry数组里11 table = newTable; //HashMap的table属性引用新的Entry数组12 threshold = (int)(newCapacity * loadFactor);//修改阈值13 &#125; 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer() 方法将原有Entry数组的元素拷贝到新的Entry数组里。 1234567891011121314151617 1 void transfer(Entry[] newTable) &#123; 2 Entry[] src = table; //src引用了旧的Entry数组 3 int newCapacity = newTable.length; 4 for (int j = 0; j &lt; src.length; j++) &#123; //遍历旧的Entry数组 5 Entry&lt;K,V&gt; e = src[j]; //取得旧Entry数组的每个元素 6 if (e != null) &#123; 7 src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） 8 do &#123; 9 Entry&lt;K,V&gt; next = e.next;10 int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置11 e.next = newTable[i]; //标记[1]12 newTable[i] = e; //将元素放在数组上13 e = next; //访问下一个Entry链上的元素14 &#125; while (e != null);15 &#125;16 &#125;17 &#125; newTable[i] 的引用赋给了 e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到Entry链的尾部(如果发生了hash冲突的话），这一点和JDK1.8有区别，下文详解。在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 下面举个例子说明下扩容过程。假设了我们的hash算法就是简单的用 key mod table.size()。其中的哈希桶数组table的size = 2， 所以 key = 3、7、5，put顺序依次为 $ 5、7、3$。在 mod 2 以后都冲突在 table[1] 这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小 size 大于table 的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize 成 $4$，然后所有的 $Node$ 重新 rehash 的过程。 下面我们讲解下JDK1.8做了哪些优化。经过观测可以发现，我们使用的是 $2$ 次幂的扩展(指长度扩为原来 $2$ 倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动 $2$ 次幂的位置。看下图可以明白这句话的意思，$n$ 为 $table$ 的长度， 图$(a)$表示扩容前的 key1 和 key2 两种 key 确定索引位置的示例 图$(b)$表示扩容后 key1和 key2 两种 key 确定索引位置的示例，其中 hash1 是 key1 对应的哈希与高位运算结果。 元素在重新计算hash之后，因为 $n$ 变为 $2$ 倍，那么 $n-1$ 的 $mask$ 范围在高位多 $1bit$ (红色)，因此新的 $index$ 就会发生这样的变化： 因此，我们在扩充 $HashMap$ 的时候，不需要像 $JDK1.7$ 的实现那样重新计算 $hash$，只需要看看原来的 $hash$ 值新增的那个 $bit$ 是 $1$ 还是 $0$ 就好了，是 $0$ 的话索引没变，是 $1$ 的话索引变成 原索引+oldCap，可以看看下图为 $16$ 扩充为 $32$ 的 $resize$ 示意图： 这个设计确实非常的巧妙，既省去了重新计算 $hash$ 值的时间，而且同时，由于新增的 $1bit$ 是 $0$ 还是 $1$ 可以认为是随机的，因此 $resize$ 的过程，均匀的把之前的冲突的节点分散到新的 $bucket$ 了。这一块就是 $JDK1.8$ 新增的优化点。有一点注意区别，$JDK1.7$中 $rehash$ 的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，$JDK1.8$ 不会倒置。有兴趣的同学可以研究下$JDK1.8$的 $resize$ 源码，写的很赞，如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 线程安全性 HashMap为什么是线程不安全的? 首先，说HashMap是线程不安全的，并不仅仅是指循环链表的问题，而是在设计与实现上就没有考虑并发环境。 HashMap的设计目标是非并发环境下的简洁高效，并没有采取任何措施来保证put,remove以及其他操作的线程安全性，get操作的死循环问题只是线程不安全中最突出的代表。 1234JDK 1.7在 resize() 操作时, 可能有多个线程同时检测到需要扩容, 然后分别进行扩容, 而扩容时将结点元素从 oldTable 迁移到 newTable 时采用的是头插法, 头插法的结果是与原顺序相反, 就比如 1-&gt;2-&gt;3的逆序3-&gt;2-&gt;1, 在使用 next 指针把两个线程分别扩容后的结果连接起来时, 是有可能发生 1-&gt;2-&gt;3-&gt;2-&gt;1 的, 这样首尾相连就会形成环形, 从而无限循环。1.8 修复了链表倒置的问题, 但是在 put 的手仍然会出现数据不一致的问题. 123456上面说到，HashMap会进行resize操作，在resize操作的时候会造成线程不安全。下面将举两个可能出现线程不安全的地方。- put的时候导致的多线程数据不一致这个问题比较容易想象，比如有两个线程A和B，首先A希望插入一个key-value对到HashMap中，首先计算记录所要落到的桶的索引坐标，然后获取到该桶里面的链表头结点，此时线程A的时间片用完了，而此时线程B被调度得以执行，和线程A一样执行，只不过线程B成功将记录插到了桶里面，假设线程A插入的记录计算出来的桶索引和线程B要插入的记录计算出来的桶索引是一样的，那么当线程B成功插入之后，线程A再次被调度运行时，它依然持有过期的链表头但是它对此一无所知，以至于它认为它应该这样做，如此一来就覆盖了线程B插入的记录，这样线程B插入的记录就凭空消失了，造成了数据不一致的行为。- get操作可能因为resize而引起死循环(CPU 100%), 扩容的核心代码为上面代码块中的transfer()方法. 具体分析见：美团文章 - 线程安全性部分 解决办法： Collections.synchronizedMap() : 加入了 synchronized 代码块 java.util.concurrent.ConcurrentHashMap() : CAS + synchronized 使锁更细化 ConcurrentHashMap : put() 方法的逻辑 判断 Node[] 数组是否初始化，没有则进行初始化操作 通过 hash 定位数组的索引下标，是否有 Node 结点，如果没有则使用 CAS 进行添加(链表的头结点) ，添加失败则进入下次循环. 检查到内部正在扩容，就帮助它一起扩容 如果 f != null ，则使用 synchronized 锁住 f 元素(链表/红黑树的头元素) 4.1 如果是 Node (链表结构) 则执行链表的添加操作 4.2 如果是 TreeNode (树形结构) 则执行树添加操作 判断链表长度已经达到临界值 $8$(默认值，可修改)，当结点数超过这个值就需要把链表转换为树结构。 ConcurrentHashMap : 需要注意的点 size() 方法和 mappingCount() 方法的异同，两者计算是否准确？ 多线程环境下如何进行扩容？ HashMap、Hashtable、ConcurrentHashMap区别 比较方面 HashMap Hashtable ConcurrentHashMap 是否线程安全 否 是 是 线程安全采用的方式 无 采用synchronized类锁，效率低 CAS + synchronized，锁住的只有当前操作的bucket，不影响其他线程对其他bucket的操作，效率高 数据结构 数组+链表+红黑树(链表长度超过8则转红黑树) 数组+链表 数组+链表+红黑树(链表长度超过8则转红黑树) 是否允许null键值 是 否 否 哈希地址算法 (h=key.hashCode())^(h&gt;&gt;&gt;16 key.hashCode() (h=key.hashCode())^(h&gt;&gt;&gt;16)&amp;0x7fffffff 定位算法 (n-1)&amp;hash (hash&amp;0x7fffffff)%n (n-1)&amp;hash 扩容算法 当键值对数量大于阈值，则容量扩容到原来的2倍 当键值对数量大于等于阈值，则容量扩容到原来的2倍+1 当键值对数量大于等于sizeCtl，单线程创建新哈希表，多线程复制bucket到新哈希表，容量扩容到原来的2倍 链表插入 将新节点插入到链表尾部 将新节点插入到链表头部 将新节点插入到链表尾部 继承的类 继承abstractMap抽象类 继承Dictionary抽象类 继承abstractMap抽象类 实现的接口 实现Map接口 实现Map接口 实现ConcurrentMap接口 默认容量大小 16 11 16 默认负载因子 0.75 0.75 0.75 统计size方式 直接返回成员变量size 直接返回成员变量count 遍历CounterCell数组的值进行累加，最后加上baseCount的值即为size","categories":[{"name":"Java","slug":"Java","permalink":"http://raymond-zhao.top/categories/Java/"}],"tags":[{"name":"HashMap","slug":"HashMap","permalink":"http://raymond-zhao.top/tags/HashMap/"}]},{"title":"LFU缓存","slug":"2020-07-05-Algo-LFU","date":"2020-07-05T13:29:35.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/07/05/2020-07-05-Algo-LFU/","link":"","permalink":"http://raymond-zhao.top/2020/07/05/2020-07-05-Algo-LFU/","excerpt":"","text":"LFU缓存LeetCode 460. LFU缓存请你为 最不经常使用（LFU）缓存算法设计并实现数据结构。它应该支持以下操作：$get$ 和 $put$。 $get(key)$: - 如果键存在于缓存中，则获取键的值（总是正数），否则返回 $-1$。 $put(key, value)$ - 如果键已存在，则变更其值；如果键不存在，请插入键值对。当缓存达到其容量时，则应该在插入新项之前，使最不经常使用的项无效。在此问题中，当存在平局（即两个或更多个键具有相同使用频率）时，应该去除最久未使用的键。 「项的使用次数」就是自插入该项以来对其调用 $get$ 和 $put$ 函数的次数之和。使用次数会在对应项被移除后置为 $0$ 。 123456789101112LFUCache cache = new LFUCache( 2 /* capacity (缓存容量) */ );cache.put(1, 1);cache.put(2, 2);cache.get(1); // 返回 1cache.put(3, 3); // 去除 key 2cache.get(2); // 返回 -1 (未找到key 2)cache.get(3); // 返回 3cache.put(4, 4); // 去除 key 1cache.get(1); // 返回 -1 (未找到 key 1)cache.get(3); // 返回 3cache.get(4); // 返回 4 Solution-双哈希 freqMap: 以频率freq为索引(key)，每个索引存放一个双向链表(value)。 双向链表：存放所有使用频率为freq 的缓存，缓存里存放三个信息，分别为key,value以及使用频率freq。 cache: 以key为索引，每个索引存放对应缓存在freqMap中链表里的内存地址。 minFreq: 当前缓存最少使用的频率，为删除操作服务。 利用双哈希可以实现get和put操作的时间复杂度为 $O(1)$. 接下来要关注的点就是如何实现get和put的功能的同时维护以上三个变量。 get(key): 通过索引key在cache中找到缓存freqMap中的链表的内存地址 如果不存在：直接返回 $-1$； 如果存在：获取缓存的相关信息，取得缓存的键值以及使用频率。 get后使用频率freq加 $1$，更新缓存在freqMap中的位置。已知这个缓存的key,value,freq，那么该缓存该存放到freqMap中以freq + 1为索引的链表中。以 $O(1)$ 删除该缓存对应的结点，根据情况更新minFreq值，然后以 $O(1)$ 插入到以freq + 1为索引的链表头部完成更新。 之所以插入到链表头部，是为了保证缓存在当前链表中从链表头到链表尾的插入时间是有序的，为删除操作服务。 put(key, value)操作: 就跟所有的Map操作一样，先检查key-value是否已存在，然后再做对应的处理。 已存在：将当前缓存的值更新为value 不存在：添加进哈希表，同时检查缓存是否已达到最大容量capacity，如果达到，先删除最近最少用的缓存，再进行插入。 插入时结点的频率为 $1$，链接到freqMap中频率为 $1$ 的链表的头部。 此时minFreq一定为 $1$. 删除操作：由于维护了minFreq，所以能够知道freqMap中目前使用频率最少的索引，以 $O(1)$ 获得频率等于minFreq的链表；同时由于保证了链表从链表头到链表尾的插入时间是有序的，所以freqMap.get(minFreq)获得的链表中的链表尾的结点即为使用频率最小且插入时间最早的结点，删除它，同时根据情况更新minFreq，整个时间复杂度为 $O(1)$. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128class LFUCache &#123; private int minFreq, capacity, size; // 最小频率, 缓存容量, 缓存实际存储的元素数量 private Map&lt;Integer, DLinkedNode&gt; cache; private Map&lt;Integer, DLinkedList&gt; freqMap; public LFUCache(int capacity) &#123; this.capacity = capacity; cache = new HashMap&lt;&gt;(); freqMap = new HashMap&lt;&gt;(); &#125; public int get(int key) &#123; DLinkedNode node = cache.get(key); if (node == null) return -1; // 维护 minFreq 以及 node 本身的频率 incrementFreq(node); return node.value; &#125; public void put(int key, int value) &#123; if (capacity == 0) return; DLinkedNode node = cache.get(key); if (node == null) &#123; // 添加新结点 if (size == capacity) &#123; // 达到最大容量，删除最近最少用结点。 DLinkedList minFreqList = freqMap.get(this.minFreq); DLinkedNode tailNode = minFreqList.tail.pre; cache.remove(tailNode.key); minFreqList.remove(tailNode); size--; &#125; DLinkedNode newNode = new DLinkedNode(key, value); cache.put(key, newNode); DLinkedList dLinkedList = freqMap.get(1); if (dLinkedList == null) &#123; dLinkedList = new DLinkedList(); freqMap.put(1, dLinkedList); &#125; dLinkedList.addToHead(newNode); size++; this.minFreq = 1; &#125; else &#123; // 调整结点 node.value = value; incrementFreq(node); &#125; &#125; private void incrementFreq(DLinkedNode node) &#123; // 1. 从原链表中删除 int freq = node.freq; DLinkedList freqLinkedList = freqMap.get(freq); freqLinkedList.remove(node); // 2. 添加 node 到新的 freq 对应的链表的头部 if (freq == this.minFreq &amp;&amp; freqLinkedList.isEmpty()) // 如果该结点已是最小频率的结点并且已经空了 this.minFreq = freq + 1; // 3. 将结点添加到新的 freq 对应的链表 ++node.freq; freqLinkedList = freqMap.get(freq + 1); if (freqLinkedList == null) &#123; freqLinkedList = new DLinkedList(); freqMap.put(freq + 1, freqLinkedList); &#125; freqLinkedList.addToHead(node); &#125; /** * 自定义双向链表结点 */ private static class DLinkedNode &#123; private int key, value; private int freq = 1; private DLinkedNode pre, next; public DLinkedNode() &#123;&#125; public DLinkedNode(int key, int value) &#123; this.key = key; this.value = value; &#125; &#125; /** * 自定义双向链表 */ private static class DLinkedList &#123; private DLinkedNode head, tail; public DLinkedList() &#123; head = new DLinkedNode(); tail = new DLinkedNode(); head.next = tail; tail.pre = head; &#125; /** * 移除频率增加前的链表中的结点 */ private void remove(DLinkedNode node) &#123; node.pre.next = node.next; node.next.pre = node.pre; &#125; /** * 添加到频率增加后的链表的头部 */ private void addToHead(DLinkedNode node) &#123; node.pre = head; node.next = head.next; head.next.pre = node; head.next = node; &#125; private boolean isEmpty() &#123; return head.next == tail ? true : false; &#125; &#125;&#125; Redis中的LFU","categories":[{"name":"算法","slug":"算法","permalink":"http://raymond-zhao.top/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://raymond-zhao.top/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"Java LRU及Redis LRU","slug":"2020-07-03-Algo-LRU","date":"2020-07-03T08:20:13.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/07/03/2020-07-03-Algo-LRU/","link":"","permalink":"http://raymond-zhao.top/2020/07/03/2020-07-03-Algo-LRU/","excerpt":"","text":"LRU缓存LRU简介LRU 是 Least Recently Used 的缩写，即最不经常用、最长时间未使用，在操作系统中是一种常用的页面置换算法，选择最近最久未使用的页面予以淘汰。该算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间 $t$，当须淘汰一个页面时，选择现有页面中其 $t$ 值最大的，即最久未使用的页面予以淘汰。 这里说的缓存是一种广义的概念，在计算机存储层次结构中，低一层的存储器都可以看做是高一层的缓存。 缓存可以有效地解决存储器性能与容量的矛盾，但如果缓存算法设计不当，非但不能提高访问速度，反而会使系统变得更慢。 从本质上来说，缓存之所以有效是因为程序和数据的局部性（locality）。程序会按固定的顺序执行，数据会存放在连续的内存空间并反复读写。这些特点使得我们可以缓存那些经常用到的数据，从而提高读写速度。 缓存的大小是固定的，它应该只保存最常被访问的那些数据。然而未来不可预知，我们只能从过去的访问序列做预测，于是就有了各种各样的缓存替换策略。 LeetCode题目题目描述 点我去做 - 146. LRU缓存机制 运用你所掌握的数据结构，设计和实现一个 LRU (最久未使用) 缓存机制。它应该支持以下操作： 获取数据 get 和 写入数据 put 。 获取数据 get(key)：如果关键字 (key) 存在于缓存中，则获取关键字的值（总是正数），否则返回 -1。 写入数据 put(key, value)： 如果关键字已经存在，则变更其数据值； 如果关键字不存在，则插入该组「关键字/值」； 当缓存容量达到上限时，它应该在写入新数据之前删除最久未使用的数据值，从而为新的数据值留出空间。 进阶：你是否可以在 $O(1)$ 时间复杂度内完成这两种操作？ 1234567891011LRUCache cache = new LRUCache( 2 /* 缓存容量 */ );cache.put(1, 1);cache.put(2, 2);cache.get(1); // 返回 1cache.put(3, 3); // 该操作会使得关键字 2 作废cache.get(2); // 返回 -1 (未找到)cache.put(4, 4); // 该操作会使得关键字 1 作废cache.get(1); // 返回 -1 (未找到)cache.get(3); // 返回 3cache.get(4); // 返回 4 解题思路实现本题的两种操作，需要用到一个哈希表和一个双向链表。在Java语言中，LinkedHashMap拥有这种数据结构。而且利用这内置的数据结构很容易就可以实现LRU缓存的功能。 12345678910111213141516171819202122public class LRUCache extends LinkedHashMap&lt;Integer, Integer&gt; &#123; private int capacity; public LRUCache(int capacity) &#123; super(capacity, 0.75f, true); this.capacity = capacity; &#125; public int get(int key) &#123; return super.getOrDefault(key, -1); &#125; public void put(int key, int value) &#123; super.put(key, value); &#125; @Override protected boolean removeEldestEntry(Map.Entry&lt;Integer, Integer&gt; eldest) &#123; return size() &gt; capacity; &#125;&#125; 扩展阅读：LinkedHashMap源码分析-像一只狗-掘金 上面的代码虽然能够通过 LeetCode 的测试，但是如果在面试的时候面试官更想要看到的应该是候选人手动实现 LRU 的功能，而不是直接调用JDK。 通过上面的内容可以了解到LRU 缓存机制主要用到了哈希表与双向链表，在这个过程中： 哈希表：通过缓存数据的key在 $O(1)$ 的时间内获得Node在双向链表中的位置，结构其实是 Map&lt;Integer, Node&gt;； 双向链表：按照被使用的顺序存储键值对(结点)，靠近 head 的结点是最近刚刚使用的，而靠近 tail 的结点则是最长时间未使用的。 这样一来，我们需要做的就是： 使用哈希表进行定位，找出缓存的数据在双向链表中的位置； 根据进行的操作 get/put 以及容量 capacity 维护缓存数据在哈希表和双向链表中存储 把最近访问过的数据项移动到链表头部； 在哈希表中 insert/update/delete 数据项。 算法流程： 创建双向链表的结点Node 创建双向链表用于维护结点的使用信息 get(key) 操作 如果 key 不存在，则返回 -1； 如果 key 存在，则 key 对应的结点是刚刚被使用过的结点，通过哈希表定位到该结点在双向链表中的位置，并将其移动到双向链表的头部，最后返回该结点的值。 put(key, value)操作 key == null，使用 key-value 创建一个新的结点 node，添加该结点到双向链表头部 head，并将 key-node 添加到哈希表中。然后判断结点数量是否超出容量，如果超出，则删除双向链表的尾部结点，并删除哈希表中其对应的映射。 key != null，与 get 操作类似，先通过哈希表定位，再将对应结点的值更新为 value，并将该结点移动到双向链表的头部。 哨兵结点： 在双向链表的实现中，使用伪头部 dummy head 和伪尾部 dummy tail 标记界限，这样在添加和删除结点的时候就不需要再检查相邻的结点是否存在。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788public class LRUCache &#123; private static class DLinkedNode &#123; private int key, value; private DLinkedNode pre, next; public DLinkedNode() &#123;&#125; public DLinkedNode(int key, int value) &#123; this.key = key; this.value = value; &#125; &#125; private Map&lt;Integer, DLinkedNode&gt; cache = new HashMap&lt;&gt;(); private int size; // 缓存中实际存储的元素数量 private int capacity; // 缓存容量 private DLinkedNode head, tail; public LRUCache(int capacity) &#123; this.size = 0; this.capacity = capacity; head = new DLinkedNode(); tail = new DLinkedNode(); head.next = tail; tail.pre = head; &#125; public int get(int key) &#123; DLinkedNode node = cache.get(key); if (node == null) return -1; // 不空, 将获取到的结点放到双向链表头部 moveToHead(node); return node.value; &#125; public void put(int key, int value) &#123; if (capacity == 0) return; DLinkedNode node = cache.get(key); if (node == null) &#123; // 创建新结点并插入到双向链表头部, 存入 cache 中。 DLinkedNode newNode = new DLinkedNode(key, value); cache.put(key, newNode); addToHead(newNode); ++size; if (size &gt; capacity) &#123; // 删除双向链表尾部结点 同时清除 cache 中的记录。 DLinkedNode tailNode = removeTail(); cache.remove(tailNode.key); --size; &#125; &#125; else &#123; // 已经存在 key , 修改 value, 移动到头部。 node.value = value; moveToHead(node); &#125; &#125; private void moveToHead(DLinkedNode node) &#123; removeNode(node); // 1. 在原链表中移除当前位置的结点, 否则只是移动到头部的话会产生重复。 addToHead(node); // 2. 移动到头部。 &#125; private void removeNode(DLinkedNode node) &#123; node.pre.next = node.next; node.next.pre = node.pre; &#125; private void addToHead(DLinkedNode node) &#123; // 又是经典的双向链表插入结点问题了 node.pre = head; node.next = head.next; head.next.pre = node; head.next = node; &#125; private DLinkedNode removeTail() &#123; DLinkedNode tailNode = tail.pre; removeNode(tailNode); return tailNode; &#125;&#125; 时间复杂度：get()、put()都是 $O(1)$。 空间复杂度：$O(capacity)$，哈希表和双向链表最多存储 capacity + 1 个元素。 上面的代码应该可以让面试官比较满意了，但是如果想更加突出自己的过人之处还该考虑点什么？ 哨兵 (head、tail) 已经考虑； 线程安全 (synchronized 可行考虑)； 吞吐量 (CAS操作) 可以再提高一点吗？ 考虑使用(ConcurrentHashMap) 分段锁； 如果容忍一定的准确性损失 (ReadWriteLock)； 环形 LRU(Disruptor Ring Buffer)； 提前分配空间的 LRU； 分布式的 LRU 应该如何设计？ 线程安全并且带有定时任务的LRU缓存ConcurrentLinkedQueueConcurrentLinkedQueue 是一个基于单向链表的无界无锁线程安全的队列，适合在高并发环境下使用，效率比较高。在使用时，可以把它理解为基本的队列，但是保证了多线程下的安全性。和普通队列一样，它也是按照先进先出(FIFO)的规则对接点进行排序。 另外，队列元素中不可以放置 null 元素。 ConcurrentLinkedQueue 整个继承关系如下图所示： ConcurrentLinkedQueue 中最主要的两个方法是：offer(value)和poll()，分别实现队列的两个重要的操作：入队和出队。(offer(value) 基本等价于 add(value))。 当添加一个元素到队列的时候，它会添加到队列的尾部，当获取一个元素时，它会返回队列头部的元素。 利用 ConcurrentLinkedQueue 先进先出的特性，每当 put/get (缓存被使用)元素的时候，就将元素存放在队列尾部，这样能够保证头部的元素是最近最少使用的。 ReadWriteLockReadWriteLock 是一个接口，位于java.util.concurrent.locks包下，里面只有两个方法分别返回读锁和写锁： 1234567891011public interface ReadWriteLock &#123; /** * @return the lock used for reading */ Lock readLock(); /** * @return the lock used for writing */ Lock writeLock();&#125; ReentrantReadWriteLock 是 ReadWriteLock 接口的具体实现类。 读写锁比较适合缓存这种读多写少的场景。读写锁可以保证多个线程同时读取，但是只有一个线程可以写入。但是当读锁被线程持有的时候，读锁是无法被其他线程申请，会处于阻塞状态，直至读锁被释放。该锁也是可重入锁。 另外，同一个线程持有写锁时可以继续申请读锁，但是持有读锁的时候不可以申请写锁。 ScheduledExecutorServiceScheduledExecutorService 是一个接口，ScheduledThreadPoolExecutor 是其主要实现类。 ScheduledThreadPoolExecutor 主要用来在给定的延迟后运行任务，或者定期执行任务。 这个在实际项目用到的比较少，因为有其他方案选择比如 quartz。但是，在一些需求比较简单的场景下还是非常有用的！ ScheduledThreadPoolExecutor 使用的任务队列 DelayQueue 封装了一个 PriorityQueue，PriorityQueue 会对队列中的任务进行排序，执行所需时间短的放在前面先被执行，如果执行所需时间相同则先提交的任务将被先执行。 线程安全的LRU缓存 实现方法：ConcurrentHashMap + ConcurrentLinkedQueue + ReadWriteLock。 这里可能会有疑问的点是：在LeetCode中选择使用双向链表来维护结点之前按使用时间的关系，也可以实现在 $O(1)$ 的时间内完成将结点的移动和删除操作，但是这里用队列来保存结点，如果刚刚访问了队列中间的结点，那这个结点之前或之后的结点该如何处理呢？ 这里的做法是：每当 put/get 元素的时候，就将这个元素对应的 key 放到队尾，这样就能保证队列头部的元素就是最近最少使用的，当缓存容量不够的时候，直接移除队列头部的key以及这个key对应的缓存即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public class LRUCache&lt;K, V&gt; &#123; private final int maxCapacity; private ConcurrentHashMap&lt;K, V&gt; cacheMap; private ConcurrentLinkedQueue&lt;K&gt; keys; // 读写锁 private ReadWriteLock readWriteLock = new ReentrantReadWriteLock(); private Lock writeLock = readWriteLock.writeLock(); private Lock readLock = readWriteLock.readLock(); public LRUCache(int maxCapacity) &#123; if (maxCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal max capacity\" + maxCapacity); this.maxCapacity = maxCapacity; cacheMap = new ConcurrentHashMap&lt;&gt;(maxCapacity); keys = new ConcurrentLinkedQueue&lt;&gt;(); &#125; public V put(K key, V value) &#123; // 加写锁 writeLock.lock(); try &#123; // 1. 判断 key 是否存在于当前缓存 if (cacheMap.containsKey(key)) &#123; moveToTail(key); cacheMap.put(key, value); return value; &#125; // 2. 是否超出缓存容量，超出的话就移除队列头部的元素以及其对应的缓存 if (cacheMap.size() == maxCapacity) &#123; System.out.println(\"reached to maxCapacity\"); removeEldestKey(); &#125; // 3. key 不存在于当前缓存 将 key 添加到队列的尾部并且缓存 key 及其对应的元素 keys.offer(key); cacheMap.put(key, value); return value; &#125; finally &#123; writeLock.unlock(); &#125; &#125; public V get(K key) &#123; readLock.lock(); try &#123; // 1. 判断 key 是否存在于缓存 if (cacheMap.containsKey(key)) &#123; // 2 存在的话将 key 移动到队列尾部 moveToTail(key); return cacheMap.get(key); &#125; return null; &#125; finally &#123; readLock.unlock(); &#125; &#125; public V remove(K key) &#123; writeLock.lock(); try &#123; // 1. 判断 key 是否存在于缓存 if (cacheMap.containsKey(key)) &#123; keys.remove(key); return cacheMap.remove(key); &#125; return null; &#125; finally &#123; writeLock.unlock(); &#125; &#125; private void removeEldestKey() &#123; K eldestKey = keys.poll(); if (eldestKey != null) cacheMap.remove(eldestKey); &#125; /** * 将元素添加到队列的尾部(put/get的时候执行) */ private void moveToTail(K key) &#123; keys.remove(key); keys.add(key); &#125; private int size() &#123; return cacheMap.size(); &#125;&#125; 非并发环境测试 1234567891011121314151617181920212223public class NonConcurrentTest &#123; public static void main(String[] args) &#123; LRUCache&lt;Integer, String&gt; lruCache = new LRUCache&lt;&gt;(3); lruCache.put(1, \"Java\"); System.out.println(lruCache.get(1)); // Java lruCache.remove(1); System.out.println(lruCache.get(1)); // null lruCache.put(2, \"C++\"); lruCache.put(3, \"Python\"); // [\"C++\", \"Python\"] 队头在左 队尾在右 System.out.println(lruCache.keys); // [2, 3] System.out.println(lruCache.get(2)); // C++ // [\"Python\", \"C++\"] lruCache.put(4, \"C\"); // [\"Python\", \"C++\", \"C\"] System.out.println(lruCache.keys); // [3, 2, 4] lruCache.put(5, \"PHP\"); // [\"C++\", \"C\", \"PHP\"] System.out.println(lruCache.get(2)); // C++ // [\"C\", \"PHP\", \"C++\"] System.out.println(lruCache.keys); // [4, 5, 2] &#125;&#125; 并发环境测试 1234567891011121314151617181920212223242526272829303132public class ConcurrentTest &#123; /** * 初始化一个固定容量为 10 的线程池和 count 为 10 的 CountDownLatch。 * 将 100 次操作分 10 次添加到线程池 * 然后等待线程池执行完成 10 次操作（正常情况下会有10个线程同时执行任务，所以速度很快）。 * @param args */ public static void main(String[] args) throws InterruptedException &#123; int threadNum = 10; int batchSize = 10; LRUCache&lt;String, Integer&gt; lruCache = new LRUCache&lt;&gt;(batchSize * 10); ExecutorService fixedThreadPool = Executors.newFixedThreadPool(threadNum); CountDownLatch countDownLatch = new CountDownLatch(threadNum); AtomicInteger atomicInteger = new AtomicInteger(0); long startTime = System.currentTimeMillis(); for (int t = 0; t &lt; threadNum; t++) &#123; fixedThreadPool.submit(() -&gt; &#123; for (int i = 0; i &lt; batchSize; i++) &#123; int value = atomicInteger.incrementAndGet(); lruCache.put(\"id:\" + value, value); &#125; countDownLatch.countDown(); &#125;); &#125; countDownLatch.await(); fixedThreadPool.shutdown(); System.out.println(\"Cache size:\" + lruCache.size()); long endTime = System.currentTimeMillis(); long duration = endTime - startTime; System.out.println(String.format(\"Time cost：%dms\", duration)); &#125;&#125; 线程安全并且带有定时任务的LRU缓存实际上就是在上面实现的LRU的基础上加上一个定时任务去删除缓存，实现定时任务的方式有很多种： Timer :不被推荐，多线程会存在问题。 ScheduledExecutorService ：定时器线程池，可以用来替代 Timer DelayQueue ：延时队列 quartz ：一个很火的开源任务调度框架，很多其他框架都是基于 quartz 开发的，比如当当网的elastic-job就是基于quartz二次开发之后的分布式调度解决方案 $\\dots$ 因为只是一个小练习，所以选择JDK自带的ScheduledExecutorService，它基于DelayQueue做了很多封装，能够满足大部分需求。 在上面实现的基础上增加一个方法，在put操作的时候，通过这个方法就能直接设置过期时间。 1234567private void removeAfterExpire(K key, long expireTime) &#123; scheduledExecutorService.schedule(() -&gt; &#123; //过期后清除该键值对 cacheMap.remove(key); keys.remove(key); &#125;, expireTime, TimeUnit.MILLISECONDS);&#125; 12345678910111213141516171819202122232425public V put(K key, V value, long expireTime) &#123; // 加写锁 writeLock.lock(); try &#123; //1.key是否存在于当前缓存 if (cacheMap.containsKey(key)) &#123; moveToTail(key); cacheMap.put(key, value); return value; &#125; //2.是否超出缓存容量，超出的话就移除队列头部的元素以及其对应的缓存 if (cacheMap.size() == maxCapacity) &#123; System.out.println(\"maxCapacity of cache reached\"); removeEldestKey(); &#125; //3.key不存在于当前缓存。将key添加到队列的尾部并且缓存key及其对应的元素 keys.add(key); cacheMap.put(key, value); if (expireTime &gt; 0) removeAfterExpireTime(key, expireTime); return value; &#125; finally &#123; writeLock.unlock(); &#125;&#125; 存在的问题：Lock 与 ConcurrentXxx一起使用显得多余。从此也可以产生两种方案 读写锁加基本的HashMap、双向链表，或者synchronized同步方法。 使用并发工具类ConcurrentXxxx 待改进之处：维护队列 $O(n)$，可以使用其他数据结构减少到 $O(1)$。 Redis 中的 LRURedis 作为缓存使用时，一些场景下要考虑内存的空间消耗问题。Redis 会删除过期键以释放空间，过期键的删除策略有两种： 惰性删除：每次从键空间获取键时，都检查键是否过期，如果过期的话就删除该键，否则返回该键。 定期删除：每隔一段时间，程序就对数据库进行一次检查，删除里面的过期键。 当需要从缓存中淘汰数据时，我们希望能淘汰那些将来不可能再被使用的数据，保留那些将来还会频繁访问的数据，但最大的问题是缓存并不能预言未来。一个解决方法就是通过 LRU 进行预测： 最近被频繁访问的数据将来被访问的可能性也越大； 缓存中的数据一般会有这样的访问分布： 一部分数据拥有绝大部分的访问量； 当访问模式很少改变时，可以记录每个数据的最后一次访问时间，拥有最少空闲时间的数据可以被认为将来最有可能被访问到。 LRU配置参数Redis配置中和LRU有关的有三个： maxmemory：配置 Redis 存储数据时指定限制的内存大小，比如 100m。当缓存消耗的内存超过这个数值时, 将触发数据淘汰。该数据配置为 $0$ 时，表示缓存的数据量没有限制, 即 $LRU$ 功能不生效。$64$ 位的系统默认值为 $0$，$32$ 位的系统默认内存限制为 $3GB$。 maxmemory_policy：触发数据淘汰后的淘汰策略。 maxmemory_samples：随机采样的精度，也就是随即取出 $key$ 的数目。该数值配置越大, 越接近于真实的 $LRU$ 算法，但是数值越大，相应消耗也变高，对性能有一定影响，样本值默认为 $5$ 。 内存淘汰策略在 Redis 中的内存淘汰策略有： volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最久未使用的数据淘汰； volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰； volatile-random：从已设置过期时间的数据集（server.db[i].expires）中随机选取数据淘汰； allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最久未使用的 key； allkeys-random：从数据集（server.db[i].dict）中随机选取数据淘汰； no-eviction：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。 volatile-xxx 这三个淘汰策略使用的不是全量数据，有可能无法淘汰出足够的内存空间。在没有过期键或者没有设置超时属性的键的情况下，这三种策略和 noeviction 差不多。 一般的经验规则： 使用 allkeys-lru 策略：当预期请求符合一个幂次分布(二八法则等)，比如一部分的子集元素比其它元素被访问的更多时，可以选择这个策略。 使用 allkeys-random 策略：循环连续的访问所有的键时，或者预期请求分布平均（所有元素被访问的概率都差不多）。 使用 volatile-ttl：要采取这个策略，缓存对象的 TTL 值最好有差异。 volatile-lru 和 volatile-random 策略，当想要使用单一的 Redis 实例来同时实现缓存淘汰和持久化一些经常使用的键集合时很有用。 对未设置过期时间的键进行持久化保存，对设置了过期时间的键参与缓存淘汰。 不过一般运行两个实例是解决这个问题的更好方法。 为键设置过期时间也是需要消耗内存的，所以使用 allkeys-lru 这种策略更加节省空间，因为这种策略下可以不为键设置过期时间。 4.0 版本后增加以下两种： volatile-lfu：从已设置过期时间的数据集(server.db[i].expires)中挑选最不经常使用的数据淘汰。 allkeys-lfu：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 key。 近似LRU算法 $LRU$ 算法需要使用一个双向链表来记录数据的最近被访问顺序，出于节省内存的考虑，$Redis$ 的 $LRU$ 算法并未完整实现。 $Redis$ 并不会选择最久未被访问的键进行回收，而是尝试运行一个近似 $LRU$ 算法，通过对少量键进行取样，然后回收其中的最久未被访问的键。通过调整每次回收时的采样数量 maxmemory-samples，可以实现调整算法的精度。 根据 Redis 作者的说法，每个 Redis Object 可以挤出 $24 bits$ 的空间，但 $24 bits$ 是不够存储两个指针的，而够存储一个低位时间戳，Redis Object 以秒为单位存储了对象新建或者更新时的 Unix Time，也就是 LRU Clock，$24 bits$ 数据要溢出的话需要 $194$ 天，而缓存的数据更新非常频繁，已经足够了。 Redis 的键空间是放在一个哈希表中的，要从所有的键中选出一个最久未被访问的键，需要另外一个数据结构存储这些源信息。最初，Redis只是随机地选 $3$ 个 $key$，然后从中淘汰，后来算法改进到了N个key 的策略，默认是 $5$ 个。 Redis 3.0 之后又改善了算法的性能，提供了一个待淘汰候选 $key$ 的pool，里面默认有 $16$ 个 $key$，按照空闲时间排好序。更新时从 Redis 键空间随机选择 $N$ 个 $key$，分别计算它们的空闲时间 idle，$key$ 只会在 pool 不满或者空闲时间大于 pool 里最小的时，才会进入 pool，然后从 pool 中选择空闲时间最大的 $key$ 淘汰掉。 扩展阅读 博客园-再见紫罗兰 简书-后厂村老司机-Redis的缓存淘汰策略LRU与LFU 掘金-Takagi_san-Redis内存淘汰策略源码分析以及LFU/LRU实现","categories":[{"name":"算法","slug":"算法","permalink":"http://raymond-zhao.top/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://raymond-zhao.top/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"Redis内部数据结构","slug":"2020-06-19-Redis-DataStructure","date":"2020-06-19T14:16:12.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/06/19/2020-06-19-Redis-DataStructure/","link":"","permalink":"http://raymond-zhao.top/2020/06/19/2020-06-19-Redis-DataStructure/","excerpt":"","text":"Redis 和其他很多key-value 数据库的不同之处在于，Redis 不仅支持简单的字符串键值对，它还提供了一系列数据结构类型值，比如列表、哈希、集合和有序集，并在这些数据结构类型上 定义了一套强大的 API 。 通过对不同类型的值进行操作，Redis 可以很轻易地完成其他只支持字符串键值对的key-value 数据库很难(或者无法)完成的任务。 在 Redis 的内部，数据结构类型值由高效的数据结构和算法进行支持，并且在 Redis 自身的构建当中，也大量用到了这些数据结构。 1 简单动态字符串Sds (Simple Dynamic String，简单动态字符串)是 Redis 底层所使用的字符串表示，它被用在几乎所有的 Redis 模块中。 1.1 SDS的用途 实现字符串对象(StringObject); 在 Redis 程序内部用作 char* 类型的替代品。 Redis 是一个键值对数据库(key-value DB)，数据库的值可以是字符串、集合、列表等多种类 型的对象，而数据库的键则总是字符串对象。 对于那些包含字符串值的字符串对象来说，每个字符串对象都包含一个 sds 值。 Note: “包含字符串值的字符串对象” ，这种说法初听上去可能会有点奇怪，但是在 Redis 中， 一个字符串对象除了可以保存字符串值之外，还可以保存 long 类型的值，所以为了严谨起见， 这里需要强调一下:当字符串对象保存的是字符串时，它包含的才是 sds 值，否则的话，它就是一个 long 类型的值。 1.2 Redis中的字符串在 C 语言中，字符串可以用一个 \\0 结尾的 char 数组来表示。 比如说，hello world 在 C 语言中就可以表示为 &quot;hello world\\0&quot; 。 这种简单的字符串表示在大多数情况下都能满足要求，但是，它并不能高效地支持长度计算和追加(append)这两种操作: 每次计算字符串长度(strlen(s))的复杂度为 $\\Theta(N)$ 对字符串进行 N 次追加，必定需要对字符串进行 N 次内存重分配(realloc)。 考虑到这两个原因，Redis 使用 sds 类型替换了 C 语言的默认字符串表示:sds 既可以高效地 实现追加和长度计算，并且它还是二进制安全的。 SDS的实现 123456789typedef char *sds;struct sdshdr &#123; // buf 已占用长度 int len; // buf 剩余可用长度 int free; // 实际保存字符串数据的地方 char buf[]; &#125;; 通过 len 属性，sdshdr 可以实现复杂度为 θ(1) 的长度计算操作。另一方面，通过对 buf 分配一些额外的空间，并使用 free 记录未使用空间的大小，sdshdr 可以让执行追加操作所需的内存重分配次数大大减少。 1.3 优化追加操作sds.c/sdsMakeRoomFor 函数描述了 sdshdr 的这种内存预分配优化策略，以下是这个函数的Python伪代码版本: 12345678910111213141516171819def sdsMakeRoomFor(sdshdr, required_len): # 预分配空间足够，无须再进行空间分配 if (sdshdr.free &gt;= required_len): return sdshdr # 计算新字符串的总长度 newlen = sdshdr.len + required_len # 如果新字符串的总长度小于 SDS_MAX_PREALLOC # 那么为字符串分配 2 倍于所需长度的空间 # 否则就分配所需长度加上 SDS_MAX_PREALLOC 数量的空间 if newlen &lt; SDS_MAX_PREALLOC: newlen *= 2 else: newlen += SDS_MAX_PREALLOC # 分配内存 newsh = zrelloc(sdshdr, sizeof(struct sdshdr)+newlen+1) # 更新 free 属性 newsh.free = newlen - sdshdr.len # 返回 return newsh 1.4 小结 Redis 的字符串表示为 sds ，而不是 C 字符串(以 \\0 结尾的 char*)。 对比 C 字符串，sds 有以下特性: 可以高效地执行长度计算(strlen); 可以高效地执行追加操作(append); 二进制安全; sds 会为追加操作进行优化: 加快追加操作的速度，并降低内存分配的次数，代价是多占用了一些内存，而且这些内存不会被主动释放。 2 双端链表双端链表作为一种通用的数据结构，在 Redis 内部使用得非常多:它既是 Redis 列表结构的底层实现之一，还被大量 Redis 模块所使用，用于构建 Redis 的其他功能。 2.1 双端链表的应用实现 Redis 的列表类型 双端链表还是 Redis 列表类型的底层实现之一，当对列表类型的键进行操作——比如执行 RPUSH 、LPOP 或 LLEN 等命令时，程序在底层操作的可能就是双端链表。 Note: Redis 列表使用三种数据结构作为底层实现: 双端链表 压缩列表 快表(Quick List, Redis 3.2 之后提供) 因为双端链表占用的内存比压缩列表要多，所以当创建新的列表键时，列表会优先考虑使用压缩列表作为底层实现，并且在有需要的时候，才从压缩列表实现转换到双端链表实现。 Redis 自身功能的构建 除了实现列表类型以外，双端链表还被很多 Redis 内部模块所应用: 事务模块使用双端链表来按顺序保存输入的命令; 服务器模块使用双端链表来保存多个客户端; 订阅/发送模块使用双端链表来保存订阅模式的多个客户端; 事件模块使用双端链表来保存时间事件(time event). 2.2 双端链表的实现双端链表的实现由 listNode 和 list 两个数据结构构成，下图展示了由这两个结构组成的一 个双端链表实例: 其中，listNode 是双端链表的节点: 12345678typedef struct listNode &#123; // 前驱节点 struct listNode *prev; // 后继节点 struct listNode *next; // 值 void *value;&#125; listNode; 而 list 则是双端链表本身: 1234567891011121314typedef struct list &#123; // 表头指针 listNode *head; // 表尾指针 listNode *tail; // 节点数量 unsigned long len; // 复制函数 void *(*dup)(void *ptr); // 释放函数 void (*free)(void *ptr); // 比对函数 int (*match)(void *ptr, void *key);&#125; list; 另外，从这两个数据结构的定义上，也可以它们的一些行为和性能特征: listNode 带有 prev 和 next 两个指针，因此，对链表的遍历可以在两个方向上进行:从 表头到表尾，或者从表尾到表头。 list 保存了 head 和 tail 两个指针，因此，对链表的表头和表尾进行插入的复杂度都为 $\\Theta(1)$ ——这是高效实现 LPUSH 、RPOP 、RPOPLPUSH 等命令的关键。 list 带有保存节点数量的 len 属性，所以计算链表长度的复杂度仅为 $\\Theta(1)$ ，这也保证 了 LLEN 命令不会成为性能瓶颈。 2.3 迭代器Redis 为双端链表实现了一个迭代器 ，这个迭代器可以从两个方向对双端链表进行迭代: 沿着节点的next指针前进，从表头向表尾迭代; 沿着节点的prev指针前进，从表尾向表头迭代. 以下是迭代器的数据结构定义 123456typedef struct listIter &#123; listNode *next; // 下一节点 int direction; // 迭代方向 // 如果值为 adlist.h/AL_START_HEAD，那么迭代器执行从表头到表尾的迭代; // 如果值为 adlist.h/AL_START_TAIL，那么迭代器执行从表尾到表头的迭代;&#125; listIter; 2.4 小结 Redis 实现了自己的双端链表结构 双端链表主要有两个作用: 作为 Redis 列表类型的底层实现之一; 作为通用数据结构，被其他功能模块所使用。 双端链表及其节点的性能特性如下: 节点带有前驱和后继指针，访问前驱节点和后继节点的复杂度为 $O(1)$ ，并且对链表的迭代可以在从表头到表尾和从表尾到表头两个方向进行; 链表带有指向表头和表尾的指针，因此对表头和表尾进行处理的复杂度为 $O(1)$ ; 链表带有记录节点数量的属性，所以可以在 $O(1)$ 复杂度内返回链表的节点数量(长度). 3 字典字典(dictionary)，又名映射(map)或关联数组(associative array)， 它是一种抽象数据结构，由一集键值对(key-value pairs)组成，各个键值对的键各不相同，程序可以将新的键值对添加到字典中，或者基于键进行查找、更新或删除等操作。 3.1 字典的应用字典在 Redis 中的应用广泛，使用频率可以说和 SDS 以及双端链表不相上下，基本上各个功能模块都有用到字典的地方。其中，字典的主要用途有以下两个: 实现数据库键空间 (key space)，这里指的是整个 Redis 中的键，包括字符串、列表、集合，哈希、有序集合中的所有键; 用作 Hash 类型键的其中一种底层实现。 🙋实现数据库键空间 Redis 是一个键值对数据库，数据库中的键值对就由字典保存：每个数据库都有一个与之相对应的字典，这个字典被称之为键空间(key space)。 当用户添加一个键值 $key-value$ 中的 $key$ 对到数据库时(不论键值对是什么类型)，程序就将该键值对添加到键空间;当用户从数据库中删除一个键值对时，程序就会将这个键值对从键空间中删除等等。 123456# 清空键空间上的所有键值对数据:redis&gt; FLUSHDBok# 返回键空间上现有的键值对:redis&gt; DBSIZE(integer) 0 大部分针对数据库的命令，比如 DBSIZE 、FLUSHDB 、:ref:RANDOMKEY等等，都是构建于对字典的操作之上的;而那些创建、更新、删除和查找键值对的命令，也无一例外地需要在键空间上进行操作。 🙋用作 Hash 类型键的其中一种底层实现 Redis 的 Hash 类型键使用以下两种数据结构作为底层实现: 字典 压缩列表 因为压缩列表比字典更节省内存，所以程序在创建新 Hash 键时，默认使用压缩列表作为底层实现，当有需要时，程序才会将底层实现从压缩列表转换到字典。 3.2 字典的实现实现字典的方法有很多种： 最简单的就是使用链表或数组，但是这种方式只适用于元素个数不多的情况下; 要兼顾高效和简单性，可以使用哈希表; 如果追求更为稳定的性能特征，并且希望高效地实现排序操作的话，则可以使用更为复杂的平衡树。 在众多可能的实现中，Redis 选择了高效且实现简单的哈希表作为字典的底层实现。dict.h/dict 给出了这个字典的定义: 1234567891011121314151617/** 字典** 每个字典使用两个哈希表，用于实现渐进式 rehash */typedef struct dict &#123; // 特定于类型的处理函数 dictType *type; // 类型处理函数的私有数据 void *privdata; // 哈希表(2 个) dictht ht[2]; // 记录 rehash 进度的标志，值为-1 表示 rehash 未进行 int rehashidx; // 当前正在运作的安全迭代器数量 int iterators;&#125; dict; 🙋哈希表的实现 字典所使用的哈希表实现由 dict.h/dictht 类型定义: 1234567891011121314/*** 哈希表的定义*/typedef struct dictht &#123; // 哈希表节点指针数组(俗称桶，bucket) dictEntry **table; // 指针数组的大小 unsigned long size; // 指针数组的长度掩码，用于计算索引值 unsigned long sizemask; // 哈希表现有的节点数量 unsigned long used; &#125; dictht; table 属性是一个数组，数组的每个元素都是一个指向 dictEntry 结构的指针。每个 dictEntry 都保存着一个键值对，以及一个指向另一个 dictEntry 结构的指针: 123456789101112131415/*** 哈希表节点*/typedef struct dictEntry &#123; // key void *key; // value union &#123; void *val; uint64_t u64; int64_t s64; &#125; v; // 链往后继节点 struct dictEntry *next;&#125; dictEntry; next 属性指向另一个 dictEntry 结构，多个 dictEntry 可以通过 next 指针串连成链表，从这里可以看出，dictht 使用链地址法来处理键碰撞: 当多个不同的键拥有相同的哈希值时，哈希表用一个链表将这些键连接起来。 下图展示了一个由 dictht 和数个 dictEntry 组成的哈希表例子: 如果再加上之前列出的 dict 类型，那么整个字典结构可以表示如下: 在上图的字典示例中，字典虽然创建了两个哈希表，但正在使用的只有 $0$ 号哈希表，这说明字典未进行 rehash 状态。 🙋哈希算法 Redis 目前使用两种不同的哈希算法: MurmurHash2 32 bit 算法:这种算法的分布率和速度都非常好，具体信息请参考 MurmurHash 的主页 。 基于 djb 算法实现的一个大小写无关散列算法 使用哪种算法取决于具体应用所处理的数据: 命令表以及 Lua 脚本缓存都用到了算法 2 算法 1 的应用则更加广泛:数据库、集群、哈希键、阻塞操作等功能都用到了这个算法 3.3 创建新字典123// dictCreate 函数创建并返回一个新字典:dict *d = dictCreate(&amp;hash_type, NULL);// d 的值可以用图片表示如下: 新创建的两个哈希表都没有为 table 属性分配任何空间: ht[0]-&gt;table 的空间分配将在第一次往字典添加键值对时进行; ht[1]-&gt;table 的空间分配将在 rehash 开始时进行. 3.4 添加键值对到字典根据字典所处的状态，将一个给定的键值对添加到字典可能会引起一系列复杂的操作: 如果字典为未初始化(也即是字典的 $0$ 号哈希表的 table 属性为空)，那么程序需要对 $0$ 号哈希表进行初始化; 如果在插入时发生了键碰撞，那么程序需要处理碰撞; 如果插入新元素使得字典满足了 rehash 条件，那么需要启动相应的 rehash 程序. 当程序处理完以上三种情况之后，新的键值对才会被真正地添加到字典上，整个添加流程可以用下图表示： 在接下来的三节中，我们将分别看到添加操作如何在以下三种情况中执行: 字典为空; 添加新键值对时发生碰撞处理; 添加新键值对时触发了 rehash 操作. 3.5 添加新元素到空白字典当第一次往空字典里添加键值对时，程序会根据 dict.h/DICT_HT_INITIAL_SIZE 里指定的大 小为 d-&gt;ht[0]-&gt;table 分配空间。 字典空白时的样子如 3.3 创建新字典 图所示，以下是往空白字典添加了第一个键值对之后的样子: 3.6 添加新键值对时发生碰撞处理在哈希表实现中，当两个不同的键拥有相同的哈希值时，我们称这两个键发生碰撞(collision)，而哈希表实现必须想办法对碰撞进行处理。 字典哈希表所使用的碰撞解决方法被称之为链地址法:这种方法使用链表将多个哈希值相同的节点串连在一起，从而解决冲突问题。 假设现在有一个带有三个节点的哈希表，如下图: 对于一个新的键值对 key4 和 value4 ，如果 key4 的哈希值和 key1 的哈希值相同，那么它们将在哈希表的 $0$ 号索引上发生碰撞。通过将 key4-value4 和 key1-value1 两个键值对用链表连接起来，就可以解决碰撞的问题: 3.7 添加新键值对时触发了 rehash 操作对于使用链地址法来解决碰撞问题的哈希表 dictht 来说，哈希表的性能依赖于它的大小(size属性)和它所保存的节点的数量(used 属性)之间的比率: 比率在 $1:1$ 时，哈希表的性能最好; 如果节点数量比哈希表的大小要大很多的话，那么哈希表就会退化成多个链表，哈希表本身的性能优势就不再存在. 举个例子，对于下面这个哈希表，平均每次失败查找只需要访问 $1$ 个节点(非空节点访问 $2$ 次，空节点访问 $1$ 次): 而对于下面这个哈希表，平均每次失败查找需要访问 $5$ 个节点: 为了在字典的键值对不断增多的情况下保持良好的性能，字典需要对所使用的哈希表(ht[0]) 进行 rehash 操作:在不修改任何键值对的情况下，对哈希表进行扩容，尽量将比率维持在 $1:1$ 左右。 dictAdd 在每次向字典添加新键值对之前，都会对哈希表 ht[0] 进行检查，对于 ht[0] 的 size 和 used 属性，如果它们之间的比率 $ratio = used / siz$e 满足以下任何一个条件的话， rehash 过程就会被激活: 自然 $rehash :ratio \\ge 1$ ，且变量 dict_can_resize 为真。 强制 $rehash : ratio$ 大 于 变 量 dict_force_resize_ratio（默认5） 。 什么时候 dict_can_resize 会为假? 一个数据库就是一个字典，数据库里的哈希类型键也是一个字典，当 Redis 使用子进程对数据库执行后台持久化任务时(比如执行 BGSAVE 或 BGREWRITEAOF 时)，为了最大化地利用系统的 copy on write 机制，程序会暂时将 dict_can_resize 设为假，避免执行自然 rehash ，从而减少程序对内存的触碰(touch)。 当持久化任务完成之后，dict_can_resize 会重新被设为真。 另一方面，当字典满足了强制 rehash 的条件时，即使 dict_can_resize 不为真(有 BGSAVE或 BGREWRITEAOF 正在执行)，这个字典仍然会被 rehash 。 3.8 Rehash执行过程字典的 rehash 操作实际上就是执行以下任务: 创建一个比ht[0]-&gt;table容量更大的ht[1]-&gt;table; 将ht[0]-&gt;table中的所有键值对迁移到ht[1]-&gt;table; 将原有ht[0]的数据清空，并将ht[1]替换为新的ht[0](备胎上位) 经过以上步骤之后，程序就在不改变原有键值对数据的基础上，增大了哈希表的大小。 作为例子，以下四个小节展示了一次对哈希表进行 rehash 的完整过程。 1 开始 rehash 这个阶段有两个事情要做: 设置字典的 rehashidx 为 $0$ ，标识着 rehash 的开始; 为ht[1]-&gt;table分配空间，大小至少为ht[0]-&gt;used的两倍. 这时的字典是这个样子: 2 Rehash 进行中 在这个阶段，ht[0]-&gt;table 的节点会被逐渐迁移到 ht[1]-&gt;table ，因为 rehash 是分多次进行的(细节在下一节解释)，字典的 rehashidx 变量会记录 rehash 进行到 ht[0] 的哪个索引。以下是 rehashidx 值为 2 时，字典的样子: 注意除了节点的移动外，字典的 rehashidx 、ht[0]-&gt;used 和 ht[1]-&gt;used 三个属性也产生了变化。 3 节点迁移完毕 到了这个阶段，所有的节点都已经从 ht[0] 迁移到 ht[1] 了: 4 rehash 完毕 在 rehash 的最后阶段，程序会执行以下工作: 释放ht[0]的空间; 用ht[1]来代替ht[0]，使原来的ht[1]成为新的ht[0]; 创建一个新的空哈希表，并将它设置为ht[1]; 将字典的 rehashidx 属性设置为 -1 ，标识 rehash 已停止; 以下是字典 rehash 完毕之后的样子: 对比字典 rehash 之前和 rehash 之后，新的 ht[0] 空间更大，并且字典原有的键值对也没有被修改或者删除。 3.9 渐进式 rehash假设这样一个场景:在一个有很多键值对的字典里，某个用户在添加新键值对时触发了 rehash 过程，如果这个 rehash 过程必须将所有键值对迁移完毕之后才将结果返回给用户，这样的处理方式谁能接受？另一方面，要求服务器必须阻塞直到 rehash 完成，这对于 Redis 服务器本身也是不能接受的。 所以rehash 程序并不是在激活之后就马上执行直到完成的，而是分多次、渐进式地完成的。 为了解决这个问题，Redis 使用了渐进式(incremental)的 rehash 方式：通过将 rehash 分散到多个步骤中进行，从而避免了集中式的计算。 渐进式 rehash 主要由 _dictRehashStep 和 dictRehashMilliseconds 两个函数进行: _dictRehashStep 用于对数据库字典、以及哈希键的字典进行被动 rehash ; dictRehashMilliseconds则由 Redis 服务器常规任务程序(server cron job)执行，用于对数据库字典进行主动 rehash. _dictRehashStep 每次执行 _dictRehashStep ，ht[0]-&gt;table 哈希表第一个不为空的索引上的所有节点就会全部迁移到 ht[1]-&gt;table . 在 rehash 开始进行之后(d-&gt;rehashidx 不为 $-1$)，每次执行一次添加、查找、删除操作，_dictRehashStep 都会被执行一次: dictRehashMilliseconds dictRehashMilliseconds 可以在指定的毫秒数内，对字典进行 rehash 。 当 Redis 的服务器常规任务执行时，dictRehashMilliseconds 会被执行，在规定的时间内， 尽可能地对数据库字典中那些需要 rehash 的字典进行 rehash ，从而加速数据库字典的 rehash 进程(progress)。 其他措施 在哈希表进行 rehash 时，字典还会采取一些特别的措施，确保 rehash 顺利、正确地进行: 因为在 rehash 时，字典会同时使用两个哈希表，所以在这期间的所有查找、删除等操作，除了在 ht[0] 上进行，还需要在ht[1]上进行。 在执行添加操作时，新的节点会直接添加到ht[1]而不是ht[0]，这样保证ht[0]的节点数量在整个 rehash 过程中都只减不增。 3.10 字典收缩上面关于 rehash 的章节描述了通过 rehash 对字典进行扩展(expand)的情况，如果哈希表的可用节点数比已用节点数大很多的话，那么也可以通过对哈希表进行 rehash 来收缩(shrink) 字典。 收缩 rehash 和上面展示的扩展 rehash 的操作几乎一样，它执行以下步骤: 创建一个比ht[0]-&gt;table小的ht[1]-&gt;table; 将ht[0]-&gt;table中的所有键值对迁移到ht[1]-&gt;table; 将原有ht[0]的数据清空，并将ht[1]替换为新的ht[0]. 扩展 rehash 和收缩 rehash 执行完全相同的过程，一个 rehash 是扩展还是收缩字典，关键在于新分配的 ht[1]-&gt;table 的大小: 如果 rehash 是扩展操作，那么 ht[1]-&gt;table 比 ht[0]-&gt;table 要大; 如果 rehash 是收缩操作，那么 ht[1]-&gt;table 比 ht[0]-&gt;table 要小. 字典的收缩规则由 redis.c/htNeedsResize 函数定义: 123456789101112131415/** 检查字典的使用率是否低于系统允许的最小比率 ** 是的话返回 1 ，否则返回 0 。*/int htNeedsResize(dict *dict) &#123; long long size, used; // 哈希表已用节点数量 size = dictSlots(dict); // 哈希表大小 used = dictSize(dict); // 当哈希表的大小大于 DICT_HT_INITIAL_SIZE // 并且字典的填充率低于 REDIS_HT_MINFILL 时 // 返回 1 return (size &amp;&amp; used &amp;&amp; size &gt; DICT_HT_INITIAL_SIZE &amp;&amp; (used*100/size &lt; REDIS_HT_MINFILL));&#125; 在默认情况下，REDIS_HT_MINFILL 的值为 $10$ ，也即是说，当字典的填充率低于 $10%$ 时，程序就可以对这个字典进行收缩操作了。 字典收缩和字典扩展的一个区别是: 字典的扩展操作是自动触发的(不管是自动扩展还是强制扩展); 而字典的收缩操作则是由程序手动执行。 因此，使用字典的程序可以决定何时对字典进行收缩: 当字典用于实现哈希键的时候，每次从字典中删除一个键值对，程序就会执行一次htNeedsResize 函数，如果字典达到了收缩的标准，程序将立即对字典进行收缩; 当字典用于实现数据库键空间 (key space) 的时候，收缩的时机由 redis.c/tryResizeHashTables 函数决定 3.11 字典的迭代字典带有自己的迭代器实现—对字典进行迭代实际上就是对字典所使用的哈希表进行迭代: 迭代器首先迭代字典的第一个哈希表，然后，如果 rehash 正在进行的话，就继续对第二个哈希表进行迭代。 当迭代哈希表时，找到第一个不为空的索引，然后迭代这个索引上的所有节点。 当这个索引迭代完了，继续查找下一个不为空的索引，如此循环，一直到整个哈希表都迭代完为止。 1234567891011121314151617def iter_dict(dict): # 迭代 0 号哈希表 iter_table(ht[0]-&gt;table) # 如果正在执行 rehash ，那么也迭代 1 号哈希表 if dict.is_rehashing(): iter_table(ht[1]-&gt;table) def iter_table(table): # 遍历哈希表上的所有索引 for index in table: # 跳过空索引 if table[index].empty(): continue # 遍历索引上的所有节点 for node in table[index]: # 处理节点 do_something_with(node) 3.12 小结 字典由键值对构成的抽象数据结构。 Redis 中的数据库和哈希键都基于字典来实现。 Redis 字典的底层实现为哈希表，每个字典使用两个哈希表，一般情况下只使用 $0$ 号哈希 表，只有在 rehash 进行时，才会同时使用 $0$ 号和 $1$ 号哈希表。 哈希表使用链地址法来解决键冲突的问题。 rehash 可以用于扩展或收缩哈希表。 对哈希表的 rehash 是分多次、渐进式地进行的。 4 跳跃表跳跃表(skiplist)是一种随机化的数据，由 $William Pugh$ 在论文《Skip lists: a probabilistic alternative to balanced trees》中提出，这种数据结构以有序的方式在层次化的链表中保存元素，它的效率可以和平衡树媲美——查找、删除、添加等操作都可以在对数期望时间下完成， 并且比起平衡树来说，跳跃表的实现要简单直观得多。 以下是一个典型的跳跃表例子(图片来自维基百科): 从图中可以看到，跳跃表主要由以下部分构成: 表头(head): 负责维护跳跃表的节点指针 跳跃表节点: 保存着元素值，以及多个层 层: 保存着指向其他元素的指针。高层的指针越过的元素数量大于等于低层的指针，为了提高查找的效率，程序总是从高层先开始访问，然后随着元素值范围的缩小，慢慢降低层次。 表尾:全部由NULL组成，表示跳跃表的末尾。 4.1 跳跃表的实现为了适应自身的功能需要，Redis 基于 William Pugh 论文中描述的跳跃表进行了以下修改: 允许重复的score值:多个不同的member的score值可以相同。 进行对比操作时，不仅要检查score值，还要检查member：当score值可以重复时， 单靠 score 值无法判断一个元素的身份，所以需要连 member 域都一并检查才行。 每个节点都带有一个高度为 $1$ 层的后退指针，用于从表尾方向向表头方向迭代：当执行 ZREVRANGE 或 ZREVRANGEBYSCORE 这类以逆序处理有序集的命令时，就会用到这个属性。 这个修改版的跳跃表由 redis.h/zskiplist 结构定义: 12345678typedef struct zskiplist &#123; // 头节点、尾结点 struct zskiplistNode *header, *tail; // 节点数量 unsigned long length; // 目前表内节点的最大层数 int level;&#125; zskiplist; 跳跃表的节点由 redis.h/zskiplistNode 定义: 12345678910111213typedef struct zskiplistNode &#123; // member object robj *obj; double score; struct zskiplistNode *backward; // level struct zskiplistLevel &#123; // 前进指针 struct zskiplistNode *forward; // 这个层跨越的节点数量 unsigned int span; &#125; level[];&#125; zskiplistNode; 4.2 跳跃表的应用和字典、链表或者字符串这几种在 Redis 中大量使用的数据结构不同，跳跃表在 Redis 的唯一作用，就是实现有序集数据类型。 跳跃表将指向有序集的 score 值和 member 域的指针作为元素，并以 score 值为索引，对有序集元素进行排序。 举个例子，以下代码就创建了一个带有 $3$ 个元素的有序集: 123456789127.0.0.1:6379&gt; zadd s 6 x 10 y 15 z(integer) 3127.0.0.1:6379&gt; zrange s 0 -1 withscores1) \"x\"2) \"6\"3) \"y\"4) \"10\"5) \"z\"6) \"15\" 在底层实现中，Redis 为 x 、y 和 z 三个 member 分别创建了三个字符串，并为 6 、10 和 15分别创建三个 double 类型的值，然后用一个跳跃表将这些指针有序地保存起来，形成这样一个跳跃表： 为了展示的方便，在图片中我们直接将 member 和 score 值包含在表节点中，但是在实际的定义中，因为跳跃表要和另一个实现有序集的结构(字典)分享 member 和 score 值，所以跳跃表只保存指向 member 和 score 的指针。 小结 跳跃表是一种随机化(概率性)数据结构，它的查找、添加、删除操作都可以在对数期望时间下完成。 跳跃表目前在 Redis 的唯一作用就是作为有序集类型的底层数据结构(之一，另一个构成有序集的结构是字典)。 为了适应自身的需求，Redis 基于 $William Pugh$ 论文中描述的跳跃表进行了修改，包括: score 值可重复 对比一个元素需要同时检查它的score和memeber 每个节点带有高度为 $1$ 层的后退指针，用于从表尾方向向表头方向迭代。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://raymond-zhao.top/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://raymond-zhao.top/tags/Redis/"}]},{"title":"学学 Docker","slug":"2020-06-08-Docker","date":"2020-06-08T00:38:01.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/06/08/2020-06-08-Docker/","link":"","permalink":"http://raymond-zhao.top/2020/06/08/2020-06-08-Docker/","excerpt":"","text":"Docker简介Docker 是一个开放源代码软件，是一个开放平台，用于开发应用、交付（shipping）应用、运行应用。 Docker允许用户将基础设施（Infrastructure）中的应用单独分割出来，形成更小的颗粒（容器），从而提高交付软件的速度。[1] Docker容器 与虚拟机类似，但原理上，容器是将操作系统层虚拟化，虚拟机则是虚拟化硬件，因此容器更具有便携性、高效地利用服务器。 容器更多的用于表示 软件的一个标准化单元。由于容器的标准化，因此它可以无视基础设施（Infrastructure）的差异，部署到任何一个地方。另外，Docker也为容器提供更强的业界的隔离兼容。[2] Docker 利用Linux核心中的资源分离机制，例如cgroups，以及Linux核心名字空间（namespaces），来创建独立的容器（containers）。这可以在单一Linux实体下运作，避免引导一个虚拟机造成的额外负担[3]。Linux核心对名字空间的支持完全隔离了工作环境中应用程序的视野，包括行程树、网络、用户ID与挂载文件系统，而核心的cgroup提供资源隔离，包括CPU、存储器、block I/O与网络。从0.9版本起，Dockers在使用抽象虚拟是经由libvirt的LXC与systemd - nspawn提供界面的基础上，开始包括libcontainer库做为以自己的方式开始直接使用由Linux核心提供的虚拟化的设施。 – Wikipedia Docker安装与卸载Docker安装(CentOS)123456789101112131415# 因为连接的阿里云服务器，默认root用户，所以未加sudo$ yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine$ yum install -y yum-utils$ yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo$ yum install docker-ce docker-ce-cli containerd.io$ systemctl start docker # 开机自启 Docker卸载12$ yum remove docker-ce docker-ce-cli containerd.io$ rm -rf /var/lib/docker Docker镜像加速阿里云镜像加速-控制台-容器镜像服务-镜像加速器 Docker常用命令帮助命令123$ docker version$ docker --help # 查看帮助命令 如果对某个命令不熟悉 可以 docker [command] --help# 比如 docker run --help 镜像命令 docker pull的时候具体发生了什么？ 1$ docker images [OPTIONS] [REPOSITORY[:TAG]] Command Info docker images 查看所有镜像 docker images java[:tag] 列出与java有关的镜像 docker images --digests 列出摘要信息 docker images --filter 按给定条件过滤，label,before,since,dangling,reference 1$ docker rmi [OPTIONS] IMAGE [IMAGE...] 删除镜像，同时删除多个镜像时用空格分隔 Command Info docker rmi -f nginx 强制删除镜像 docker rmi -f $(docker images -a -q) 删除所有镜像 docker rmi $(docker images -f &quot;dangling=true&quot; -q) 组合过滤删除 容器相关命令 Command Info docker container ls 列出容器 docker stop/start xxxx 停止/启动容器 docker rm xx [yy zz] 删除一个或多个容器 docker rm -f $(docker ps -a) 强制删除所有容器 docker container stats -a 查看容器运行信息 docker exec -it nginx /bin/bash 以交互模式启动容器 12# Run a command in a new container 启动一个新的容器$ docker run [OPTIONS] IMAGE [COMMAND] [ARG...] 1234567891011# -p 端口映射 外部主机端口:容器端口 -P 随机暴露端口# --name 运行的别名 可以自选# -v || --volume 挂载卷 外部主机目录:容器内目录 用于数据持久化与数据同步 容器内外的文件操作将会同步# -e || --env 环境变量# -d || --detach 脱机模式运行$ docker run -p 3306:3306 --name mysql \\-v /mydata/mysql/log:/var/log/mysql \\-v /mydata/mysql/data:/var/lib/mysql \\-v /mydata/mysql/conf:/etc/mysql \\-e MYSQL_ROOT_PASSWORD=root \\-d mysql:5.7 12# 把本地的标准输入输出与正在运行的容器进行绑定$ docker attach [OPTIONS] CONTAINER 其他常用命令12345# 返回 Docker 对象的详细信息$ docker inspect [OPTIONS] NAME|ID [NAME|ID...]# -a || --author, -m || message 将镜像提交到本地 与git commit 类似# 根据容器的变动创建一个新的镜像$ docker commit -a=\"raymond\" -m=\"commit msg\" 容器ID tomcat:1.0 Docker镜像讲解镜像是什么？镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的所有内容，包括代码、运行时、库、环境变量和配置文件 Docker镜像加载原理UnionFS(联合文件系统)UnionFS（联合文件系统）：Union文件系统是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下，Union文件系统是Dokcer镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的镜像。 特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统加载起来，这样最终的文件系统会包含所有的底层文件和目录。 Docker的存储驱动的实现是基于 Union File System，简称UnionFS，它是一种为Linux 、FreeBSD 和NetBSD 操作系统设计的，把其他文件系统联合到一个联合挂载点的文件系统服务。它用到了一个重要的资源管理技术,叫写时复制。写时复制（copy-on-write），也叫隐式共享，是一种对可修改资源实现高效复制的资源管理技术。对于一个重复资源，若不修改，则无需立刻创建一个新的资源，该资源可以被共享使用。当发生修改的时候，才会创建新资源。这会大大减少对于未修改资源复制的消耗。 镜像加载原理UnionFS文件系统是Docker镜像的基础，镜像可以通过分层来进行继承，基于Base镜像（没有父镜像），可以制作各种具体的应用镜像。Docker的存储驱动有Overlay/Overlay2, AUFS. bootfs: 在Docker镜像中最底层是bootfs(boot file sysem)文件系统，bootfs主要包含bootloader和kernel。Linux刚启动时会加载bootfs文件系统，bootloader主要作用是引导加载kernel。在Docker镜像中，bootfs这一层与典型的Linux/Unix系统是一样的，包含bootloader和kernel。当bootloader加载完成之后整个内核都存放在内存中，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载bootfs。 rootfs: Docker镜像中用户空间的文件系统是rootfs，包含/dev、/proc、/bin 等目录。对于Base Image来说，底层直接用Host的kernel，自己只需要提供rootfs。而对于一个精简的OS，rootfs的体积可以很小，只需要包含最基本的命令、工具和程序库就可以。 不同Linux发行版的主要区别就是rootfs。比如Ubuntu14.04使用upstart管理服务，apt管理软件包；而CentOS7使用systemd和yum。这些都是用户空间上的区别，Linux kernel差别不大。因此Docker可以同时支持多种发行版的Linux镜像，模拟出多种操作系统环境。 容器只能使用Host的kernel，并且不能修改。所有容器都共用host的kernel，在容器中没办法对kernel升级。如果容器对kernel版本有要求（比如应用只能在某个kernel版本下运行），则不建议用容器，这种场景虚拟机可能更合适。 容器的可写层当容器启动时，一个新的可写层被加载到镜像的顶部。这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。所有对容器的改动，无论添加、删除、还是修改文件都只会发生在容器层中。即只有容器层是可写的，容器层下面的所有镜像层都是只读的。其中镜像层数量可能会很多，所有镜像层会联合在一起组成一个统一的文件系统（UnionFS）。如果不同镜像层中有一个相同路径的文件，比如/a，上层的/a会覆盖下层的/a，也就是说用户只能访问到最上层中的文件/a。在容器层中，用户看到的是一个叠加之后的文件系统。下图是容器可写层的图解。 容器可写层的操作 添加文件，在容器中创建文件时，新文件被添加到容器层中。 读取文件，在容器中读取某个文件时，Docker 会从上往下依次在各镜像层中查找此文件。一旦找到，打开并读入内存。 修改文件，在容器中修改已存在的文件时，Docker 会从上往下依次在各镜像层中查找此文件。一旦找到，立即将其复制到容器层，然后再修改。 删除文件，在容器中删除文件时，Docker 也是从上往下依次在镜像层中查找此文件。找到后，会在容器层中记录下此删除操作。 上面的操作中，只有当需要修改时才复制一份数据，这种特性被称作写时复制（copy-on-write）。可见容器层保存的是镜像变化的部分，不会对镜像本身进行任何修改。容器层记录对镜像的修改，所有镜像层都是只读的，不会被容器修改，所以镜像可以被多个容器共享。 容器数据卷为什么需要数据卷？数据卷是存在于一个或多个容器中的特定文件或文件夹，这个文件或文件夹以独立于 docker 文件系统的形式存在于宿主机中。数据卷的最大特定是：其生存周期独立于容器的生存周期。 在宿主机上不能很方便地访问容器中的文件 无法在多个容器之间共享数据 当容器删除时，容器中产生的数据将丢失。 使用数据卷的场景 在多个容器之间共享数据，多个容器可以同时以只读或者读写的方式挂载同一个数据卷，从而共享数据卷中的数据。 当宿主机不能保证一定存在某个目录或一些固定路径的文件时，使用数据卷可以规避这种限制带来的问题。 当你想把容器中的数据存储在宿主机之外的地方时，比如远程主机上或云存储上。 当你需要把容器数据在不同的宿主机之间备份、恢复或迁移时，数据卷是很好的选择。 使用数据卷12345$ docker run -it -v /data --name centos1 -d centos$ docker exec -it centos1 /bin/bash[root@a10ac03e69dd /]# ls # 可以看到创建的 /data目录# 同步卷$ docker run -it --volumes-from centos1 --name centos2 -d centos Docker中的所有卷，默认情况下都在 /var/lib/docker/volumes/ 12345678910111213141516$ docker volume create MyVolume$ docker inspect MyVolume➜ volumes docker volume --helpUsage: docker volume COMMANDManage volumesCommands: create Create a volume inspect Display detailed information on one or more volumes ls List volumes prune Remove all unused local volumes rm Remove one or more volumesRun 'docker volume COMMAND --help' for more information on a command. 安装MySQL并进行数据同步 MySQL的数据持久化问题 1234$ docker run -d -p 3307:3306 -v /home/mysql/conf:/etc/mysql/conf.d -v /home/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root --name mysql1 mysql:5.7$ docker exec -it mysql1 mysql -uroot -prootmysql&gt; create database test;# 外部卷将会出现新创建的数据库 具名挂载与匿名挂载12345678# 匿名挂载$ docker run -d --name nginx01 -v /etc/nginx nginx# 具名挂载$ docker run -d --name nginx02 -v juming-nginx:/etc/nginx nginx# -v 容器内路径 # 匿名挂载# -v 卷名:容器内路径 # 具名挂载# -v /宿主机路径:容器内路径 # 指定路径挂载 Dockerfile什么是Dockerfile？Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。 镜像构建以下为 CentOS的官方镜像的Dockerfile，可以看出CentOS是从scratch上添加了centos的压缩包，然后又添加了一些基础的metadata构成的。 123456789101112131415FROM scratchADD centos-7-x86_64-docker.tar.xz /LABEL \\ org.label-schema.schema-version=\"1.0\" \\ org.label-schema.name=\"CentOS Base Image\" \\ org.label-schema.vendor=\"CentOS\" \\ org.label-schema.license=\"GPLv2\" \\ org.label-schema.build-date=\"20200504\" \\ org.opencontainers.image.title=\"CentOS Base Image\" \\ org.opencontainers.image.vendor=\"CentOS\" \\ org.opencontainers.image.licenses=\"GPL-2.0-only\" \\ org.opencontainers.image.created=\"2020-05-04 00:00:00+01:00\"CMD [\"/bin/bash\"] Dockerfile构建命令 介绍 FROM 指定基础镜像，必是 Dockerfile 文件中的首条命令。 MAINTAINER 镜像的维护者，姓名+邮箱 RUN 在镜像的构建过程中执行特定的命令，并生成一个中间镜像。 COPY 复制文件 ADD 更高级的复制文件 ENV 设置环境变量 EXPOSE 为构建的镜像设置监听端口，使容器在运行时监听。 VOLUME 用于创建挂载点，即向基于所构建镜像创始的容器添加卷。 WORKDIR 在容器内设置一个工作目录，Dockerfile 中其后的命令 RUN、CMD、ENTRYPOINT、ADD、COPY 等命令都会在该目录下执行。 CMD 用于指定在容器启动时所要执行的命令。RUN 在构建的时候执行，并生成一个新的镜像，CMD 在容器运行的时候执行，在构建时不进行任何操作。 ENTRYPOINT 用于给容器配置一个可执行程序。docker run运行容器时指定的参数都会被传递给 ENTRYPOINT ，且会覆盖 CMD 命令指定的参数。 LABEL 为镜像添加元数据，元数以键值对的形式指定。 ARG 用于指定传递给构建运行时的变量 ONBUILD 用于设置镜像触发器 STOPSIGNAL 用于设置停止容器所要发送的系统调用信号 SHELL 用于设置执行命令（shell式）所使用的的默认 shell 类型 详细介绍 纯洁的微笑 实战1-构建CentOS1$ vim Dockerfile 123456789101112FROM centosMAINTAINER raymond&lt;740567396@qq.com&gt;ENV MYPATH /usr/localWORKDIR $MYPATHRUN yum -y install vimRUN yum -y install net-toolsCMD echo $MYPATHCMD echo \"--end--\"CMD [\"/bin/bash\"] 1234567891011# 如果文件名就是 Dockerfile 的话可以不加 -f Dockerfile 否则 -f 文件名# 最后有个 . 不可以少$ docker build -f Dockerfile -t mycentos:0.1 .$ docker login# ----Username And Password-----# 在此要使用 docker tag 将本地镜像名 tag 上 dockerhub 用户名# 比如我本地的是 mycentos1 我的dockerhub用户名是 raymondzhaodocker# 那么 tag 成$ docker tag mycentos:0.1 raymondzhaodocker/mycentos:0.1# 否则 denied: requested access to the resource is denied$ docker push raymondzhaodocker/mycentos 实战2-打包Spring Boot12345FROM java:8COPY target/*.jar /app.jarCMD [\"--server.port=8080\"]EXPOSE 8080ENTRYPOINT [\"java\", \"-jar\", \"/app.jar\"] 小结 Docker网络12345$ docker network ls [OPTIONS]NETWORK ID NAME DRIVER SCOPEe14287f9fd31 bridge bridge local642edd192245 host host local2272acc1f6df none null local Docker四种网络模式 bridge：桥接式 host：开放式 none: 封闭式 Container(join) ： 联合挂载式网络模式，是host网络模式的延伸 网络模式桥接模式 Docker 的默认网络方式 12345678910111213$ ip addr # centos 命令1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:16:3e:0c:56:96 brd ff:ff:ff:ff:ff:ff inet 172.16.139.14/20 brd 172.16.143.255 scope global dynamic eth0 valid_lft 315321918sec preferred_lft 315321918sec3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:69:a1:6d:e0 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 可以看到三个网卡lo,eth0,docker0，分别为本地环路，ethernet0(以太网)，以及Docker进程启动时创建的docker0. 当Docker进程启动时，会在主机上创建一个名为docker0的虚拟网桥，此主机上启动的Docker容器会连接到这个虚拟网桥上，所以有默认地址172.17.0.0/16的地址。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。 从docker0子网中分配一个IP给容器使用，并设置docker0的IP地址为容器的默认网关。在主机上创建一对虚拟网卡veth pair设备，Docker将veth pair设备的一端放在新创建的容器中，并命名为eth0（容器的网卡），另一端放在主机中，以vethxxx这样类似的名字命名，并将这个网络设备加入到docker0网桥中。 123456$ docker inspect e14287f9fd31 # 这个是 NETWORK_ID 本节开始前查询出的桥接模式的 id# 启动一个Nginx容器之后 会发现下面的文件中 Containers 中多一条 nginx的数据$ docker --name nginx -p 80:80 -d nginx# 如果使用 ip addr 的话会发现多出一个 ip addr 此处名字 veth5a0f960@if3637: veth5a0f960@if36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 82:52:b0:fe:16:07 brd ff:ff:ff:ff:ff:ff link-netnsid 0 123456789101112131415161718192021222324252627282930313233343536373839404142434445[ &#123; \"Name\": \"bridge\", \"Id\": \"e14287f9fd319f2e57dfb2a3f07cfaff266be66edd66fe68848d996a0ebe3710\", \"Created\": \"2020-06-08T09:51:00.898646131+08:00\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": &#123; \"Driver\": \"default\", \"Options\": null, \"Config\": [ &#123; \"Subnet\": \"172.17.0.0/16\", \"Gateway\": \"172.17.0.1\" &#125; ] &#125;, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": &#123; \"Network\": \"\" &#125;, \"ConfigOnly\": false, \"Containers\": &#123; \"43ee792642bfce2e349eb463db14412476b69ac7066e367eeae4cc13aa1289b9\": &#123; \"Name\": \"nginx\", \"EndpointID\": \"3c0badf28429f752f17ac4c2eeb5a94d0f911fcac577557dbebfb77f270e86e4\", \"MacAddress\": \"02:42:ac:11:00:02\", \"IPv4Address\": \"172.17.0.2/16\", \"IPv6Address\": \"\" &#125; &#125;, \"Options\": &#123; \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" &#125;, \"Labels\": &#123;&#125; &#125;] Host模式如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。 Container模式这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lO 网卡设备通信。 –link自定义网络12345$ docker network create mynet$ docker inspect mynet # 查看自建的网络信息# 在自建网络上启动容器$ docker run -it --name nginx1 -p 81:81 --net mynet -d nginx$ docker inspect mynet # 再查看自建的网络信息看看有什么变化？ 网络连通还记得之前在默认bridge下创建的容器nginx吗？刚刚在自定义网络mynet中又新建了一个nginx1，如果想让nginx和nginx1在同一个网络中该怎么办？ --link似乎可以解决掉，但是有更好的方式。 12345# docker network connect [OPTIONS] NETWORK CONTAINER$ docker network connect mynet nginx# 再查看一下 mynet 就可以发现 Containers 中不止一个 nginx1 了# 此时 nginx 一个容器，两个 ip$ docker inspect mynet 实战：Redis集群启动六个 redis 容器123456789101112131415161718192021# 创建网卡 子网地址可能会有变化 在这个子网下可以容纳 255*255 - 2 个容器$ docker network create redis-net --subnet 172.16.0.0/16for port in $(seq 1 6); \\do \\mkdir -p /mydata/redis/node-$&#123;port&#125;/conftouch /mydata/redis/node-$&#123;port&#125;/conf/redis.confcat &lt;&lt; EOF &gt;/mydata/redis/node-$&#123;port&#125;/conf/redis.confport 6379cluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000cluster-announce-ip 172.16.0.1$&#123;port&#125;cluster-announce-port 6379cluster-announce-bus-port 16379appendonly yesEOFdocker run -p 637$&#123;port&#125;:6379 -p 1637$&#123;port&#125;:16379 --name redis-$&#123;port&#125; \\-v /mydata/redis/node-$&#123;port&#125;/data:/data \\-v /mydata/redis/node-$&#123;port&#125;/conf/redis.conf:/etc/redis/redis.conf \\--net redis-net --ip 172.16.0.1$&#123;port&#125; -d redis redis-server /etc/redis/redis.conf \\done 创建三主三从集群123456789101112131415161718192021222324252627282930313233343536373839404142# 随便进入一个redis 注意 redis 镜像里没有 /bin/bash 只有 /bin/sh$ docker exec -it redis-1 /bin/sh$ redis-cli --cluster create 172.16.0.11:6379 172.16.0.12:6379 172.16.0.13:6379 172.16.0.14:6379 172.16.0.15:6379 172.16.0.16:6379 --cluster-replicas 1# --cluster-replicas 1 的意思是复制一份，6个容器，又需要复制，所以自然是三主三从了。# 可以看到是以 Slot 插槽方式创建的三个主结点&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica 172.16.0.15:6379 to 172.16.0.11:6379Adding replica 172.16.0.16:6379 to 172.16.0.12:6379Adding replica 172.16.0.14:6379 to 172.16.0.13:6379M: 4eede4f17800f1896e59658517a7aa85e064374c 172.16.0.11:6379 slots:[0-5460] (5461 slots) masterM: aa60f670a81aa2bcc03457a4d1e8650ee6817c81 172.16.0.12:6379 slots:[5461-10922] (5462 slots) masterM: 59444f77747d6c0f6cdccfeb9ee29bdd09c9de2e 172.16.0.13:6379 slots:[10923-16383] (5461 slots) masterS: 510b0b6f666382c74f3bca61e4018563100f2758 172.16.0.14:6379 replicates 59444f77747d6c0f6cdccfeb9ee29bdd09c9de2eS: 6e3bdb3e58afd0a23a5acfa30fbc0f82ca15896f 172.16.0.15:6379 replicates 4eede4f17800f1896e59658517a7aa85e064374cS: 2b59b296014cedfdc6dc12a2e62a70f1ca5f57ee 172.16.0.16:6379 replicates aa60f670a81aa2bcc03457a4d1e8650ee6817c81Can I set the above configuration? (type 'yes' to accept): yes# 可以看出 172.16.0.15 是 172.16.0.11 的 slave# 一定要加 -c 否则是单机# 随便以交互模式进入一个 redis 容器， redis的 /bin/ 下没有bash，只有 sh$ docker exec it redis1 /bin/sh$ redis-cli -c$ cluster info # 查看集群信息$ cluster nodes # 查看结点信息$ exit; # 退出 redis# 模拟宕机$ docker stop redis-1# 此时以交互模式启动 redis-5 查看结点信息 可以看到 redis-1 已宕机# 但是 get hello 是正常的，因为 slave 顶替了它的 master 的角色$ get hello# 如果此时 redis-1 恢复正常 重新上线 则会与 redis-5 的角色互换 Docker ComposeDocker Swarm","categories":[{"name":"Docker","slug":"Docker","permalink":"http://raymond-zhao.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://raymond-zhao.top/tags/Docker/"}]},{"title":"剑指 Offer 题解","slug":"2020-05-10-Algo-SwordToOffer","date":"2020-05-10T05:18:55.000Z","updated":"2022-02-07T09:29:02.000Z","comments":true,"path":"2020/05/10/2020-05-10-Algo-SwordToOffer/","link":"","permalink":"http://raymond-zhao.top/2020/05/10/2020-05-10-Algo-SwordToOffer/","excerpt":"","text":"前言 本文档中所有内容均采用 CC BY-NC 4.0 许可协议。 你可以复制共享、演绎创作，但不得用于商业目的。 本文档起源于本人于 $2020$ 年 $4$ 月起在 LeetCode 中《剑指 Offer》 所做的笔记。 看别人题解时的状态：一看就会，一听就懂，一做就错。 于是决定采用边阅读，边手敲题解，就算抄我也要抄一遍的方式进行学习，使用这种方法确实有一定作用。在手敲的过程中往往会有一些独特的体会以及更深刻的记忆，有时候自己也补上几句，久而久之，断断续续过去了三四个月，随着秋招的全面开启，这份笔记也越来越长，逐渐变成了现在这个样子。 $NOTE$: 本文档仅用于个人学习目的，旨在为准备校招的同学们以及其他对算法感兴趣的朋友提供一份便利，同时为了宣传 LeetCode-Krahets 的智慧结晶。 因为，这么精致的内容不让更多人看到实在是太说不过去了！ 喜欢网页版的同学建议前往大佬的博客，其中有许多更形象生动的 $GIF$ 图。 喜欢 $PDF$ 的同学自然推荐这个。 我不是知识的生产者，只是知识的搬运工。 写于 $2020$ 年 $8$ 月 $9$ 日 $22$ 时 $35$ 分。 致谢 Credits are Krahets ‘s! 此中题解大部分出自 $LeetCode-Krahets$，大佬博客 $Krahets’s Blog$。 第一次看到大佬的题解时就有一种赏心悦目的感觉，排版精良，一丝不苟，哪怕只是一个再寻常不过的数字 $0$ 都会用 $LaTex$ 所修饰。毕竟，态度决定高度，细节决定成败。 本文中仍然存在很多问题，包括但不限于排版风格尚未完全统一、中英文标点符号不恰当、汉字错误、部分数学内容没有被 $LaTex$ 所渲染，以后将会逐渐完善。 可以说，如果不是这位大佬严谨细致题解，我很难坚持把 $LeetCode$ 上《剑指 $Offer$》系列题目看一遍。每当有题目没有头绪时，习惯性地在题解中寻找大佬的头像，阅读完后总有一种醍醐灌顶的感觉，感慨：山重水复疑无路，柳暗花明又一村。 题目来源 LeetCode中国 - 《剑指 Offer》 系列 牛客网 - 《剑指 Offer》 系列 其他参考 CS-Notes - 剑指Offer题解 《剑指Offer：名企面试官精讲典型编程题》，何海涛。 Tips链表结点定义 123456789/*** 单链表结点定义* 如不特殊说明，后文 链表结点 均采用此结构。*/public class ListNode &#123; private int val; private ListNode next; public ListNode(int val) &#123; this.val = val; &#125;&#125; 二叉树节点定义 12345678910/*** 二叉树节点定义* 如不特殊说明，后文 树节点 均采用此结构。*/public class TreeNode &#123; private int val; private TreeNode left; private TreeNode right; public TreeNode(int val) &#123; this.val = val; &#125;&#125; 名词约定 链表中的 $Node$ 中文为 结点； 树中的 $TreeNode$ 中文为 节点。 经验 很多考研的同学都知道下面这个口诀$$🐶-sin 🐶 \\approx \\frac{1}{6}🐶^3 ,\\quad 🐶 \\rightarrow 0$$算法里针对某类特定的题也应该有类似的口诀， 比如： 看到有序数组的问题首先想到二分法； 二叉搜索树中：二叉搜索树的中序遍历结果时递增序列； 关于搜索问题必须想到 $DFS$、$BFS$； 想到以后再添加$\\dots$ 3. 数组中重复的数字题目描述在一个长度为 $n$ 的数组里的所有数字都在 $0$ 到 $n-1$ 的范围内。数组中某些数字是重复的，但不知道有几个数字是重复的，也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 1234567Input:&#123;2, 3, 1, 0, 2, 5&#125;Output:22, 3, 1, 0, 2, 5 Solution 1 - 计数采用“计数”的思想，申请一个长度为 $n$ 的数组 validation (当然也可以用 map or set ，但是数组效率更高一些)，刚申请时每个元素都为 $0$，此时遍历一遍原数组，将原数组 nums 中的元素作为 validation 中的下标，并将 validation 中的元素加 $1$ ，表示元素出现了一次。在将原数组扫描一遍后，validation 中记录了每个元素出现的次数，再扫描一遍 validation，凡是 $\\ge 1$ 的元素均为重复元素，返回第一个 $\\ge 1$ 的元素即可。 12345678910public int findRepeatNumber(int[] nums) &#123; if (nums == null || nums.length == 0) return -1; int[] count = new int[nums.length]; for (int i = 0; i &lt; nums.length; i++) count[nums[i]]++; for (int i = 0; i &lt; count.length; i++) if (count[i] &gt; 1) return i; return -1;&#125; 时间复杂度: $O(n)$。 空间复杂度: $O(n)$。 Solution 2 - 遍历一次对于这种数组元素在$ [0, n-1]$ 范围内的问题，可以将值为 $i$ 的元素调整到第 $i$ 个位置上进行求解。本题要求找出重复的数字，因此在调整过程中，如果第 $i$ 位置上已经有一个值为 $i$ 的元素，就可以知道 $i$ 值重复。 12345678910111213141516171819202122public int findRepeatNumber(int[] nums) &#123; if (nums == null || nums.length == 0) return -1; for (int i = 0; i &lt; nums.length; i++) &#123; // 如果当前索引处的数值不等于索引 // 将当前索引处的值调整到 i = nums[i] 或者找到重复值为止 while (i != nums[i]) &#123; // 如果相等说明已出现了重复 if (nums[i] == nums[nums[i]]) return nums[i]; else swap(nums, i, nums[i]); &#125; &#125; return -1;&#125;private void swap(int[] nums, int i, int j) &#123; int tmp = nums[i]; nums[i] = nums[j]; nums[j] = tmp;&#125; 时间复杂度: $O(n)$。 空间复杂度: $O(1)$。 4. 二维数组中的查找题目描述在一个 $m\\times n$ 的二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 1234567891011Consider the following matrix:[ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30]]Given target = 5, return true.Given target = 20, return false. Solution - 逼近利用二维数组每一行从左到右递增，每一列从上到下递增的特点，可以从数组右上角开始： 如果扫描到的元素比 target 小，则在当前列从上到下搜索； 如果扫描到的元素比 target 大，则在当前行从右向左搜索。 1234567891011121314public boolean findNumberIn2DArray(int target, int [][] matrix) &#123; if (matrix == null || matrix.length == 0 || matrix[0].length == 0) return false; int rows = matrix.length, cols = matrix[0].length; int row = 0, col = cols - 1; while (row &lt; rows &amp;&amp; col &gt;= 0) &#123; // 从右上角开始寻找 if (target == matrix[row][col]) return true; else if (target &gt; matrix[row][col]) row++; // 向下寻找 else col--; &#125; return false;&#125; 时间复杂度: $O(rows+cols)$。 空间复杂度: $O(1)$。 5. 替换空格题目描述将一个字符串中的空格替换成 &quot;%20&quot;。 12345Input:\"A B\"Output:\"A%20B\" Solution 1 - “倒排索引”① 在字符串尾部填充任意字符，使得字符串的长度等于替换之后的长度。因为一个空格要替换成三个字符（%20），所以当遍历到一个空格时，需要在尾部填充两个任意字符。 ② 令 $P1$ 指向字符串原来的末尾位置，$P2$ 指向字符串现在的末尾位置。$P1$ 和 $P2$ 从后向前遍历，当 $P1$ 遍历到一个空格时，就需要令 $P2$ 指向的位置依次填充 02%（注意是逆序的），否则就填充上 $P1$ 指向字符的值。从后向前遍历是为了在改变 $P2$ 所指向的内容时，不会影响到 $P1$ 遍历原来字符串的内容。 ③ 当 $P2$ 遇到 $P1$ 时（$P2 \\le P1$），或者遍历结束（$P1 &lt; 0$），退出。 1234567891011121314151617181920212223public String replaceSpace(String s) &#123; if (s == null || s.length() == 0) return \"\"; StringBuilder builder = new StringBuilder(s); int leftIndex = s.length() - 1; for (int i = 0; i &lt;= leftIndex; i++) // 注意这里是 &lt;= 而不是 &lt; if (builder.charAt(i) == ' ') builder.append(\" \"); // 下面需要的是索引，索引一定要考虑可能会越界的情况 // 比如, 左右索引都是 length() - 1, 而不是 length(). int rightIndex = builder.length() - 1; while (leftIndex &gt;= 0 &amp;&amp; rightIndex &gt; leftIndex) &#123; char c = builder.charAt(leftIndex--); if (c == ' ') &#123; builder.setCharAt(rightIndex--, '0'); builder.setCharAt(rightIndex--, '2'); builder.setCharAt(rightIndex--, '%'); &#125; else &#123; builder.setCharAt(rightIndex--, c); &#125; &#125; return builder.toString();&#125; 时间复杂度: $O(n)$。 空间复杂度: $O(n)$。 Solution 2 - 遍历添加123456789101112public String replaceSpace(String s) &#123; return s.replace(\" \", \"%20\");&#125;public String replaceSpace(String s) &#123; StringBuilder res = new StringBuilder(); for (Character c : s.toCharArray()) &#123; if (c == ' ') res.append(\"%20\"); else res.append(c); &#125; return res.toString();&#125; 时间复杂度: $O(n)$。 空间复杂度: $O(n)$。 6. 从尾到头打印链表题目描述输入一个链表的头节点，从尾到头反过来返回每个节点的值（用数组返回）。 Solution 1 - 栈利用“栈”先进后出的特点，可以实现逆序的效果。 在 $JDK$ 的使用文档里，对于 java.util.Stack 有这样一句话： “A more complete and consistent set of LIFO stack operations is provided by the Deque interface and its implementations, which should be used in preference to this class. ” For example: 1Deque&lt;Integer&gt; stack = new ArrayDeque&lt;Integer&gt;(); 意思就是推荐使用 $Deque$ 来作为 $Stack$ 的替代品，用 $ArrayDeque$ 作为其实现类，也可以使用 $LinkedList$，具体问题具体分析，看是增删多，还是查询多。 1234567891011121314public int[] reversePrint(ListNode head) &#123; if (head == null) return new int[0]; Deque&lt;Integer&gt; stack = new ArrayDeque&lt;&gt;(); while (head != null) &#123; stack.push(head.val); head = head.next; &#125; int index = 0; int[] res = new int[stack.size()]; while (!stack.isEmpty()) res[index++] = stack.pop(); return res;&#125; 时间复杂度: $O(n)$。 空间复杂度: $O(n)$。 Solution 2 - 递归要逆序打印链表 1-&gt;2-&gt;3（3,2,1)，可以先逆序打印链表 2-&gt;3(3,2)，最后再打印第一个结点 1。而链表 2-&gt;3 可以看成一个新的链表，要逆序打印该链表可以继续使用求解函数，也就是在求解函数中调用自己，这就是递归函数。 12345678910111213141516171819public class Solution &#123; private List&lt;Integer&gt; tmp = new ArrayList&lt;&gt;(); public int[] reversePrint(ListNode head) &#123; recur(head); int[] res = new int[tmp.size()]; for(int i = 0; i &lt; res.length; i++) res[i] = tmp.get(i); return res; &#125; public void recur(ListNode head) &#123; if(head == null) return; recur(head.next); tmp.add(head.val); &#125;&#125; 时间复杂度: $O(n)$。 空间复杂度: $O(n)$。 Solution 3 - 头插法遍历原链表，将原链表中的每个元素以头插法的方式插入到一个辅助链表中，此时辅助链表中的元素恰好为原链表中的元素逆序后的结果。再遍历辅助链表存储每个元素即可。 在此过程中需要维护一个头结点，维护待插入结点的下一个结点，如果不维护下一个结点的话，将会丢失下一个要插入元素。 123456789101112131415161718192021222324/*** 效率更差，只是为了回忆 头插倒序 这个知识点。*/public int[] reversePrint(ListNode head) &#123; ListNode tmpHead = new ListNode(-1); while (head != null) &#123; // 保存下一个结点，否则当前结点插入后便找不到下一个开始的目标 ListNode nextNode = head.next; head.next = tmpHead.next; tmpHead.next = head; head = nextNode; &#125; List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); tmpHead = tmpHead.next; while (tmpHead != null) &#123; // 当还未遍历到尾部时 result.add(tmpHead.val); tmpHead = tmpHead.next; &#125; int[] res = new int[result.size()]; int index = 0; for (int num : result) res[index++] = num; return res;&#125; 时间复杂度: $O(n)$。 空间复杂度: $O(n)$。 Solution 4 - 两次遍历123456789101112131415161718public int[] reversePrint(ListNode head) &#123; // 先获取链表长度，创建对应长度数组 ListNode currNode = head; int len = 0; while(currNode != null)&#123; len++; currNode = currNode.next; &#125; int[] result = new int[len]; // 再次遍历链表，将值倒序填充至结果数组 currNode = head; while(currNode != null)&#123; result[len - 1] = currNode.val; len--; currNode = currNode.next; &#125; return result;&#125; 时间复杂度: $O(n)$。 空间复杂度: $O(n)$。 7(🌲) . 重建二叉树题目描述输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列 ${1,2,4,7,3,5,6,8}$ 和中序遍历序列 ${4,7,2,1,5,3,8,6}$ ，则重建二叉树并返回。 Solution - 递归 前序遍历：根、左、右 中序遍历：左、根、右 根据前序遍历与中序遍历可以确定一棵二叉树，有以下几个要点(假设序列如题目描述中)： 前序遍历的首个结点为当前子树的根节点 $root$，即 $1$ 在中序遍历中找到当前子树的根节点，那么处于根节点左边的则为左子树中的结点，处于根节点右边的则为左、右子树中的节点。将中序遍历划分为 [left | root | right]，即[4,7,2 | 1 | 5, 3, 8, 6] 根据中序遍历[left | root | right]中的左右子树中节点的数量，再将前序遍历划分为[root | left | right]，即[1 | 2, 4, 7 | 3, 5, 6, 8] 经过以上三步后可以确定三个结点，即 根节点root(1) 左子树根节点root.left(2) 右子树根节点root.right(3) 递归解析 递推参数： 前序遍历中根节点的索引preRoot、中序遍历左边界inLeft、中序遍历右边界inRight 终止条件： 当 inLeft &gt; inRight，子树中序遍历为空，说明已经越过叶子节点，此时返回 null。 递推工作： 建立根节点root： 值为前序遍历中索引为preRoot的节点值。 搜索根节点root在中序遍历的索引i： 为了提升搜索效率，使用哈希表 dic 预存储中序遍历的值与索引的映射关系，每次搜索的时间复杂度为 $O(1)$； 构建根节点root的左子树和右子树： 通过调用 recur() 方法开启下一层递归。 左子树： 根节点索引为 preRoot + 1 ，中序遍历的左子树的左右边界分别为 inLeft 和i - 1； 右子树： 根节点索引为 i - inLeft + preRoot + 1（即：根节点索引 $+$ 左子树长度 $ + 1$），中序遍历的右子树的左右边界分别为i + 1和inRight。 返回值： 返回 root，含义是当前递归层级建立的根节点 root 为上一层递归的根节点的左或右子节点。 1234567891011121314151617181920212223242526272829class Solution &#123; private Map&lt;Integer, Integer&gt; dic = new HashMap&lt;&gt;(); private int[] po; // pre order public TreeNode buildTree(int[] preorder, int[] inorder) &#123; po = preorder; for (int i = 0; i &lt; inorder.length; i++) dic.put(inorder[i], i); // 存储中序遍历中的结点及其索引 return recur(0, 0, inorder.length - 1); &#125; private TreeNode recur(int preRoot, int inLeft, int inRight) &#123; // 如果左索引大于了右索引，说明已经到头了。 if (inLeft &gt; inRight) return null; // 1. 构造子树根结点 TreeNode root = new TreeNode(po[preRoot]); // 获取要构造的子树根节点在中序遍历中的位置 // 然后它左边的都是左子树中的结点 // 右边的都是右子树中的结点。 int i = dic.get(po[preRoot]); // 2. 构造子树根节点的左子结点 root.left = recur(preRoot + 1, inLeft, i - 1); // 3. 构造子树根节点的右子结点 root.right = recur(preRoot + i - inLeft + 1, i + 1, inRight); return root; &#125;&#125; 时间复杂度：$O(n)$，遍历中序序列 $O(n)$，递归共建立 $n$ 个节点，每层递归中的节点建立、搜索操作占用 $O(1)$，因此递归占用 $O(n)$。 空间复杂度：$O(n)$，$HashMap$ 使用 $O(n)$；递归操作递归栈使用 $O(n)$。 8(🌲). 二叉树的下一个结点题目描述给定一个二叉树和其中的一个节点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的节点不仅包含左右子节点，同时包含指向父节点的指针。节点结构如下： 123456789101112public class TreeLinkNode &#123; private int val; private TreeLinkNode left; private TreeLinkNode right; private TreeLinkNode next; // 指向父结点的指针 TreeLinkNode(int val) &#123; this.val = val; &#125;&#125; Solution中序遍历的顺序是左、根、右，所以中序遍历中的第一个节点位于二叉树中最左侧。在中序遍历的过程中： 如果一个节点 $root$ 的右子树非空，那么它的下一节点是右子树中的最左节点； 否则，向上回溯寻找第一个左指针指向 $root$ 的祖先节点。 123456789101112131415161718public TreeLinkNode GetNext(TreeLinkNode pNode) &#123; if (pNode.right != null) &#123; TreeLinkNode node = pNode.right; // 寻找当前节点右子树的最左节点，因为遍历顺序左、根、右 while (node.left != null) node = node.left; return node; &#125; else &#123; // 如果右节点为空，则向上寻找第一个左指针指向 root 的祖先节点 while (pNode.next != null) &#123; TreeLinkNode parent = pNode.next; if (parent.left == pNode) return parent; pNode = pNode.next; &#125; &#125; return null;&#125; 时间复杂度: $O(n)$。 空间复杂度: $O(1)$。 9. 用两个栈实现队列题目描述用两个栈实现一个队列。队列的声明如下，请实现它的两个函数 appendTail 和 deleteHead ，分别完成在队列尾部插入整数和在队列头部删除整数的功能(若队列中没有元素，deleteHead 操作返回 -1)。 12345输入：[\"CQueue\",\"appendTail\",\"deleteHead\",\"deleteHead\"][[],[3],[],[]]输出：[null,null,3,-1] Solution - 辅助栈队列为先进先出，栈为先进后出，那么利用两个栈，一个作为入栈 stack1，一个作为出栈 stack2，便可以实现队列的先进先出功能。 入队操作：将元素压入 stack1 中。 出队操作：首先将 stack1 中的元素弹出并且压入到 stack2 中，再将 stack2 中的元素弹出并压入到 stack1 中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/*** 使用 数组 实现栈的功能* 54ms, 86.71%*/class CQueue &#123; private Deque&lt;Integer&gt; stack1, stack2; public CQueue() &#123; stack1 = new ArrayDeque&lt;&gt;(); stack2 = new ArrayDeque&lt;&gt;(); &#125; public void appendTail(int value) &#123; stack1.push(value); &#125; public int deleteHead() &#123; // 1. 如果 stack2 非空，顶部本身就是整个头结点。 if (!stack2.isEmpty()) return stack2.pop(); // 2. 在 1 的基础上，stack1 也空，自然删除失败。 while (stack1.isEmpty()) return -1; // 3. 将 1 中的元素转到 2 中 while (!stack1.isEmpty()) stack2.push(stack1.pop()); return stack2.pop(); &#125;&#125;/*** 使用 链表 实现栈的功能* 可以看到，在这个过程中主要是插入删除操作* 而链表相对于数组更适合增删操作，所以本题效率也更高。* 52ms, 91.92%*/class CQueue &#123; private LinkedList&lt;Integer&gt; A, B; public CQueue() &#123; A = new LinkedList&lt;Integer&gt;(); B = new LinkedList&lt;Integer&gt;(); &#125; public void appendTail(int value) &#123; A.addLast(value); &#125; public int deleteHead() &#123; if(!B.isEmpty()) return B.removeLast(); if(A.isEmpty()) return -1; while(!A.isEmpty()) B.addLast(A.removeLast()); return B.removeLast(); &#125;&#125; 时间复杂度：$O(n)$，appendTail用时 $O(1)$, deleteHead用时 $O(n)$。 空间复杂度：$O(n)$，最坏情况下，$A、B$ 共保存 $n$ 个元素。 10 - I. 斐波那契数列 接下里的 10-II ~ 10-IV 都是斐波那契数列的衍生题。 题目描述写一个函数，输入 $n$ ，求斐波那契（$Fibonacci$）数列的第 $n$ 项。斐波那契数列的定义如下： 12F(0) = 0, F(1) = 1F(N) = F(N - 1) + F(N - 2), 其中 N &gt; 1. 斐波那契数列由 $0$ 和 $1$ 开始，之后的斐波那契数就是由之前的两数相加而得出。 答案需要取模 $1e9+7$（$1000000007$），如计算初始结果为：$1000000008$，请返回 $1$。 Solution 1 - 暴力法(自顶向下)12345// 暴力递归 不可取public int Fibonacci(int n) &#123; if (n &lt;= 1) return n; return Fibonacci(n - 1) + Fibonacci(n - 2);&#125; 时间复杂度：$O(2^n)$。 Solution 2 - 动态规划(自下而上)123456789// 动态规划 - 状态转移方程public int fib(int n) &#123; if (n &lt;= 1) return n; int[] dp = new int[n + 1]; dp[1] = 1; for (int i = 2; i &lt;= n; i++) dp[i] = (dp[i - 1] + dp[i - 2]) % 1000000007; return dp[n];&#125; 时间复杂度：$O(n)$。 空间复杂度：$O(n)$。 Solution 3 - 记忆化搜索1234567891011// 考虑到第 i 项只与第 i-1 和第 i-2 项有关// 因此只需要存储前两项的值就能求解第 i 项，从而将空间复杂度由 O(N) 降低为 O(1)。public int fib(int n) &#123; int pre1 = 0, pre2 = 1, sum; for(int i = 0; i &lt; n; i++)&#123; sum = (pre1 + pre2) % 1000000007; pre1 = pre2; pre2 = sum; &#125; return pre1;&#125; 时间复杂度：$O(n)$。 空间复杂度：$O(1)$。 Solution 4 - 快速幂根据斐波那契数列自身的性质，可以构造如下关系式$$\\begin{cases}F_2 = F_1 +F_0 \\F_1 = F_1\\end{cases}{\\tag 1}$$而表达式 $(1)$ 可以用矩阵表示为$$\\begin{bmatrix} F_2 \\ F_1 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 0 \\end{bmatrix} \\times \\begin{bmatrix} F_1 \\ F_0 \\end{bmatrix}$$ 那么$$\\begin{bmatrix} F_3 \\ F_2 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 0 \\end{bmatrix} \\times \\begin{bmatrix} F_2 \\ F_1 \\end{bmatrix}=\\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 0 \\end{bmatrix}^2 \\times \\begin{bmatrix} F_2 \\ F_1 \\end{bmatrix}$$递推下去可以得到$$\\begin{bmatrix} F_n \\ F_{n-1} \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 0 \\end{bmatrix} \\times \\begin{bmatrix} F_2 \\ F_1 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 0 \\end{bmatrix}^{n-1} \\times \\begin{bmatrix} F_1 \\ F_0 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 0 \\end{bmatrix}^{n-1} \\times \\begin{bmatrix} 1 \\ 0 \\end{bmatrix}$$Java实现矩阵比较麻烦，学有余力的同学可以看看。 时间复杂度：$O(\\lg n)$。 10 - II. 青蛙跳台阶问题题目描述一只青蛙一次可以跳上 $1$ 级台阶，也可以跳上 $2$ 级台阶。求该青蛙跳上一个 $n$ 级的台阶总共有多少种跳法。 答案需要取模 $1e9+7$（$1000000007$），如计算初始结果为：$1000000008$，请返回 $1$。 Solution - 记忆化搜索 真正的粉丝都是直奔 $Solution$ 🤓 此类求有多少可能性的题目一般都具有递推性质，即 $f(n)$ 依赖于 $f(1),f(2),\\dots,f(n-1)$。 假设青蛙跳上第 $n$ 级台阶有 $f(n)$ 种方式，那它可以从第 $n-1$ 级台阶跳一级台阶，也可以从第 $n-2$ 级台阶一次跳两级台阶。也就是：$$f(n)=f(n-1)+f(n-2)$$这个递推式是不是和斐波那契递推式有点相似？但是它们的初值呢？ 🐇 斐波那契： $f(0)=0,f(1)=f(2)=1$ 🐸 青蛙跳： $f(0)=1,f(1)=1,f(2)=2$ 123456789public int numWays(int n) &#123; int pre2 = 1, pre1 = 1, numWays; for (int i = 0; i &lt; n; i++) &#123; numWays = (pre2 + pre1) % 1000000007; pre2 = pre1; pre1 = numWays; &#125; return pre2;&#125; 时间复杂度：$O(n)$。 空间复杂度：$O(1)$。 10 - III 变态跳台阶题目描述一只青蛙一次可以跳上 $1$ 级台阶，也可以跳上 $2$ 级 $\\dots$ 它也可以跳上 $n$ 级。求该青蛙跳上一个 $n$ 级的台阶总共有多少种跳法。 Solution - 1: 数学推导跳上 $n-1$ 级台阶，可以从 $n-2$ 级跳 $1$ 级上去，也可以从 $n-3$ 级跳 $2$ 级上去… 那么$$f(n-1) = f(n-2) + f(n-3) + \\dots + f(0) {\\tag 1}$$同样，跳上 $n$ 级台阶，可以从 $n-1$ 级跳 $1$ 级上去，也可以从 $n-2$ 级跳 $2$ 级上去… ，那么$$f(n) = f(n-1) + f(n-2) + \\dots + f(0) {\\tag 2}$$由 $(2) - (1)$ 可得 $f(n)=2f(n-1)$。 所以，以 $1$ 为首项，$2$ 为公比的等比数列。 123public int JumpFloorII(int target) &#123; return (int) Math.pow(2, target - 1);&#125; 时间复杂度：$O(1)$。 空间复杂度：$O(1)$。 Solution 2 - 动态规划12345678public int JumpFloorII(int target) &#123; int[] dp = new int[target]; Arrays.fill(dp, 1); for (int i = 1; i &lt; target; i++) for (int j = 0; j &lt; i; j++) dp[i] += dp[j]; return dp[target - 1];&#125; 时间复杂度：$O(n^2)$。 空间复杂度：$O(n)$。 10 - IV 矩形覆盖可以用 $2\\times1$ 的小矩形横着或者竖着去覆盖更大的矩形。请问用 $n$ 个 $2\\times 1$ 的小矩形无重叠地覆盖一个 $2\\times n$ 的大矩形，总共有多少种方法？ Solution - 动态规划当 $n$ 为 $1$ 时，只有一种覆盖方法；当 $n$ 为 $2$ 时，有两种覆盖方法。 要覆盖 $2 \\times n$ 的矩形，可以先覆盖 $2 \\times 1$ ，再覆盖 $2 \\times (n-1)$ 的矩形，或者先覆盖 $2 \\times 2$ ，再覆盖 $2 \\times (n-2)$ 的矩形。而覆盖 $2\\times (n-1)$ 和 $2\\times (n-2)$ 的矩形可以看成是子问题。$$f(n)=\\begin{cases}1 &amp;, n=1 \\2 &amp;, n=2 \\f(n-1)+f(n-2) &amp;, n\\ge3\\end{cases}$$ 1234567891011public int RectCover(int target) &#123; if (target &lt;= 2) return target; int pre2 = 1, pre1 = 2; int result = 0; for (int i = 3; i &lt;= target; i++) &#123; result = pre2 + pre1; pre2 = pre1; pre1 = result; &#125; return result;&#125; 时间复杂度：$O(n)$。 空间复杂度：$O(1)$。 11. 旋转数组的最小数字题目描述把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。例如数组 $[3,4,5,1,2]$ 为 $[1,2,3,4,5]$ 的一个旋转，该数组的最小值为 $1$。 12输入：[3,4,5,1,2]输出：1 $NOTE$：给出的所有元素都大于 $0$，若数组大小为 $0$，请返回 $0$。 Solution - 1 暴力搜索不就是要求数组里的最小数字嘛，我直接搜索一遍不行吗？净拿修饰词扰乱军心！ 123456public int minArray(int[] numbers) &#123; int min = Integer.MAX_VALUE; for (int num : numbers) min = Math.min(num, min); return min;&#125; 时间复杂度：$O(n)$。 空间复杂度：$O(1)$。 Solution - 2 二分搜索使用暴力搜索，遍历一遍数组就可以获得数组中的最小元素，时间复杂度为 $O(n)$，空间复杂度为 $O(1)$。但是题目描述了那么多，肯定不是为了让你使用暴力搜索的。 如果利用数组部分有序这个特点，可以将时间复杂度降低到 $O(\\lg n)$。 在数组操作中，只要读出了 有序 的明示或者暗示，无论数据类型是整型还是字符串，首先想到 二分搜索。 在示例 $[3,4,5,1,2]$ 中，可以把 $1$ 看作一个 波谷，也可以认为是一个转折点。可以看到，在 $1$ 的左侧的 $[3,4,5]$ 是有序(升序)的，而在包含 $1$ 在内的 $[1,2]$ 也是有序(升序)的，所以可以认为转折点将数组分成了两个有序数组。 设置 start,end,mid 三个变量，分别代表数组左右两端，以及中间。 $numbers[mid] &gt; numbers[end]$ : 如果是升序，mid 应该小于 end，但是这里是大于，说明 mid 左侧都是有序的，旋转点 $x$ 一定在 $[mid+1, end]$ 之间，所以接下来应该在 $[mid+1, end]$ 之间寻找旋转点，同时修改 start = mid + 1。 $numbers[mid] &lt; numbers[end]$ : 说明 mid 右侧是有序的，而旋转点 $x$ 一定在 $[start, mid]$ 之间，所以接下来应该在之 $[start, mid]$ 间寻找旋转点，同时修改 end = mid。 $numbers[mid] == numbers[end]$ ：例如 $[5,0,1,1,1,1]$，执行end-- 缩小搜索区间。 1234567891011121314151617public int minArray(int[] numbers) &#123; if (numbers == null || numbers.length == 0) return 0; int start = 0, end = numbers.length - 1; while (start &lt; end) &#123; // 二分搜索算法于 1946 年提出，然而第一个没有 bug 的二分搜索算法在 1962 年才出现 // 原因是 mid = (start + end) / 2 会出现数值溢出 // 如果 start, end 都在 Integer.MAX_VALUE 的一半左右，相加的结果呢？ int mid = start + (end - start) / 2; if (numbers[mid] &gt; numbers[end]) &#123; start = mid + 1; &#125; else if (numbers[mid] &lt; numbers[end]) &#123; end = mid; &#125; else end--; &#125; return numbers[start];&#125; 时间复杂度：$O(\\lg n)$，但是在 $[1,1,1,1,1,1,\\dots]$ 这种特例下，会退化成 $O(n)$。 时间复杂度：$O(1)$。 12. 矩阵中的路径设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。 路径可以从矩阵中的任意一格开始，每一步可以在矩阵中向左、右、上、下移动一格。如果一条路径经过了矩阵的某一格，那么该路径不能再次进入该格子。例如，在下面的 $3\\times 4$ 的矩阵中包含一条字符串&quot;bfce&quot;的路径。 12345[[\"a\",\"b\",\"c\",\"e\"],[\"s\",\"f\",\"c\",\"s\"],[\"a\",\"d\",\"e\",\"e\"]] 但矩阵中不包含字符串&quot;abfb&quot;的路径，因为字符串的第一个字符 &quot;b&quot; 占据了矩阵中的第一行第二个格子之后，路径不能再次进入这个格子。 Solution - DFS 关于搜索问题，首先想到两种常用的搜索算法，$DFS$、$BFS$。 解题思路： 本问题是典型的矩阵搜索问题，可使用深度优先搜索($DFS$) $+$ 剪枝操作解决。 算法原理： 深度优先搜索： 可以理解为暴力方法遍历矩阵中所有字符串可能性，$DFS$ 通过递归，先朝一个方向搜索到底，再回溯至上个结点，沿一个之前未搜索过的方向搜索，如此反复。 剪枝： 在搜索中，遇到 这条路不可能和目标字符串匹配成功 的情况，则立即返回，称之为可行性剪枝。 算法流程 递归参数： 当前元素在矩阵 board 中的行列索引 i 和 j，当前目标字符在 word 中的索引 k。 终止条件： 返回 $true$ : 字符串 word 已全部匹配，即 k = len(word) - 1； 返回 $false$ : $(1)$ 行或列索引越界 $(2)$ 当前矩阵元素与目标字符不符 $(3)$ 当前矩阵元素已访问过。 递推工作： 标记当前矩阵元素： 将 board[i][j] 值暂存于变量 tmp，并修改为字符 /，代表此元素已访问过，防止之后搜索时重复访问。 搜索下一单元格： 朝当前元素的 上、下、左、右 四个方向开启下层递归，使用 || 连接(或运算，代表只需要存在一条可行路径)，并记录结果到 res。 还原当前矩阵元素： 将tmp暂存值还原至 board[i][j] 元素。 回溯返回值： 返回 res，代表是否搜索到目标字符串。 12345678910111213141516171819202122232425262728293031323334public boolean exist(char[][] board, String word) &#123; char[] words = word.toCharArray(); for (int i = 0; i &lt; board.length; i++) for (int j = 0; j &lt; board[0].length; j++) if (dfs(board, i, j, words, 0)); return true; return false;&#125;public boolean dfs(char[][] board, int i, int j, char[] words, int k) &#123; // 1. 剪枝 if (i &gt;= board.length || i &lt; 0 || j &lt; 0 || j &gt;= board.length || board[i][j] != words[k]) return false; // 2. 剪枝 if (k == words.length - 1) return true; // 3. 存储矩阵中的当前字符 char tmp = board[i][j]; // 4. 置标志位 board[i][j] = '/'; // 5. 开始沿四个方向搜索 boolean res = dfs(board, i + 1, j, words, k + 1) || dfs(board, i - 1, j, words, k + 1) || dfs(board, i, j + 1, words, k + 1) || dfs(board, i, j - 1, words, k + 1); // 6. 回填 board[i][j] = tmp; return res;&#125; 时间复杂度：$O(3^kmn)$，最差情况下，需要遍历矩阵中长度为 $k$ 字符串的所有解决方案，时间复杂度为 $O(3^k)$ ；矩阵中共有 $mn$ 个元素可做起始点，时间复杂度为 $O(mn)$ 。 空间复杂度：$O(k)$，搜索过程中的递归深度不超过 $k$ ，因此系统递归栈占用 $O(k)$ ，最坏情况下 $k=mn$ 。 13. 机器人的运动范围题目描述地上有一个 $m$ 行和 $n$ 列的方格。一个机器人从坐标 $(0,0)$ 的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于 $k$ 的格子。 例如，当 $k$ 为 $18$ 时，机器人能够进入方格 $(35,37)$，因为 $3+5+3+7 = 18$。但是，它不能进入方格 $(35,38)$，因为 $3+5+3+8 = 19$。请问该机器人能够达到多少个格子？ 12345输入：m = 2, n = 3, k = 1输出：31 &lt;= n,m &lt;= 1000 &lt;= k &lt;= 20 本题与矩阵中的路径类似，是典型的矩阵搜索问题。此类问题通常可使用 深度优先搜索($DFS$) 或 广度优先搜索($BFS$)解决。 在介绍 $DFS / BFS$ 算法之前，为提升计算效率，首先讲述两项前置工作：数位之和计算、 搜索方向简化。 数位之和计算： 设某数字 $x$ ，向下取整符号 $//$ ，求余符号 $\\odot$ ，则有： $x \\odot 10$ : 得到 $x$ 的个位数字； $x // 10$ : 令 $x$ 的十进制数向右移动一位，即删除个位数字。 因此可通过循环求得数位和 $s$。 123456def sum(s): s = 0 while x != 0: s += x % 10 x = x // 10 return s 由于机器人每次只能移动一格，因此每次只需要计算 $x$ 到 $x \\pm 1$ 的数位和增量。本题说明 $1 \\le n,m \\le 100$ ，以下公式仅在此范围内适用。 数位和增量表达式： 设 $x$ 的数位和为 $s_x$ ， $x+1$ 的数位和为 $s_{x+1}$ 当 $(x+1)\\odot 10 = 0$ 时： $s_{x+1}=s_x - 8$ ，例如 $19，20$ 的数位和分别为 $10，2$ 当 $(x+1)\\odot 10 \\ne 0$ 时： $s_{x+1}=s_x + 1$ ，例如 $1，2$ 的数位和分别为 $1，2$ 1(x + 1) % 10 != 0 ? s_x + 1 : s_x - 8 搜索方向简化 数位和特点: 根据数位和增量公式得知，数位和每逢 进位 突变一次。 解的三角形结构: 根据数位和特点，矩阵中 满足数位和的解 构成的几何形状形如多个 等腰直角三角形 ，每个三角形的直角顶点位于 $0,10,20,\\dots,0,10,20,\\dots$ 等数位和突变的矩阵索引处 。 三角形内的解虽然都满足数位和要求，但由于机器人每步只能走一个单元格，而三角形间不一定是连通的，因此机器人不一定能到达，称之为 不可达解 ; 同理，可到达的解称为 可达解 （本题求此解） 。 结论：根据可达解的结构，易推出机器人可仅通过向右和向下移动，访问所有可达解 三角形内部： 全部连通，易证； 两三角形连通处： 若某三角形内的解为可达解，则必与其左边或上边的三角形连通（即相交），即机器人必可从左边或上边走进此三角形。 Solution 1 - DFS算法解析： 递归参数： 当前元素在矩阵中的行列索引 $i,j$ ，两者的数位和 $si,sj$ 。 终止条件： 当 $(1)$ 行列索引越界或 $(2)$ 数位和超出目标值 $k$ 或 $(3)$ 当前元素已访问过时，返回 $0$ ，代表不计入可达解。 递推工作： 标记当前单元格： 将索引 $(i,j)$ ，存入 $Set$ visited 中，代表此单元格已被访问过。 搜索下一单元格： 计算当前元素的下、右两个方向元素的数位和，并开启下层递归。 回溯返回值： 返回1 + 右侧搜索的可达解总数 + 下方搜索的可达解总数，代表本单元格递归搜索的可达解总数。 123456789101112131415161718class Solution &#123; private int m, n, k; private boolean[][] visited; public int movingCount(int m, int n, int k) &#123; this.m = m; this.n = n; this.k = k; this.visited = new boolean[m][n]; return dfs(0, 0, 0, 0); &#125; private int dfs(int i, int j, int si, int sj) &#123; if(i &gt;= m || j &gt;= n || k &lt; si + sj || visited[i][j]) return 0; visited[i][j] = true; return 1 + dfs(i + 1, j, (i + 1) % 10 != 0 ? si + 1 : si - 8, sj) + dfs(i, j + 1, si, (j + 1) % 10 != 0 ? sj + 1 : sj - 8); &#125;&#125; 时间复杂度：$O(mn)$，最差情况下需要遍历所有单元格。 空间复杂度：$O(mn)$，$Set$ 内存储所有单元格的索引。 Solution 2 - BFS算法解析： 初始化： 将机器人初始点 $(0, 0)$ 加入队列 queue ； 迭代终止条件： queue 为空，代表已遍历完所有可达解。 迭代工作： 单元格出队： 将队首单元格的 索引、数位和 弹出，作为当前搜索单元格。 判断是否跳过： 当 $(1)$ 行列索引越界 $(2)$ 数位和超出目标值 $k$ $(3)$ 当前元素已访问过时，执行 continue 。 标记当前单元格 ：将单元格索引 (i, j) 存入 $Set$ visited 中，代表此单元格 已被访问过 。 单元格入队： 将当前元素的 下方、右方 单元格的 索引、数位和 加入 queue 。 返回值： $Set$ visited 的长度 len(visited) ，即可达解的数量。 12345678910111213141516public int movingCount(int m, int n, int k) &#123; boolean[][] visited = new boolean[m][n]; int res = 0; Queue&lt;int[]&gt; queue= new LinkedList&lt;&gt;(); queue.add(new int[] &#123; 0, 0, 0, 0 &#125;); while(queue.size() &gt; 0) &#123; int[] x = queue.poll(); int i = x[0], j = x[1], si = x[2], sj = x[3]; if(i &gt;= m || j &gt;= n || k &lt; si + sj || visited[i][j]) continue; visited[i][j] = true; res ++; queue.add(new int[] &#123; i + 1, j, (i + 1) % 10 != 0 ? si + 1 : si - 8, sj &#125;); queue.add(new int[] &#123; i, j + 1, si, (j + 1) % 10 != 0 ? sj + 1 : sj - 8 &#125;); &#125; return res;&#125; 时间复杂度：$O(mn)$，最差情况下需要遍历所有单元格。 空间复杂度：$O(mn)$，$Set$ 内存储所有单元格的索引。 14- I. 剪绳子题目描述给定一个正整数 $n$，将其拆分为至少两个正整数的和，并使这些整数的乘积最大化。 返回可以获得的最大乘积。 12345输入: 10输出: 36解释: 10 = 3 + 3 + 4, 3 × 3 × 4 = 362 &lt;= n &lt;= 58 Solution 1 - 函数极值 将正整数 $n$ 拆分为 $a$ 个小数字: $n = n_1 + n_2 + \\dots + n_a$； 本题等价于求解：$max(n_1 \\times n_2 \\times \\dots n_a)$。 在均值不等式中有$$\\frac{n_1 + n_2 + \\dots + n_a}{a} \\ge \\sqrt{n_1n_2\\dots n_a}$$当且仅当 $n_1=n_2=\\dots = n_a$ 时等式成立。 推导 将绳子按照 $x$ 等分为 $a$ 段，即 $n = ax$ ，则其乘积为 $x^a$ 。观察等式 $(1)$ ，由于 $n$ 为常数，因此当 $x^{\\frac{1}{x}}$ 取最大值时，乘积取到最大值。 $$x^a=x^{\\frac{n}{x}}=(x^{\\frac{1}{x}})^n \\tag{1}$$ 所以，将原问题转化为求 $y=x^{\\frac{1}{x}}$ 的极大值，因此按照微积分中幂指函数微分求导的方法： $$\\begin{align*} 两边同时取自然对数得: \\quad lny &amp;= \\frac{1}{x}lnx\\tag{2} \\ 隐函数求导: \\quad \\frac{1}{y}y^{‘}&amp;=\\frac{1-lnx}{x^2}\\tag{3} \\ 回代\\ y=x^{\\frac{1}{x}}\\ 得: \\quad y^{‘}&amp;=\\frac{1-lnx}{x^2}x^{\\frac{1}{x}}\\tag{4}\\end{align*}$$ 令等式 $y^{‘}=0$ ，解得驻点 $x_0=e \\approx 2.718$，由题意得： 当 $x\\in (0,e]$ 时，$y^{‘}\\ge0 $，所以 $y$ 单调递增； 当 $x \\in [e, +\\infty)$ 时，$y^{‘}\\le0 $，所以 $y$ 单调递减。 所以，$x_0$ 为 $y$ 的极大值点。 根据题意可知切分长度需要为整数，因此取 $x=2$ 或 $x=3$ , 而 $$y_{(3)}^6=(3^{\\frac{1}{3}})^6 = 9 \\y_{(2)}^6=(2^{\\frac{1}{2}})^6 = 8$$ 经过以上推导之后可以发现拆分规则： 最优：将数字 $n$ 拆分成尽可能多个 $3$，余数可能为 $0、1、2$； 次优：若余数为 $2$，则保留，不再拆分为 $1+1$； 最差：若余数为 $1$；则应把一份 $3+1$ 替换为 $2+2$。 算法流程： 当 $n \\le 3$ 时，按照规则不应该拆分，但由于题目要求必须拆分，因为必须拆出一个因子 $1$，返回 $n-1$； 当 $n &gt; 3$ 时，求 $n$ 除以 $3$ 的整数部分 $a$ 和余数部分 $b$ ，并分为以下三种情况： $b=0$，直接返回 $3^a$； $b=1$，将一个 $1+3$ 替换为 $2+2$，返回 $3^{a-1}\\times 4$； $b=2$，返回 $3^a \\times 2$。 1234567public int integerBreak(int n) &#123; if (n &lt;= 3) return n - 1; int a = n / 3, b = n % 3; if (b == 0) return (int) Math.pow(3, a); if (b == 1) return (int) Math.pow(3, a - 1) * 4; return (int) Math.pow(3, a) * 2;&#125; 时间复杂度：$O(1)$。 空间复杂度：$O(1)$。 Solution 2 - 动态规划对于正整数 $n$，当 $n \\ge 2$ 时，可以拆分成两个正整数之和。假设 $k$ 是拆分出的第一个正整数，则剩余部分为 $n - k$， $n - k$ 可以不拆分，或者继续拆分成至少两个正整数之和。由于每个正整数对应的最大乘积取决于比它小的正整数的最大乘积，因此可用动态规划求解。 状态定义：设 $dp$ 数组为动态规划数组，$dp[i]$ 表示在将正整数 $i$ 拆分成至少两个正整数之和时，目前所有拆分数的最大乘积。 状态转移：对于正整数 $i$ ，假设拆出的数为 $j$ 则余下的部分为 $i-j$ ，而对于 $i-j$ 亦有两种选择，要么就此为止，要么继续拆。则 就此为止：此时乘积为 $j \\times (i-j)$； 继续拆：即将 $i-j$ 继续拆分，此时乘积为 $j \\times dp[i-j]$。 因此状态转移方程为：$$dp[i]=max(j\\times (i - j), j * dp[i - j]), 1 \\le j &lt; i$$ 初始值：对于数字 $0$，不可拆，赋值为 $dp[0] = 0$，数字 $1$ 需要拆分成 $0$ 和 $1$，因此 $dp[1] = 0 \\times 1 = 0$。 1234567891011public int cuttingRope(int n) &#123; int[] dp = new int[n + 1]; for (int i = 2; i &lt;= n; i++) &#123; int curMax = 0; for (int j = 1; j &lt; i; j++) &#123; curMax = Math.max(curMax, Math.max(j * (i - j), j * dp[i - j])); &#125; dp[i] = curMax; &#125; return dp[n];&#125; 时间复杂度：$O(n^2)$，其中 $n$ 是给定的正整数。对于从 $2$ 到 $n$ 的每一个整数都要计算对应的 $dp$ 值，计算一个整数对应的 $dp$ 值需要 $O(n)$ 的时间复杂度，因此总的时间复杂度为 $O(n^2)$。 空间复杂度：$O(n)$，其中 $n$ 是给定的正整数。创建一个数组 $dp$，其长度为 $n + 1$。 14- II. 剪绳子 II题目描述给你一根长度为 $n$ 的绳子，请把绳子剪成整数长度的 $m$ 段（$m、n$都是整数，$n&gt;1$并且 $m&gt;1$），每段绳子的长度记为 $k[0],k[1], \\dots ,k[m - 1]$ 。请问 $k[0]\\times k[1]\\times \\cdots \\times k[m - 1]$ 可能的最大乘积是多少？ 例如，当绳子的长度是 $8$ 时，我们把它剪成长度分别为 $2、3、3$ 的三段，此时得到的最大乘积是 $18$。 答案需要取模 $1e9+7（1000000007）$，如计算初始结果为：$1000000008$，请返回 $1$。 Solution - 二分法: 点我去看1234567891011121314class Solution &#123; public int cuttingRope(int n) &#123; if (n &lt;= 3) return n - 1; int b = n % 3, p = 1000000007; long rem = 1, x = 3; for (int a = n / 3 - 1; a &gt; 0; a /= 2) &#123; if (a % 2 == 1) rem = (rem * x) % p; x = (x * x) % p; &#125; if (b == 0) return (int) (rem * 3 % p); if (b == 1) return (int) (rem * 4 % p); return (int) (rem * 6 % p); &#125;&#125; 时间复杂度：$O(log_2 N)$。其中 $N = a$，二分法为对数级别复杂度，每轮仅有求整、求余、次方运算。 空间复杂度：$O(1)$。变量 $a, b, p, x, rem$ 使用常数大小额外空间。 15. 二进制中 1 的个数题目描述输入一个整数，输出该数二进制表示中 $1$ 的个数。 Solution 1 - 位运算如果一个整数不为 $0$，那么这个整数至少有一位是 $1$。如果我们把这个整数减 $1$，那么原来处在整数最右边的 $1$ 就会变为 $0$，原来在 $1$ 后面的所有的 $0$ 都会变成 $1$ (如果最右边的 $1$ 后面还有 $0$ 的话)。其余所有位将不会受到影响。 123n = 10101000n-1 = 10100111n&amp;(n-1) = 10100000 减 $1$ 的结果是把最右边的一个 $1$ 开始的所有位都取反了。 把一个整数减去 $1$ ，再和原整数做(与&amp;)运算，会把该整数最右边一个 $1$ 变成 $0$。 那么一个整数的二进制有多少个 $1$，就可以进行多少次这样的操作。 12345678public int hammingWeight(int n) &#123; int count = 0; while (n != 0) &#123; count++; n &amp;= (n - 1); &#125; return count;&#125; 时间复杂度：$O(n)$，最坏情况下 $n$ 的二进制表示中含有 $n$ 个 $1$。 空间复杂度： $O(1)$。 Solution 2 - 逐位与逐位判断 根据 与运算 定义，设二进制数字 $n$ ，则有： 若 $n &amp; 1 = 0$ ，则 $n$ 二进制位最右一位为 $0$； 若 $n &amp; 1 = 1$ ，则 $n$ 二进制位最右一位为 $1$。 循环判断 若 $n$ 最右一位为 $1$ ，结果加 $1$； 将 $n$ 右移一位(本题要求把数字看作无符号数，因此采用无符号右移。) 12345678public int hammingWeight(int n) &#123; int count = 0; while (n != 0) &#123; count += n &amp; 1; n &gt;&gt;&gt;= 1; &#125; return count;&#125; 时间复杂度：$O(\\log_2 M)$，其中 $M$ 代表输入数字 $n$ 中最高位 $1$ 的位置，例如输入 $16$，则 $16_{10}=1111_2$，那么 $M = log_2 16 = 4$。 空间复杂度： $O(1)$。 Solution 3 - JDK123public int hammingWeight(int n) &#123; return Integer.bitCount(n);&#125; 16. 数值的整数次方题目描述给定一个 $double$ 类型的浮点数 $base$ 和 $int$ 类型的整数 $exponent$。求 $base$ 的 $exponent$ 次方。保证 $base$ 和 $exponent$ 不同时为 $0$。不得使用库函数，同时不需要考虑大数问题。 Solution - 快速幂二进制角度 设 $x=base, n=exponent$。 十进制整数 $n$ 可以表示为二进制的 $b_m\\dots b_3b_2b_1$ ，则有 二进制转十进制： $n=1b_1+2b_2+4b_3+\\cdots+2^{m-1}b_m$ 幂的二进制展开： $x^n=x^{1b_1+2b_2+4b_3+\\cdots + 2^{m-1}b_m}=x^{1b_1}x^{2b_2}x^{4b_3}\\dots x^{2^{m-1}b_m}$ 因此，根据以上操作，可在循环中计算 $x^{2^0b_1},x^{2^1b_2},\\dots,x^{2^{m-1}b_m}$ 的值，并将所有 $x^{2^{i-1}b_i}$ 累积相乘即可。 当 $b_i=0$ 时， $x^{2{i-1}}b_i=1$。 当 $b_i=1$ 时， $x^{2{i-1}}b_i=x^{2^{i-1}}$。 二分角度$$x^n=\\begin{cases}x^{\\frac{n}{2}}·x^{\\frac{n}{2}}, &amp; n%2=0\\x·x^{\\frac{n}{2}}·x^{\\frac{n}{2}}, &amp; n%2=1\\end{cases}$$ 幂结果获取 根据二分推导，可通过循环 $x=x^2$ 操作，每次把幂从 $n$ 降至 $n/2$ ，直至将幂降为 $0$ ； 设 $res = 1$ ，则初始状态 $x^n=x^n\\times res$ 。在循环二分时，每当 $n$ 为奇数时，将多出的一项 $x$ 乘入 $res$ ，则最终可转化为 $x^n=x^0 \\times res = res$。 转化为位运算 向下取整 $n/2$ 等价于右移一位 $n&gt;&gt;1$； 判断奇偶性操作 $n%2$ 等价于 $n&amp;1$。 注意点：当 $n&lt;0$ 时，把问题转化至 $n\\ge 0$ 的范围内，即执行 $x=1/x,n=-n$。 12345678910111213141516public double myPow(double x, int n) &#123; if (x == 0) return 0; long b = n; double res = 1.0; if (b &lt; 0) &#123; x = 1 / x; b = -b; &#125; while (b &gt; 0) &#123; // 如果为奇数时 if ((b &amp; 1) == 1) res *= x; x *= x; b &gt;&gt;= 1; // 相当于 b / 2 &#125; return res;&#125; 时间复杂度：$O(\\log_2n)$，二分的时间复杂度为对数级别。 时间复杂度：$O(1)$，有限个变量。 17. 打印从1到最大的n位数题目描述输入数字 $n$，按顺序打印出从 $1$ 到最大的 $n$ 位十进制数。比如输入 $3$，则打印出 $1、2、3$ 一直到最大的 $3$ 位数 $999$ 。 Solution-分治算法根据题目要求，需要考虑两个问题： 最大的 $n$ 位数(记为 $end$ )和位数 $n$ 的关系：例如最大的 $1$ 位数是 $9$ ，最大的二位数是 $99$ ，因此可以得到： $$end = 10^n - 1$$ 大数越界问题：根据题意，不用考虑。 因此，只需要定义区间 $[1,10^n-1]$ 和步长 $1$，通过 $for$ 循环生成列表 $res$ 并返回即可。 实际上，本题的主要考点是大数越界情况下的打印。需要解决以下三个问题： 表示大数的数据类型：无论是 short/int/long 及其他变量类型，数字的取值范围都是有限的。因此，大数的表示应用字符串 String 类型。 生成数字的字符串集： 使用 int 类型时，每轮可通过 +1 生成下个数字，而此方法无法应用至 String 类型。并且， String 类型的数字的进位操作效率较低，例如 “$9999$” 至 “$10000$” 需要从个位到千位循环判断，进位 $4$ 次。 观察可知，生成的列表实际上是 $n$ 位 $0\\sim9$ 的 全排列 ，因此可避开进位操作，通过递归生成数字的 String 列表。 递归生成全排列：先固定高位，向低位递归，当个位已被固定时，添加数字的字符串。例如当 $n=2$ 时(数字范围 $1 \\sim 99$ )，固定十位为 $0 \\sim 9$ ，按顺序依次开启递归，固定个位 $0 \\sim 9$ ，终止递归并添加数字字符串。 1234567891011121314151617181920212223242526272829303132class Solution &#123; private int[] res; private int nine = 0, count = 0, start, n; private char[] num, loop = &#123;'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'&#125;; public int[] printNumbers(int n) &#123; this.n = n; res = new int[(int) Math.pow(10, n) - 1]; num = new char[n]; start = n - 1; // 字符左边界 dfs(0); return res; &#125; void dfs(int x) &#123; if (x == n) &#123; String s = String.valueOf(num).substring(start); if (!s.equals(\"0\")) res[count++] = Integer.parseInt(s); if (n - start == nine) start--; return; &#125; for (char i : loop) &#123; if (i == '9') nine++; num[x] = i; dfs[x + 1]; &#125; nine--; &#125;&#125; 时间复杂度：$O(10^n)$，递归生成的排列数量为 $10^n$。 空间复杂度：$O(n)$，字符列表 $num$ 使用线性大小的额外空间。 18. 删除链表中的结点题目描述给定单向链表的头指针和一个要删除的结点的值，定义一个函数删除该结点。返回删除后的链表的头结点。 题目保证链表中节点的值互不相同。 123输入: head = [4,5,1,9], val = 5输出: [4,1,9]解释: 给定你链表中值为 5 的第二个节点，那么在调用了你的函数之后，该链表应变为 4 -&gt; 1 -&gt; 9. Solution - 双指针删除值为 val 的节点可分为两步：定位节点、修改引用。 定位结点：遍历链表，直到找到 node.val == val 时跳出，即可定位目标结点。 修改结点：设 cur 指向当前结点，pre 为 cur 的前驱结点，cur.next 为 cur 的后继结点，则执行 pre.next = cur.next，实现删除 cur 结点的功能。 算法流程 特例处理：当应删除头结点 head 时，直接返回 head.next 即可。 初始化：pre = head; cur = head.next; 定位结点：当 cur 为空，或者 cur.val == val 时跳出遍历。 保存当前结点索引，执行 pre = cur； 遍历下一结点，即 cur = cur.next。 删除结点： 若 cur 指向某结点(即找到了与 val 值相等的结点)，则执行 pre.next = cur.next ，实现删除结点的操作。 若 cur == null，代表链表中不包含 val 的结点。 返回值：返回头结点 head 即可。 12345678910111213141516171819public ListNode deleteNode(ListNode head, int val) &#123; if (head.val == val) return head.next; ListNode pre = head, cur = head.next; while (cur != null &amp;&amp; cur.val != val) &#123; // 如果当前结点不等于空(未到链表尾部) 并且当前结点的值不是目标值时 // 从头开始遍历 寻找值等于 val 的结点 pre = cur; cur = cur.next; &#125; // 既然上面的 while 循环条件是 &amp;&amp; 条件 而 &amp;&amp; 条件存在短路效应 // 所以循环后的结果存在 || 结果 // 也就是经过上面的 while 循环后，可能找到了链表中值等于 val 的结点 // 也可能未找到结点 只有在找到结点时才进行删除 if (cur.val == val) pre.next = cur.next; // 反正最后要返回 head, 所以不判断 if (head == null) 了. return head;&#125; 时间复杂度：$O(n)$，可能需要遍历整个链表搜索要删除的结点。 空间复杂度：$O(1)$，两个辅助结点。 19. 正则表达式匹配请实现一个函数用来匹配包含 . 和 * 的正则表达式。模式中的字符 . 表示任意一个字符，而 * 表示它前面的字符可以出现任意次(含 $0$ 次)。在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串 aaa 与模式 a.a 和 ab*ac*a 匹配，但与 aa.a 和 ab*a 均不匹配。 1234567891011输入:s = \"aa\"p = \"a\"输出: false解释: \"a\" 无法匹配 \"aa\" 整个字符串。输入:s = \"aa\"p = \"a*\"输出: true解释: 因为 '*' 代表可以匹配零个或多个前面的那一个元素, 在这里前面的元素就是 'a'。因此，字符串 \"aa\" 可被视为 'a' 重复了一次。 s 可能为空，且只包含从 a-z 的小写字母。 p 可能为空，且只包含从 a-z 的小写字母以及字符 . 和 *，无连续的 &#39;*&#39;。 Solution - 动态规划 感慨一下，自己现在终于能做出 DP 的题了。 关于为什么考虑动态规划？ 经验：只要读题时思维乱成一团，八成就是动态规划。关于字符串的很多复杂问题也都要用到动态规划，典型的有：最长公共子序列、最长公共子串、最长上升子序列等。 理论：本题具有最优子结构性质，但是不容易发现。 通常拥有数学中递推性质的都可以用动态规划来解决。 题目中的匹配是一个「逐步匹配」的过程：我们每次从字符串 $p$ 中取出一个字符或者「字符 + *」的组合，并在 $s$ 中进行匹配。对于 $p$ 中一个字符而言，它只能在 $s$ 中匹配一个字符，匹配的方法具有唯一性；而对于$p$ 中「字符 + *」的组合而言，它可以在 $s$ 中匹配任意自然数个字符，并不具有唯一性。因此我们可以考虑使用动态规划，对匹配的方案进行枚举。 不少前辈说「动态规划」问题最难的地方在于「定义状态转移方程」，但是本题最困难的地方在于「定义状态」，也就是读懂题意。在诸如问「最大最小最多最少」的问题中一般问题最后要求的结果是什么，「状态定义」就是什么。 状态定义：$f[i][j]$ 表示 $s$ 的前 $i$ 个字符能否与 $p$ 中前 $j$ 个字符匹配。 状态转移：在一维 $DP$ 中， $dp[i]$ 通常与 $dp[i-1]$ 有关系，但条件限定的话也可能与 $dp[i-1]、dp[i-3]$ 有直接关系；在二维 $DP$ 中， $dp[i][j]$ 通常与 $dp[i-1][j]、dp[i][j-1]、dp[i-1][j-1]$ 有关系，如果条件限定的话，步长可能也会大一点；在三维及之后的问题，我想你已经明白我的意思了。 在高中数学中有种方法叫做归纳推理法，它的核心思想是假设 $n=k$ 时结论成立，那么 $n=k+1$ 时也成立，这是一种从前向后的方法，而寻找「状态转移方程」的过程可以简单理解为这个方法的逆过程，也就是从后向前的过程寻找状态，也就是我想要达到 $dp[i][j]$ 这个状态，我需要 $dp[i-1][j],dp[i][j-1],dp[i-1][j-1]$ 中的一种或多种状态满足什么样的条件。在计算「最大最小最多最少」问题时还可以想到高中排列组合中的「分步用乘法，分类用加法」这句话。 铺垫了这么多， $f[i][j]$ 与 $f[i-1][j],f[i][j-1],f[i-1][j-1]$ 建立关系时需要满足什么样的条件？ 如果 $p$ 的第 $j$ 个字符是字母，那么想要匹配的话， $s$ 中的第 $i$ 个字母需要与其相同，即 $$f[i][j]=\\begin{cases}f[i-1][j-1], &amp;s[i]=p[j] \\false,&amp;s[i]\\ne p[j]\\end{cases}$$ 如果 $p$ 的第 $j$ 个字符是「*」，那么可以使用 $p$ 的第 $j-1$ 个字母零次或多次。匹配零次时有 $$f[i][j]=f[i][j-2]$$ 意思是，第 $j$ 位字符「*」没有用，第 $j-1$ 位又跟 $s$ 的第 $i$ 位不等，那就只能依靠 $p$ 的第 $i-2$ 位了。 在匹配多次时，有$$f[i][j]=\\begin{cases}f[i][j]=f[i-1][j-2], \\quad s[i]=p[j-1] \\f[i][j]=f[i-2][j-2], \\quad s[i-1]=s[i]=p[j-1] \\f[i][j]=f[i-3][j-2], \\quad s[i-1]=s[i-1]=s[i]=p[j-1] \\\\dots\\end{cases}$$「字母 + *」的组合在匹配的过程中，本质上只会有两种情况： 匹配 $s$ 末尾的字符，将该字符丢弃之后，组合仍可以继续匹配； 不匹配字符，将该组合丢弃，不再进行匹配。 于是得到$$f[i][j]=\\begin{cases}f[i-1][j]\\ or\\ f[i][j-2], &amp; s[i]=p[j-1]\\f[i][j-2], &amp; s[i]\\ne p[j-1]\\end{cases}$$ 如果 $p$ 的第 $j$ 个字符是「.」，那么 $p[j]$ 一定成功匹配 $s$ 中的任意一个小写字母。 三种情况下的状态转移方程合并$$f[i][j]=\\begin{cases}if(p[j]\\ne 「*」)=\\begin{cases}f[i-1][j-1], &amp;matches(s[i],p[j]) \\false, &amp;otherwise\\end{cases} \\otherwise=\\begin{cases}f[i-1][j]\\ or\\ f[i][j-2], &amp;matches(s[i],p[j-1]) \\f[i][j-2], &amp;otherwise\\end{cases}\\end{cases}$$其中，matches(a, b) 函数用于判断字符是否匹配。 状态初始化：$f[0][0]=true$，即两个空字符串是匹配的。 返回值：$f[m][n]$，其中 $m,\\ n$ 分别问两个字符串的长度，这里 $dp$ 数组多开一行一列，但是不用。 123456789101112131415161718192021222324public boolean isMatch(String s, String p) &#123; int m = s.length(), n = p.length() - 1; boolean[][] f = new boolean[m + 1][n + 1]; f[0][0] = true; for (int i = 0; i &lt;= m; ++i) &#123; for (int j = 1; j &lt;= n; ++j) &#123; if (p.charAt(j - 1) == '*') &#123; f[j][j] = f[i][j - 2]; if (matches(s, p, i, j - 1)) f[i][j] = f[i][j] || f[i - 1][j]; &#125; else &#123; if (matches(s, p, i, j)) f[i][j] = f[i - 1][j - 1]; &#125; &#125; &#125; return f[m][n];&#125;public boolean matches(String s, String p, int i, int j) &#123; if (i == 0) return false; if (p.charAt(j - 1) == '.') return true; return s.charAt(i - 1) == p.charAt(j - 1);&#125; 时间复杂度：$O(mn)$，需要计算出所有的状态，并且每个状态在进行转移时的时间复杂度为 $O(1)$。 空间复杂度：$O(mn)$，即为存储所有状态使用的空间。 20. 表示数值的字符串请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”$+100$”、”$5e2$”、”$-123$”、”$3.1416$”、”$0123$”都表示数值，但”$12e$”、”$1a3.14$”、”$1.2.3$”、”$+-5$”都不是。 Solution-有限状态自动机本题使用有限状态自动机。根据字符类型和合法数值的特点，先定义状态，再画出状态转移图，最后编写代码即可。 字符类型：空格「 」，数字「$0 \\sim 9$」，正负号「$+-$」，小数点「$.$」幂符号「$e$」。 状态定义：按照字符串从左到右的顺序，定义以下 $9$ 种状态。 开始的空格 幂符号前的正负号 小数点前的数字 小数点、小数点后的数字 当小数点前为空格时，小数点、小数点后的数字 幂符号 幂符号后的正负号 幂符号后的数字 结尾的空格 合法的结束状态有：$2,3,7,8$。 算法流程： 初始化： 状态转移表 $states$ ，设 $states[i]$ ，其中 $i$ 为所处状态， $states[i]$ 使用哈希表存储可转移至的状态。键值对 $(key,value)$ 含义：若输入 $key$ ，则可从状态 $i$ 转移至状态 $value$ 。 当前状态 $p$ ：起始状态初始化为 $p=0$ 。 状态转移循环：遍历字符串 $s$ 的每个字符 $c$ 记录字符类型 $t$ ：分为三种情况 当 $c$ 为正负号时， t = &#39;s&#39;； 当 $c$ 为数字时，t = &#39;d&#39;； 否则，t = c，即用字符本身表示字符类。 终止条件：若字符类型 $t$ 不在哈希表 $states[p]$ 中，说明无法转移至下一状态，因此直接返回 $False$。 状态转移：状态 $p$ 转移至 $states[p][t]$。 返回值：跳出循环后，若状态 $p \\in 2,3,7,8$ ，说明结尾合法，返回 $True$ ，否则返回 $False$。 12345678910111213141516171819202122232425class Solution &#123; public boolean isNumber(String s) &#123; Map[] states = &#123; new HashMap&lt;&gt;() &#123;&#123; put(' ', 0); put('s', 1); put('d', 2); put('.', 4); &#125;&#125;, // 0. new HashMap&lt;&gt;() &#123;&#123; put('d', 2); put('.', 4); &#125;&#125;, // 1. new HashMap&lt;&gt;() &#123;&#123; put('d', 2); put('.', 3); put('e', 5); put(' ', 8); &#125;&#125;, // 2. new HashMap&lt;&gt;() &#123;&#123; put('d', 3); put('e', 5); put(' ', 8); &#125;&#125;, // 3. new HashMap&lt;&gt;() &#123;&#123; put('d', 3); &#125;&#125;, // 4. new HashMap&lt;&gt;() &#123;&#123; put('s', 6); put('d', 7); &#125;&#125;, // 5. new HashMap&lt;&gt;() &#123;&#123; put('d', 7); &#125;&#125;, // 6. new HashMap&lt;&gt;() &#123;&#123; put('d', 7); put(' ', 8); &#125;&#125;, // 7. new HashMap&lt;&gt;() &#123;&#123; put(' ', 8); &#125;&#125; // 8. &#125;; int p = 0; char t; for(char c : s.toCharArray()) &#123; if(c &gt;= '0' &amp;&amp; c &lt;= '9') t = 'd'; else if(c == '+' || c == '-') t = 's'; else t = c; if(!states[p].containsKey(t)) return false; p = (int)states[p].get(t); &#125; return p == 2 || p == 3 || p == 7 || p == 8; &#125;&#125; 时间复杂度：$O(n)$，其中 $n$ 为字符串 $s$ 的长度，判断需遍历字符串，每轮状态转移使用 $O(1)$ 时间。 空间复杂度：$O(1)$， $states,p$ 使用常数大小的额外空间。 21. 调整数组顺序使奇数位于偶数前面题目描述输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有奇数位于数组的前半部分，所有偶数位于数组的后半部分。 123456输入：nums = [1,2,3,4]输出：[1,3,2,4]注：[3,1,2,4] 也是正确的答案之一。1 &lt;= nums.length &lt;= 500001 &lt;= nums[i] &lt;= 10000 Solution 1 - 对撞指针12345678910111213141516171819202122232425public int[] exchange(int[] nums) &#123; // 考虑对撞指针 // 左指针从左向右扫描，右指针从右向左扫描 // 当左指针扫描到偶数时停下，右指针扫描到奇数时停下, 然后交换数值。 if (nums == null || nums.length == 0 || nums.length == 1) return nums; int l = 0, r = nums.length - 1; while (l &lt; r) &#123; while (l &lt; r &amp;&amp; !isEven(nums[l])) l++; while (l &lt; r &amp;&amp; isEven(nums[r])) r--; swap(nums, l, r); &#125; return nums;&#125;// 在计算机中, 偶数的最后一位是0, 而奇数的最后一位是1public boolean isEven(int num) &#123; // 是否偶数 return (num &amp; 1) == 0;&#125;private void swap(int[] nums, int l, int r) &#123; int temp = nums[l]; nums[l] = nums[r]; nums[r] = temp;&#125; 时间复杂度：$O(n)$，扫描一遍数组。 空间复杂度：$O(1)$，有限个变量。 Solution 2 : 快慢指针123456789101112131415161718192021222324252627282930class Solution &#123; public int[] exchange(int[] nums) &#123; // 快慢指针 // 慢指针用来找偶数 快指针用来找奇数 然后交换 // 在交换之前能够保证慢指针左边都是奇数，慢指针与快指针之间的都是偶数 // 在交换后，指针移动一格，快指针继续向后寻找 if (nums == null || nums.length == 0 || nums.length == 1) return nums; int slow = 0, fast = 0; while (fast &lt; nums.length) &#123; if (!isEven(nums[fast])) &#123; swap(nums, slow, fast); slow++; &#125; fast++; &#125; return nums; &#125; // 在计算机中, 偶数的最后一位是 0, 而奇数的最后一位是 1。 public boolean isEven(int num) &#123; // 是否偶数 return (num &amp; 1) == 0; &#125; private void swap(int[] nums, int l, int r) &#123; int temp = nums[l]; nums[l] = nums[r]; nums[r] = temp; &#125;&#125; 时间复杂度：$O(n)$，扫描一遍数组。 空间复杂度：$O(1)$，有限个变量。 22. 链表中倒数第k个节点题目描述输入一个链表，输出该链表中倒数第 $k$ 个节点。为了符合大多数人的习惯，本题从 $1$ 开始计数，即链表的尾节点是倒数第 $1$ 个节点。例如，一个链表有 $6$ 个节点，从头节点开始，它们的值依次是 $1、2、3、4、5、6$。这个链表的倒数第 $3$ 个节点是值为 $4$ 的节点。 123给定一个链表: 1-&gt;2-&gt;3-&gt;4-&gt;5, 和 k = 2.返回链表 4-&gt;5. Solution-双指针两个解题思路： 遍历链表一次，获取链表的长度 $n$ ，然后再遍历一次链表，但是这次只遍历到 $n-k$ 这个位置即可，此时即为链表的倒数第 $k$ 个结点。 使用双指针，让快的指针先走 $k$ 步，那么当快的指针到达到达链表尾部时，慢的指针正好到达倒数第 $k$ 个结点。 算法流程： 初始化：设快指针为 former，慢指针为 latter，初始时都指向头结点 head。 创建步差：让 former 先走 $k$ 步。 同步前进：循环中，让 former 和 latter 同步前进，直至 former 走过链表尾结点时跳出循环。 返回值：返回 latter。 123456789101112131415class Solution &#123; public ListNode getKthFromEnd(ListNode head, int k) &#123; if (head == null || k &lt; 0) return head; ListNode former = head; while (former != null &amp;&amp; k-- &gt; 0) p1 = p1.next; // 说明链表总结点个数不足 k 个 if (k &gt; 0) return null; ListNode latter = head; while (former != null) &#123; former = former.next; latter = latter.next; &#125; return latter; &#125;&#125; 时间复杂度：$O(n)$，遍历一次链表。 空间复杂度：$O(1)$。 两个辅助结点。 LeetCode链表常见问题 24. 反转链表定义一个函数，输入一个链表的头节点，反转该链表并输出反转后链表的头节点。 1234567反转一个单链表。输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL进阶:你可以迭代或递归地反转链表。你能否用两种方法解决这道题？ Solution 1 -双指针算法流程： 初始化：定义两个指针，cur 指向当前结点， pre 指针指向当前结点的下一个结点。 局部反转：每次让 pre.next 指向 cur。 同步移动：pre 和 cur 同步移动一个结点。 返回值：当 pre == null 时，返回 cur. 123456789101112public ListNode reverseList(ListNode head) &#123; if (head == null) return null; ListNode pre = null, temp = null; ListNode cur = head; while (cur != null) &#123; temp = cur.next; // 临时保存当前结点的下一个结点 cur.next = pre; // 将当前结点指向前一个结点 pre = cur; // 之前的结点移动一次 cur = temp; // 当前结点移动一次 &#125; return pre;&#125; 时间复杂度：$O(n)$，遍历一次链表。 空间复杂度：$O(1)$。 两个辅助结点。 Solution 2 - 递归在实现逆序或者反转相关的操作时可以考虑使用“栈”这种数据结构，要么自己实现，要么调用系统栈，而提到系统栈，首先就要想到递归。递归可能不太好理解，这里有个幻灯片解释。 123456789101112public ListNode reverseList(ListNode head) &#123; // 1. 递归一定要有终止条件 if (head == null || head.next == null) return head; // 2. 这一步获取的是链表的最后一个结点 ListNode tail = reverseList(head.next); // 3. 从尾到头重新链接上 // 1-&gt;2-&gt;3-&gt;4-&gt;5 变为 1-&gt;2-&gt;3-&gt;4 5，上面的 tail 指向 5, head.next = tail // 接下来要做的就是让 5 指向 4(head) head.next.next = head; head.next = null; return tail;&#125; 时间复杂度：$O(n)$，遍历一次链表，每次链接链表的操作为 $O(1)$。 空间复杂度：$O(n)$, 系统递归栈需要存储链表中的所有结点。 25. 合并两个已排序的链表输入两个递增排序的链表，合并这两个链表并使新链表中的节点仍然是递增排序的。 Solution - 双指针两个重要的隐含条件：有序、递增。 所以可以考虑使用双指针 $l_1,l_2$ 分别遍历两链表，根据 $l_1.val,l_2.val$ 的大小关系来确定结点添加顺序，两结点指针交替前进，直至遍历完毕。 由于初始状态合并链表中无结点，所以需要设置一个头结点。 算法流程： 初始化：创建伪头结点 $dummy$，结点 $cur$ 指向 $dummy$。 循环合并：当 $l_1$ 或 $l_2$ 为空时跳出。 当 $l_1.val &lt; l_2.val$ 时: $cur$ 的后继结点指定为 $l1$ ， $l_1$ 向前走一步； 当 $l_1.val \\ge l_2.val$ 时: $cur$ 的后继结点指定为 $l2$ ， $l_2$ 向前走一步； 结点 $cur$ 向前走一步，即 $cur = cur.next$。 合并剩余尾部：跳出时有两种情况，即 $l_1$ 为空或 $l_2$ 为空。 若 $l_1\\ne null$ ，将 $l_1$ 添加至结点 $cur$ 结点之后； 否则，将 $l_2$ 添加至结点 $cur$ 之后。 1234567891011121314151617181920class Solution &#123; public ListNode mergeTwoLists(ListNode l1, ListNode l2) &#123; if (l1 == null &amp;&amp; l2 == null) return null; if (l1 == null &amp;&amp; l2 != null) return l2; if (l1 != null &amp;&amp; l2 == null) return l1; ListNode dummy = new ListNode(0), cur = dummy; while (l2 != null &amp;&amp; l1 != null) &#123; if (l1.val &lt; l2.val) &#123; cur.next = l1; l1 = l1.next; &#125; else &#123; cur.next = l2; l2 = l2.next; &#125; cur = cur.next; &#125; cur.next = (l1 != null ? l1 : l2); return dummy.next; &#125;&#125; 时间复杂度：$O(m+n)$，需要遍历两个链表。 空间复杂度：$O(1)$，两个辅助结点。 26(🌲). 树的子结构题目描述输入两棵二叉树 $A$ 和 $B$ ，判断 $B$ 是不是 $A$ 的子结构。(约定空树不是任意一个树的子结构)。 $B$ 是 $A$ 的子结构， 即 $A$ 中有出现和 $B$ 相同的结构和节点值。 Solution - 先序遍历若树 $B$ 是树 $A$ 的子结构，则子结构的根节点可能为树 $A$ 的任意一个节点。因此，判断树 $B$ 是否是树 的子结 $A$ 构，需完成以下两步工作： 先序遍历 $A$ 的每个节点 $n_A$ ，isSubStructure(A, B) 判断树 $A$ 中以 $n_A$ 为根节点的子树是否包含树 $B$ ，recur(A, B) 名词规定：树 $A$ 的根节点记作 节点 $A$ ，树 $B$ 的根节点称为节点 $B$ 。 recur(A, B)函数： 终止条件： 当结点 $B$ 为空：说明树 $B$ 已匹配完成，返回 $true$ 当结点 $A$ 为空：说明已经越过树 $A$ 叶子结点，即匹配失败，返回 $false$ 当结点 $A、B$ 的值不同：说明匹配失败，返回 $false$ 返回值： 判断 $A$ 和 $B$ 的左子结点是否相等，即recur(A.left, B.left) 判断 $A$ 和 $B$ 的右子结点是否相等，即recur(A.right, B.right) isSubstructure(A, B)函数： 特例处理：当树 $A$ 为空或树 $B$ 为空时，直接返回 $false$。 返回值：若树 $B$ 是树 $A$ 的子结构，则必须满足以下三种情况之一，因此可用或 || 连接。 以 节点 $A$ 为根节点的子树 包含树 $B$ ，对应 recur(A, B)； 树 $B$ 是 树 $A$ 左子树 的子结构，对应 isSubStructure(A.left, B)； 树 $B$ 是 树 $A$ 右子树 的子结构，对应 isSubStructure(A.right, B)。1234567891011121314public boolean isSubStructure(TreeNode A, TreeNode B) &#123; // 只有在 A B 均不为空时才有可能出现子结构 // 1. 以 A 为根结点的子树包含 B recur(A, B) // 2. 以 A 的左结点为根结点的子树包含 B isSubStructure(A.left, B) // 3. 以 A 的右结点为根结点的子树包含 B isSubStructure(A.right, B) return (A != null &amp;&amp; B != null) || (recur(A, B) || isSubStructure(A.left, B) || isSubStructure(A.right, B));&#125;public boolean recur(TreeNode A, TreeNode B) &#123; if (B == null) return true; // 如果 B 的子树为空 说明已经到达了B的叶子结点，也就是说整个树B都包含在了A中 if (A == null || A.val != B.val) return false; // 如果 A 的子树为空，说明已经到达了A的叶子结点，不会再有子树 // 继续判断 A B 的左右子结点是否相同 return recur(A.left, B.left) &amp;&amp; recur(A.right, B.right);&#125; 时间复杂度：$O(MN)$，其中 $M,N$ 分别为两树种结点的数量。 空间复杂度：$O(M)$，当$A、B$ 都退化为链表时，递归调用深度最大。 27(🌲).二叉树的镜像题目描述请完成一个函数，输入一个二叉树，该函数输出它的镜像。 123456789101112131415161718输入： 4 / \\ 2 7 / \\ / \\1 3 6 9输出： 4 / \\ 7 2 / \\ / \\9 6 3 1输入：root = [4,2,7,1,3,6,9]输出：[4,7,2,9,6,3,1]0 &lt;= 节点个数 &lt;= 1000 Solution 1 - 递归递归遍历二叉树，交换每个节点的 左/右 子节点，即可生成二叉树的镜像。 递归解析 终止条件：当节点 $root$ 为空时返回 $null$ ； 递推工作： 初始化节点 $tmp$，用于暂存 $root$ 的左子节点； 递归右子节点，将返回值作为 $root$ 的左子节点； 递归左子节点，将返回值作为 $root$ 的右子节点。 返回值：返回 $root$。 Question: 为何要暂存 $root$ 左子节点？ Answer：在递归右子节点完毕后，$root.left$ 的值已发生改变。 1234567public TreeNode mirrorTree(TreeNode root) &#123; if (root == null) return null; TreeNode temp = root.left; root.left = mirrorTree(root.right); root.right = mirrorTree(temp); return root;&#125; 时间复杂度：$O(n)$，其中 $n$ 为二叉树的节点数量，建立二叉树镜像需要遍历树的所有节点，占用 $O(n)$ 时间。 空间复杂度： $O(n)$ ，最差情况下（当二叉树退化为链表），递归时系统需使用 $O(n)$ 大小的栈空间。 扩展： 本题竟是 $Homebrew$ 之父当年没能写出的 226. 翻转二叉树 前序、中序、后序、层次遍历解本题 Solution 2 - 辅助栈利用栈（或队列）遍历树的所有节点 $node$ ，并交换每个 $node$ 的左 / 右子节点。 1234567891011121314public TreeNode mirrorTree(TreeNode root) &#123; if (root == null) return null; Deque&lt;TreeNode&gt; stack = new ArrayDeque&lt;&gt;(); stack.push(root); while (!stack.isEmpty()) &#123; TreeNode t = stack.pop(); if (t.left != null) stack.push(t.left); if (t.right != null) stack.push(t.right); TreeNode temp = t.left; t.left = t.right; t.right = temp; &#125; return root;&#125; 时间复杂度： $O(n)$ ，其中 $n$ 为二叉树的节点数量，建立二叉树镜像需要遍历树的所有节点，占用 $O(n)$ 时间。 空间复杂度： $O(n)$ ，最差情况下（当为满二叉树时），栈 $stack$ 最多同时存储 $n/2$ 个节点，占用 $O(n)$ 额外空间。 28(🌲). 对称的二叉树题目描述实现一个函数，用来判断一棵二叉树是不是对称的。如果一棵二叉树和它的镜像一样，那么它是对称的。 例如，二叉树 $[1,2,2,3,4,4,3]$ 是对称的。 1234567 1 / \\ 2 2 / \\ / \\3 4 4 30 &lt;= 节点个数 &lt;= 1000 Solution - 递归大部分人看到本题时应该都已经有思路了，本题最大的问题其实并不是思路的问题，而是是否有能力把思路表达出来的问题。 除去递归终止条件，要判断的主要有三点， 两个指针指向的节点的 $val$ 是否相等； $L$ 的左子树是否与 $R$ 的右子树对称； $L$ 的右子树是否与 $R$ 的左子树对称。 递归终止条件： 两个指针都为 $null$ 则返回 $true$ ； 两个指针其中之一为 $null$ 则返回 $false$ 。 1234567891011public boolean isSymmetric(TreeNode root) &#123; if (root == null) return false; // 如果树不为空 return recur(root.left, root.right);&#125;public boolean recur(TreeNode L, TreeNode R) &#123; if (L == null &amp;&amp; R == null) return true; if (L == null || R == null || L.val != R.val) return false; return recur(L.left, R.right) &amp;&amp; recur(L.right, R.left);&#125; 时间复杂度：$O(n)$，$n$ 为节点数量，每次递归可以判断一对节点是否对称，最多执行 $n/2$ 次； 空间复杂度：$O(n)$，二叉树退化为链表时系统使用 $O(n)$ 栈空间。 29. 顺时针打印矩阵题目描述输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字。 12输入：matrix = [[1,2,3],[4,5,6],[7,8,9]]输出：[1,2,3,6,9,8,7,4,5] Solution - 收缩逆时针打印矩阵，也可以理解为打印顺序为：从左向右，自上而下，从右向左，自下而上。因此考虑设置矩阵的上、下、左、右四个边界，模拟矩阵收缩。 算法流程： 空值处理：当 matrix 为空时，直接返回空列表。 初始化：设置上下左右四边界，top,bottom,left,right，用于矩阵向内收缩。 循环打印：按”从左向右，自上而下，从右向左，自下而上”的方式循环，每个方向打印的过程中包括以下三件事： 根据边界打印，将结果存储到 res 中尾部； 边界向内收缩 $1$； 判断边界是否相遇，若相遇则跳出。 返回值：返回结果 res。 123456789101112131415161718192021public int[] spiralOrder(int[][] matrix) &#123; if (matrix == null || matrix.length == 0) return new int[0]; int top = 0, bottom = matrix.length - 1, left = 0, right = matrix[0].length - 1; int x = 0; int[] res = new int[(bottom + 1) * (right + 1)]; while (true) &#123; // 从左向右 for (int i = left; i &lt;= right; i++) res[x++] = matrix[top][i]; if (++top &gt; bottom) break; // 上边界向下收缩 // 自上而下 for (int i = top; i &lt;= bottom; i++) res[x++] = matrix[i][right]; if (--right &lt; left) break; // 右边界向左收缩 // 从右向左 for (int i = right; i &gt;= left; i--) res[x++] = matrix[bottom][i]; if (--bottom &lt; top) break; // 下边界向上收缩 // 自下而上 for (int i = bottom; i &gt;= top; i--) res[x++] = matrix[i][left]; if (++left &gt; right) break; // 左边界向右收缩 &#125; return res;&#125; 时间复杂度：$O(MN)$，其中 $M,N$ 分别为矩阵行列。 空间复杂度：$O(1)$，四个方向值。 30. 包含min函数的栈定义栈的数据结构，请在该类型中实现一个能够得到栈的最小元素的 $min$ 函数在该栈中，调用 $min,push$ 及 $pop$ 的时间复杂度都是 $O(1)$。 12345678910MinStack minStack = new MinStack();minStack.push(-2);minStack.push(0);minStack.push(-3);minStack.min(); --&gt; 返回 -3.minStack.pop();minStack.top(); --&gt; 返回 0.minStack.min(); --&gt; 返回 -2.各函数的调用总次数不超过 20000 次 Solution-辅助栈将 min() 函数复杂度降为 $O(1)$ ，可通过建立辅助栈实现 数据栈 stackData: 用于存储所有元素，保证入栈 push()、pop()、top() 等功能。 辅助栈 stackHelper: 用于以非严格降序存储stackData中的数据，stackHelper 的栈顶元素始终为 stackData 中的最小元素，返回 stackHelper 的栈顶元素，即可实现 min() 函数的功能。 在操作的过程中为了保证 stackData 的非严格单调性，需要注意： 在 push() 操作时，设待 push 的元素为 e，如果 stackHelper 为空或者 e &lt;= stackHelper.peek()，那么将 e 也 push 到 stackHelper 中； 在 pop() 操作时，设待 pop() 的元素为 e，如果 e == stackHelper.peek()，那么从 stackHelper 也进行 pop()。 123456789101112131415161718192021222324252627282930public class MinStack &#123; private Stack&lt;Integer&gt; stackData, stackHelper; /** initialize your data structure here. */ public MinStack() &#123; stackData = new Stack&lt;&gt;(); stackHelper = new Stack&lt;&gt;(); &#125; public void push(int x) &#123; stackData.add(x); if (stackHelper.isEmpty() || x &lt;= stackHelper.peek()) stackHelper.add(x); &#125; public void pop() &#123; if (!stackData.isEmpty()) if (stackData.pop().equals(stackHelper.peek())) stackHelper.pop(); &#125; public int top() &#123; return stackData.peek(); &#125; public int min() &#123; return stackHelper.peek(); &#125;&#125; 时间复杂度：$O(1)$，不必多言。 空间复杂度：$O(n)$，最坏情况下，比如输入一个单调递减的数列，那么 stackHelper 将需要存储所有元素。 31. 栈的压入、弹出序列题目描述输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如，序列 ${1,2,3,4,5}$ 是某栈的压栈序列，序列 ${4,5,3,2,1}$ 是该压栈序列对应的一个弹出序列，但 ${4,3,5,1,2}$ 就不可能是该压栈序列的弹出序列。 12345输入：pushed = [1,2,3,4,5], popped = [4,5,3,2,1]输出：true解释：可以按以下顺序执行：push(1), push(2), push(3), push(4), pop() -&gt; 4,push(5), pop() -&gt; 5, pop() -&gt; 3, pop() -&gt; 2, pop() -&gt; 1 Solution因为栈的数据操作具有 先入后出 的特性，因此某些弹出序列是无法实现的。考虑借用一个辅助栈 $stack$ ，模拟 压入 / 弹出操作的排列。根据是否模拟成功，即可得到结果。 入栈操作：按照压栈序列的顺序执行； 出栈操作：每次入栈后，循环判断“栈顶元素=弹出序列的当前元素”是否成立，降负荷弹出序列顺序的栈顶元素全部弹出。 算法流程： 初始化：辅助栈 $stack$ ，弹出序列的索引 $i$。 遍历压栈序列：设各元素为 $num$ $num$ 入栈； 循环出栈：若 $stack$ 的栈顶元素=弹出序列元素 $popped[i]$ ，则执行出栈与 i++。 返回值：若 $stack$ 为空，则此弹出序列合法。 123456789101112public boolean validateStackSequences(int[] pushed, int[] popped) &#123; Deque&lt;Integer&gt; stack = new ArrayDeque&lt;&gt;(); int i = 0; for (int num : pushed) &#123; stack.push(num); while (!stack.isEmpty() &amp;&amp; stack.peek() == popped[i]) &#123; stack.pop(); i++; &#125; &#125; return stack.isEmpty();&#125; 时间复杂度 $O(N)$ ： 其中 $N$ 为列表 $pushed$ 的长度；每个元素最多入栈与出栈一次，即最多共 $2N$ 次出入栈操作。 空间复杂度 $O(N)$ ： 辅助栈 $stack$ 最多同时存储 $N$ 个元素。 32(🌲). 从上到下打印二叉树 I题目描述从上到下打印出二叉树的每个节点，同一层的节点按照从左到右的顺序打印。 12345678910给定二叉树: [3,9,20,null,null,15,7] 3 / \\ 9 20 / \\ 15 7输出：[3,9,20,15,7]节点总数 &lt;= 1000 Solution - 层次遍历树的遍历除了前序遍历、中序遍历、后序遍历之外，还有层次遍历，题目描述其实就是对层次遍历的解释。 在层次遍历中使用 $BFS$ 需要用到队列，在 $DFS$ 中往往需要用到栈。 算法流程 特例处理：当树的根节点为空，则直接返回空列表 [] ； 初始化：打印结果列表 res = [] ，包含根节点的队列 queue = [root]； $BFS$ 循环：当队列 queue 为空时跳出； 出队：队首元素出队，记为 node； 打印：将 node.val 添加至列表 tmp 尾部； 添加子节点： 若 node 的左（右）子节点不为空，则将左（右）子节点加入队列 queue。 返回值：返回 res 即可。 123456789101112131415public int[] levelOrder(TreeNode root) &#123; if(root == null) return new int[0]; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;()&#123;&#123; add(root); &#125;&#125;; List&lt;Integer&gt; ans = new ArrayList&lt;&gt;(); while(!queue.isEmpty()) &#123; TreeNode node = queue.poll(); ans.add(node.val); if(node.left != null) queue.add(node.left); if(node.right != null) queue.add(node.right); &#125; int[] res = new int[ans.size()]; for(int i = 0; i &lt; ans.size(); i++) res[i] = ans.get(i); return res;&#125; 时间复杂度 $O(n)$ ： $n$ 为二叉树的节点数量，即 $BFS$ 需循环 $n$ 次。 空间复杂度 $O(n)$ ： 最差情况下，即当树为满二叉树时，最多有 $n/2$ 个树节点同时在 $queue$ 中，使用 $O(n)$ 大小的额外空间。 32(🌲). 从上到下打印二叉树 II题目描述从上到下按层打印二叉树，同一层的节点按从左到右的顺序打印，每一层打印到一行。 12345678910111213141516给定二叉树: [3,9,20,null,null,15,7] 3 / \\ 9 20 / \\ 15 7输出结果:[ [3], [9,20], [15,7]]节点总数 &lt;= 1000 Solution - 层次遍历 本题解法与上题几乎完全相同，唯一的不同点在与 $BFS$ 循环中对每层结点进行分层。 算法流程 特例处理：当树的根节点为空，则直接返回空列表 [] ； 初始化：打印结果列表 res = [] ，包含根节点的队列 queue = [root]； $BFS$ 循环：当队列 queue 为空时跳出； 申请一个用于存储当前层节点的临时列表 tmp； 当前层打印循环：循环次数为 queue.size() 出队：队首元素出队，记为 node； 打印：将 node.val 添加至列表 tmp 尾部； 添加子节点： 若 node 的左（右）子节点不为空，则将左（右）子节点加入队列 queue ； 返回值：返回 res 即可。 1234567891011121314151617public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); while (!queue.isEmpty()) &#123; List&lt;Integer&gt; tmp = new ArrayList&lt;&gt;(); for (int i = queue.size(); i &gt; 0; i--) &#123; TreeNode node = queue.poll(); tmp.add(node.val); if (node.left != null) queue.offer(node.left); if (node.right != null) queue.offer(node.right); &#125; res.add(temp); &#125; return res;&#125; 时间复杂度 $O(n)$ ： $n$ 为二叉树的节点数量，即 $BFS$ 需循环 $n$ 次。 空间复杂度 $O(n)$ ： 最差情况下，即当树为时满，最多有 $n/2$ 个树节点同时在 $queue$ 中，使用 $O(n)$ 大小的额外空间。 32(🌲). 从上到下打印二叉树 III题目描述请实现一个函数按照之字形顺序打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右到左的顺序打印，第三行再按照从左到右的顺序打印，其他行以此类推。 123456789101112131415给定二叉树: [3,9,20,null,null,15,7] 3 / \\ 9 20 / \\ 15 7输出结果:[ [3], [20,9], [15,7]]节点总数 &lt;= 1000 Solution - 层次遍历本题又在上一题的基础上增加了条件 - $Z$ 字型打印，所以我们需要做的就是控制方向。双向链表正好可以完成这项工作，也就是： 在奇数层，将当前层的节点添加到链表尾部； 在偶数层，将当前层的节点添加到链表头部。 算法流程 特例处理：当树的根节点为空，则直接返回空列表 [] ； 初始化：打印结果列表 res = [] ，包含根节点的队列 queue = [root]； $BFS$ 循环：当队列 queue 为空时跳出； 申请一个用于存储当前层节点的临时列表 tmp； 当前层打印循环：循环次数为 queue.size() 出队：队首元素出队，记为 node； 打印：若为奇数层，将 node.val 添加至列表 tmp 尾部；否则添加至头部； 添加子节点： 若 node 的左（右）子节点不为空，则将左（右）子节点加入队列 queue 。 返回值：返回 res 即可。 1234567891011121314151617181920public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); while (!queue.isEmpty()) &#123; LinkedList&lt;Integer&gt; tmp = new LinkedList&lt;&gt;(); for (int i = queue.size(); i &gt; 0; i--) &#123; TreeNode node = queue.poll(); if ((res.size() &amp; 1) == 0) // 位运算判断奇偶性 tmp.addLast(node.val); // 偶数层-&gt; 队列头部 else tmp.addFirst(node.val); if (node.left != null) queue.offer(node.left); if (node.right != null) queue.offer(node.right); &#125; result.add(tmp); &#125; return res;&#125; 时间复杂度 $O(n)$ ： $n$ 为二叉树的节点数量，即 $BFS$ 需循环 $n$ 次。 空间复杂度 $O(n)$ ： 最差情况下，即当树为满二叉树时，最多有 $n/2$ 个树节点同时在 $queue$ 中，使用 $O(n)$ 大小的额外空间。 33(🌲). 二叉搜索树的后序遍历序列题目描述输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历结果。如果是则返回 true，否则返回 false。假设输入的数组的任意两个数字都互不相同。 参考以下这颗二叉搜索树： 12345678 5 / \\ 2 6 / \\ 1 3输入: [1,6,3,2,5]输出: false Solution 1 - 分治 后序遍历定义： [ 左子树 | 右子树 | 根节点 ] ，即遍历顺序为 “左、右、根” 。 二叉搜索树定义： 左子树中所有节点的值 $&lt;$ 根节点的值；右子树中所有节点的值 $&gt;$ 根节点的值；其左、右子树也分别为二叉搜索树。 根据二叉搜索树的定义，可以通过递归，判断所有子树的 正确性 （即其后序遍历是否满足二叉搜索树的定义） ，若所有子树都正确，则此序列为二叉搜索树的后序遍历。 算法流程 终止条件：$i &gt; j$，说明此子树数量 $\\le 1$，无需判断正确性，直接返回 $true$； 递推工作： 划分左右子树：遍历后序遍历的 $[i,j]$ 区间元素，寻找第一个大于根节点的节点，索引记为 $m$ 。此时，可划分出左子树区间 $[i, m-1]$ ，右子树区间 $[m, j-1]$ ，根节点索引 $j$ ； 判断是否为二叉搜索树： 左子树区间 $[i, m - 1]$ 内的所有节点都应 $&lt; postorder[j]$ 。而第 1.划分左右子树 步骤已经保证左子树区间的正确性，因此只需要判断右子树区间即可。 右子树区间 $[m, j-1]$ 内的所有节点都应 $&gt; postorder[j]$。实现方式为遍历，当遇到 $\\leq postorder[j]$ 的节点则跳出；则可通过 $p = j$ 判断是否为二叉搜索树。 返回值： 所有子树都需正确才可判定正确，因此使用 与逻辑符 $&amp;&amp;$ 连接。 $p = j$ : 判断 此树 是否正确； $recur(i, m - 1)$: 判断 此树的左子树 是否正确； $recur(m, j - 1)$: 判断 此树的右子树 是否正确。 下为动图，建议到大佬博客观看，效果更佳。 123456789101112public boolean verifyPostorder(int[] postorder) &#123; return recur(postorder, 0, postorder.length - 1);&#125;boolean recur(int[] postorder, int i, int j) &#123; if(i &gt;= j) return true; int p = i; while(postorder[p] &lt; postorder[j]) p++; int m = p; while(postorder[p] &gt; postorder[j]) p++; return p == j &amp;&amp; recur(postorder, i, m - 1) &amp;&amp; recur(postorder, m, j - 1);&#125; 时间复杂度：$O(n^2)$，每次调用 $recur(i,j)$ 减去一个根节点，因此递归占用 $O(n)$；最差情况下（即当树退化为链表），每轮递归都需遍历树所有节点，占用 $O(n)$ 。 空间复杂度：$O(n)$，最差情况下（即当树退化为链表），递归深度将达到 $n$。 Solution 2 - 单调栈 点击链接自行前往 34(🌲). 二叉树中和为某一值的路径题目描述输入一棵二叉树和一个整数，打印出二叉树中节点值的和为输入整数的所有路径。从树的根节点开始往下一直到叶节点所经过的节点形成一条路径。 给定如下二叉树，以及目标和 $sum = 22$。 123456789101112131415 5 / \\ 4 8 / / \\ 11 13 4 / \\ / \\ 7 2 5 1输出结果:[ [5,4,11,2], [5,8,4,5]]节点总数 &lt;= 10000 Solution - 回溯 本问题是典型的二叉树方案搜索问题，使用回溯法解决，其包含 先序遍历 + 路径记录 两部分。 先序遍历： 按照“根、左、右”的顺序，遍历树的所有节点。 路径记录： 在先序遍历中，当 ① 根节点到叶节点形成的路径 且 ② 路径各节点值的和等于目标值 sum 时，记录此路径。 算法流程 pathSum(root, sum) 函数： 初始化： 结果列表 res ，路径列表 path 。 返回值： 返回 res 即可。 recur(root, tar) 函数： 递推参数： 当前节点 root ，当前目标值 tar 。 终止条件： 若节点 root 为空，则直接返回。 递推工作： 路径更新： 将当前节点值 root.val 加入路径 path ； 目标值更新： tar = tar - root.val（即目标值 tar 从 sum 减至 $0$ ）； 路径记录： 当 ① root 的左 / 右子节点都为空（即 root 为叶节点） 且 ② tar 等于 $0$ （即路径和等于目标值），则将此路径 path 加入 res 。 先序遍历： 递归左 / 右子节点。 路径恢复： 向上回溯前，需要将当前节点从路径 path 中删除，即执行 path.pop() 。 $NOTE:$ 记录路径时若直接执行 res.append(path)，则是将 path 对象加入了 res；后续 path 改变时， res 中的 path 对象也会随之改变。 正确做法：res.append(list(path)) ，相当于复制了一个 path 并加入到 res。 123456789101112131415161718192021public class Solution &#123; private List&lt;List&lt;Integer&gt;&gt; res = new LinkedList&lt;&gt;(); private LinkedList&lt;Integer&gt; path = new LinkedList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; pathSum(TreeNode root, int sum) &#123; recur(root, sum); return res; &#125; private void recur(TreeNode root, int target) &#123; if (root == null) return; path.add(root.val); target -= root.val; if (target == 0 &amp;&amp; root.left == null &amp;&amp; root.right == null) res.add(new LinkedList(path)); recur(root.left, target); recur(root.right, target); &#125;&#125; 时间复杂度 $O(n)$ : $n$ 为二叉树的节点数，先序遍历需要遍历所有节点。 时间复杂度 $O(n)$ : 最差情况下，即树退化为链表时，$path$ 存储所有树节点，使用 $O(n)$ 额外空间。 35. 复杂链表的复制题目描述请实现 copyRandomList 函数，复制一个复杂链表。在复杂链表中，每个节点除了有一个 $next$ 指针指向下一个节点，还有一个 $random$ 指针指向链表中的任意节点或者 $null$。 Solution 1 - 哈希表1234567891011121314151617181920public Node copyRandomList(Node head) &#123; if (head == null) return null; Map&lt;Node, Node&gt; nodeMap = new HashMap&lt;&gt;(); Node p = head; while (p != null) &#123; // 用哈希表存储所有的新旧 结点对，也就是把为每一个结点制作了一个分身。 // 但是分身的 next 和 random 都是空的 nodeMap.put(p, new Node(p.val)); p = p.next; &#125; p = head; while (p != null) &#123; // 新结点的 next 就是 旧结点 next nodeMap.get(p).next = nodeMap.get(p.next); // 新结点的 random 就是 旧结点 random nodeMap.get(p).random = nodeMap.get(p.random); p = p.next; &#125; return nodeMap.get(head);&#125; 时间复杂度：$O(n)$。 空间复杂度：$O(n)$。 Solution 2 - DFS 题解：腐烂的橘子 本题的意思是复制一个链表并返回，这个链表与一般链表不同的是多了一个 $random$ 指针。 在这里，复制的意思是指 深拷贝(Deep Copy)，类似我们常用的“复制粘贴”。 本题链表可以以图的形式来表示，因为图的遍历方式有 $DFS、BFS$ 两种，所以对于此链表而言， $DFS、BFS$ 也应该可行。 深度优先搜索 DFS 从头结点 $head$ 开始拷贝； 由于一个结点可能被多个指针所指向，因此如果该结点已被拷贝，则不需要重复拷贝； 如果还未拷贝当前结点，则创建一个新的结点进行拷贝，并将拷贝过的结点保存在哈希表中； 使用递归拷贝所有的 $next$ 结点，再递归拷贝所有的 $random$ 结点。 1234567891011121314class Solution: def copyRandomList(self, head: 'Node') -&gt; 'Node': def dfs(head): if not head: return None if head in visited: return visited[head] # 创建新结点 copy = Node(head.val, None, None) visited[head] = copy copy.next = dfs(head.next) copy.random = dfs(head.random) return copy visited = &#123;&#125; return dfs(head) 时间复杂度：$O(n)$。 空间复杂度：$O(n)$。 36(🌲). 二叉搜索树与双向链表题目描述输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的循环双向链表。要求不能创建任何新的节点，只能调整树中节点指针的指向。 比如下面这棵树 12345 4 / \\ 2 5 / \\1 3 将这棵二叉搜索树转化为双向循环链表。链表中的每个结点都有一个前驱和后继指针。对于双向循环链表，第一个结点的前驱是最后一个结点，最后一个结点的后继是第一个结点。 下图展示了上面的二叉搜索树转化成的链表。$head$ 表示指向链表中有最小元素的。 特别地，我们希望可以就地完成转换操作。当转化完成以后，树中节点的左指针需要指向前驱，树中节点的右指针需要指向后继。还需要返回链表中的第一个结点的指针。 Solution - 中序遍历 二叉搜索树的中序遍历为 递增序列。 将二叉搜索树转换成一个 “排序的循环双向链表” ，其中包含三个要素： 排序链表： 节点应从小到大排序，因此应使用 中序遍历 “从小到大”访问树的节点； 双向链表： 在构建相邻节点（设前驱节点 $pre$ ，当前节点 $cur$ ）关系时，需要同时满足 $pre.right=cur $ 与 $cur.left=pre$ ； 循环链表： 设链表头结点 $head$ 和尾结点 $tail$，则应当构建 $head.left = tail$ 与 $tail.right=head$ 。 经过以上分析之后，选择使用中序遍历访问树中各节点，并在访问每个节点时构建 $cur$ 和前驱节点 $pre$ 的引用；待到中序遍历结束时，构建首尾节点的引用指向。 算法流程 dfs(cur): 递归法中序遍历； 终止条件： 当节点 $cur$ 为空，代表越过叶节点，直接返回。 递归左子树：即 dfs(cur.left)。 构建链表： 当 pre 为空时：代表正在访问链表头结点，记为 head； 当 pre 不为空时：修改双向结点引用，即 pre.right = cur;，cur.left = pre; 保存 cur：更新 pre = cur，即结点 cur 是后继结点的 pre。 递归右子树，即 dfs(cur.left)。 treeToDoublyList(root) 特例处理：若节点 root 为空，则直接返回； 初始化：空节点 pre ； 转化为双向链表：调用 dfs(root)； 构建循环链表：中序遍历完成后，head 指向头节点， pre 指向尾节点，因此修改 head 和 pre 的双向节点引用即可。 返回值： 返回链表的头节点 head 即可。 12345678910111213141516171819202122public class Solution &#123; private Node pre, head; public Node treeToDoublyList(Node root) &#123; if (root == null) return root; dfs(root); head.left = pre; pre.right = head; return head; &#125; public void dfs(Node cur) &#123; if (cur == null) return; dfs(cur.left); if (pre != null) pre.right = cur; else head = cur; cur.left = pre; pre = cur; dfs(cur.right); &#125;&#125; 时间复杂度：$O(n)$，$n$ 为二叉树的节点个数，中序遍历需要访问所有节点。 空间复杂度：$O(n)$，最差情况下，即树退化为链表时，递归深度达到 $n$。 37(🌲). 序列化二叉树题目描述序列化是将一个数据结构或者对象转换为连续的比特位的操作，进而可以将转换后的数据存储在一个文件或者内存中，同时也可以通过网络传输到另一个计算机环境，采取相反方式重构得到原数据。 请实现两个函数，分别用来序列化和反序列化二叉树。 123456789你可以将以下二叉树： 1 / \\ 2 3 / \\ 4 5序列化为 \"[1,2,3,null,null,4,5]\" Solution - 先序遍历要序列化二叉树，其实就是需要选用一种编码方式对其进行编码，类似的，反序列化就是使用同样的编码方式进行逆操作，进行解码。我们可以边遍历树边对其进行编码，树的遍历方式可以分为 $BFS$ 与 $DFS$，其中 $BFS$ 就是层序遍历；而 $DFS$ 包括前序、中序、后序三种遍历方式。在此选择前序遍历的方式。 serialize(TreeNode root) 初始化：初始化结果 res = &quot;&quot;； 算法流程：从根节点开始遍历，根据遍历到的节点是否为空来决定向 res 中拼接的字符串。 如果当前结点不为空，则将其值拼接到 res 后； 如果当前结点为空，则将 null 拼接到 res 后。 返回值：返回 res。 deserialize(String data) 把 data 进行分割，然后遍历分割得到的元素列表。 如果当前元素为 null，则当前子树为空树； 否则按照 先序遍历 的方式先解析这棵树的左子树，然后再解析它的右子树。 123456789101112131415161718192021222324252627282930313233343536373839public class Codec &#123; // Encodes a tree to a single string. public String serialize(TreeNode root) &#123; return encode(root, \"\"); &#125; private String encode(TreeNode root, String res) &#123; if (root != null) &#123; res += root.val + \",\"; res = encode(root.left, res); res = encode(root.right, res); &#125; else &#123; res += \"null,\"; &#125; return res; &#125; // Decodes your encoded data to tree. public TreeNode deserialize(String data) &#123; String[] nodes = data.split(\",\"); List&lt;String&gt; nodeValues = new ArrayList&lt;&gt;(Arrays.asList(nodes)); return decode(nodeValues); &#125; private TreeNode decode(List&lt;String&gt; nodeValues) &#123; if (\"null\".equals(nodeValues.get(0))) &#123; nodeValues.remove(0); return null; &#125; TreeNode root = new TreeNode(Integer.valueOf(nodeValues.get(0))); nodeValues.remove(0); root.left = decode(nodeValues); root.right = decode(nodeValues); return root; &#125;&#125; 时间复杂度：其中 $n$ 为节点的个数。在序列化与反序列化过程中，访问且仅访问每个节点一次。所以时间复杂度 $O(n)$。 空间复杂度：在序列化与反序列化过程总，递归栈最多达到 $O(n)$。 38. 字符串的排列题目描述输入一个字符串，打印出该字符串中字符的所有排列。 可以以任意顺序返回这个字符串数组，但里面不能有重复元素。 1234输入：s = \"abc\"输出：[\"abc\",\"acb\",\"bac\",\"bca\",\"cab\",\"cba\"]1 &lt;= s 的长度 &lt;= 8 Solution - 回溯排列的数量：对于一个长度为 $n$ 的字符串(假设字符各不相同)，其排列共有 $n!$ 种。 排列的生成方法：根据字符串排列的特点，考虑深度优先搜索所有排列方案。即通过字符交换，先固定 $1$ 位字符( $n$ 种情况)、再固定第 $2$ 位字符( $n-1$ 种情况)、 $\\dots$ 、最后固定第 $n$ 位字符( $1$ 种情况)。 重复方案与剪枝：当字符串存在重复字符时，排列方案中也存在重复方案，为排除重复方案，需在固定某位字符时保证“每种字符在此位只固定一次”，即遇到重复字符时不交换，直接跳过。 递归解析 终止条件：当 $x=len(c)-1$ 时，代表所有位以固定(最后一位只有 $1$ 种情况)，则将当前组合 $c$ 转化为字符串并加入 $res$ ，并返回。 递推参数：当前固定位 $x$ 。 递推工作：初始化一个 $Set$ ，用于排除重复的字符；将第 $x$ 位字符与 $i\\in [x, len(c)]$ 字符分别交换，并进入下层递归。 剪枝：若 $c[i]$ 在 $Set$ 中，代表其实重复字符，因此剪枝； 将 $c[i]$ 加入 $Set$ ，以便之后遇到重复字符时剪枝； 固定字符：将字符 $c[i]$ 和 $c[x]$ 交换，即固定 $c[i]$ 为当前位字符； 开启下层递归：调用 $dfs(x+1)$ ，即开始第 $x+1$ 个字符； 还原交换：将字符 $c[i]$ 和 $c[x]$ 交换(还原之前的交换)。 123456789101112131415161718192021222324252627282930313233public class Solution &#123; private List&lt;String&gt; res = new LinkedList&lt;&gt;(); private char[] c; public String[] permutation(String s) &#123; c = s.toCharArray(); dfs(0); return res.toArray(new String[res.size()]); &#125; private void dfs(int x) &#123; if (x == c.length - 1) &#123; res.add(String.valueOf(c)); // 添加排列方案 return; &#125; HashSet&lt;Character&gt; set = new HashSet&lt;&gt;(); for (int i = x; i &lt; c.length; i++) &#123; if (set.contains(c[i])) continue; // 重复，因此剪枝。 set.add(c[i]); swap(i, x); // 交换，将 c[i] 固定在第 x 位 dfs(x + 1); // 开启固定第 x + 1 位字符 swap(i, x); // 恢复交换 &#125; &#125; private void swap(int a, int b) &#123; char tmp = c[a]; c[a] = c[b]; c[b] = tmp; &#125;&#125; 时间复杂度： $O(N!)$ ， $N$ 为字符串的长度，时间复杂度和字符串排列的方案成线性关系，方案数为 $N!$ ，因此复杂度为 $O(N!)$。 空间复杂度： $O(N^2)$ ，全排列的递归深度为 $N$ ，系统累积使用栈空间大小为 $O(N)$ ，递归中辅助 $Set$ 累计存储的字符数量为 $N+(N-1)+\\dots+2+1$ ，即占用 $O(N^2)$ 的额外空间。 拓展解析 LeetCode-46 LeetCode-47 39. 数组中出现次数超过一半的数字123456数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。你可以假设数组是非空的，并且给定的数组总是存在多数元素。输入: [1, 2, 3, 2, 2, 2, 5, 4, 2]输出: 2限制 1 &lt;= 数组长度 &lt;= 50000 Solution 1 - 快排既然数组中总是存在多数元素，那么多数元素一定至少占了数组的 $1/2$ ，那么数组中间的位置一定是多数元素。 其实只要对数组进行排序即可，但是一般情况下快排的效率最高，所以选择快排。 1234public int majorityElement(int[] nums) &#123; Arrays.sort(nums); return nums[nums.length / 2];&#125; 时间复杂度：$O(n\\lg n)$，快排的时间复杂度。 时间复杂度：$O(\\lg n)$，快排使用分治递归，但是我也没有详细看过 $JDK$ 快排的实现。 Solution 2 - 摩尔投票法摩尔投票法： 票数和： 由于众数出现的次数超过数组长度的一半；若记 众数 的票数为 $+1$ ，非众数 的票数为 $−1$ ，则一定有所有数字的 票数和 $&gt; 0$ 。 票数正负抵消： 设数组 $nums$ 中的众数为 $x$ ，数组长度为 $n$ 。若 $nums$ 的前 $a$ 个数字的 票数和 $=0$ ，则 数组后 $(n−a)$ 个数字的 票数和一定仍 $&gt;0$ （即后 $(n-a)$ 个数字的 众数仍为 $x$ ）。 算法原理： 为构建正负抵消，假设数组首个元素 $n_1$ 为众数，遍历统计票数，当发生正负抵消时，剩余数组的众数一定不变 ，设真正的众数为 $x$ 当 $n_1 = x$：抵消的所有数字中，有一半是众数。 当 $n_1 \\ne x$：抵消的所有数字中，少于或等于一半是众数。 算法流程： 初始化：票数统计 $votes$ ，众数 $majority$。 循环抵消：遍历数组 $nums$ 中的每个元素 $num$ 如果当前票数等于 $0$，那么假设当前数字 $num$ 为众数 $x$； 当 $num = x$ 时，票数 votes++ ，否则，票数 votes--。 返回值：返回众数 $x$ 即可。 123456789public int majorityElement(int[] nums) &#123; int votes = 0, x = 0; for (int num : nums) &#123; if (votes == 0) x = num; votes += num == x ? 1 : -1; &#125; return x;&#125; 时间复杂度：$O(n)$， $n$ 为数组长度。 时间复杂度：$O(1)$，两个变量。 40. 最小的 k​ 个数题目描述输入整数数组 $arr$ ，找出其中最小的 $k$ 个数。例如，输入 $4、5、1、6、2、7、3、8$ 这 $8$ 个数字，则最小的 $4$ 个数字是 $1、2、3、4$。 12输入：arr = [3,2,1], k = 2输出：[1,2] 或者 [2,1] Solution - 快速选择先对数组进行排序，如果是非递减序的话，那么前 $k$ 个元素则为最小的 $k$ 个数。 快速排序的 $partition()$ 方法，会返回一个整数 j 使得 $a[l..j-1] \\le a[j]$，且 $a[j+1..h] \\ge a[j]$，此时 $a[j]$ 就是数组的第 $j$ 大元素。可以利用这个特性找出数组的第 $K$ 个元素，这种找第 $K$ 个元素的算法称为快速选择算法。 1234567public int[] getLeastNumbers(int[] arr, int k) &#123; Arrays.sort(arr); int[] res = new int[k]; for (int i = 0; i &lt; k; i++) res[i] = arr[i]; return res;&#125; 123456789101112131415161718192021222324252627282930313233343536373839/*** 手动快排* 注意找 Top K 问题不需要对整个数组进行快速排序* 例如本题, 直接通过快排切分排好第 k 小的数(下标为 k-1), 那么它左边的数就是比它小的另外 k-1 个数。*/public int[] getLeastNumbers(int[] arr, int k) &#123; if (k == 0 || arr == null || arr.length == 0) return new int[0]; // 最后一个参数表示要找的是下标为 k-1 的数 return quickSearch(arr, 0, arr.length - 1, k -1);&#125;private int[] quickSearch(int[] arr, int l, int r, int k) &#123; int j = partition(arr, l, r); // 每快排切分一次，找到排序后洗标为 j 的元素，如果 j 恰好等于 k 就直接返回 j 及其左边的所有数字。 if (j == k) return Arrays.copyOf(arr, j + 1); // 否则根据下标 k 与 j 的大小关系来决定是继续切分左段还是右段 return j &gt; k ? quickSearch(arr, l, j - 1, k) : quickSearch(arr, j + 1, r, k);&#125;private int partition(int[] arr, int l, int r) &#123; int x = arr[l]; int i = l, j = r + 1; while (true) &#123; while (++i &lt;= r &amp;&amp; arr[i] &lt; x); while (--j &gt;= l &amp;&amp; arr[j] &gt; x); if (i &gt;= j) break; swap(arr, i, j) &#125; arr[l] = arr[j]; arr[j] = x; return j;&#125;private void swap(int arr, int i, int j) &#123; int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp;&#125; 时间复杂度：$O(n)$。 空间复杂度：$O(1)$，不需要额外空间。 Solution 2 - 堆应该使用大顶堆来维护最小堆，而不能直接创建一个小顶堆并设置一个大小，企图让小顶堆中的元素都是最小元素。 维护一个大小为 $K$ 的最小堆过程如下：在添加一个元素之后，如果大顶堆的大小大于 $K$，那么需要将大顶堆的堆顶元素去除。 1234567891011121314public int[] getLeastNumbers(int[] arr, int k) &#123; if (k == 0 || arr == null || arr.length == 0) return new int[0]; PriorityQueue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;&gt;((o1, o2) -&gt; o2 - o1); for (int e : arr) &#123; maxHeap.offer(e); if (maxHeap.size() &gt; k) maxHeap.poll(); &#125; int[] res = new int[k]; int idx = 0; for (int e : maxHeap) res[idx++] = e; return res;&#125; 时间复杂度：$O(n\\lg k)$。 空间复杂度：$O(k)$。 41. 数据流中的中位数题目描述求数据流中的中位数。如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。 设计一个支持以下两种操作的数据结构： void addNum(int num)：从数据流中添加一个整数到数据结构中。 double findMedian()：返回目前所有元素的中位数。 123456789Case 1:Input:[\"MedianFinder\",\"addNum\",\"addNum\",\"findMedian\",\"addNum\",\"findMedian\"][[],[1],[2],[],[3],[]]Output: [null,null,null,1.50000,null,2.00000]Hint: 最多会对 addNum、findMedia 进行 50000 次调用。 Solution 1 - 快排题目中已经说了所有数值排序之后…，那排序首先想到什么呢？ 1234567891011121314151617181920// 勉强通过，面试肯定是不行的。class MedianFinder &#123; private List&lt;Integer&gt; data; /** initialize your data structure here. */ public MedianFinder() &#123; data = new ArrayList&lt;&gt;(); &#125; public void addNum(int num) &#123; data.add(num); &#125; public double findMedian() &#123; Collections.sort(data); // 自然序，从小到大。 int n = data.size(); return (n % 2 == 1) ? data.get(n &gt;&gt; 1) : (data.get((n &gt;&gt; 1) - 1) + data.get(n &gt;&gt; 1)) * 0.5; &#125;&#125; 时间复杂度：$O(n\\lg n)$。 空间复杂度: Collections.sort() 底层快排 $O(\\lg n)$，存储数据 $O(n)$。 Solution 2 - 优先队列/堆在本问题中，需要将数据流保存在列表中，而且需要保证在添加数据时，列表仍然有序。 在此选择维护两个堆，小顶堆 $A$ 与 大顶堆 $B$，两个堆中各保存列表中一半元素。设元素总数 $N = m + n$，其中 $m$ 与 $n$ 分别为 $A$ 和 $B$ 中的元素个数。 小顶堆 $A$ 保存较大的一半元素，长度为 $\\frac{N}{2}$ （$N$ 为偶数）或 $\\frac{N+1}{2}$（$N$ 为奇数）。 大顶堆 $B$ 保存较小的一半元素，长度为 $\\frac{N}{2}$ （$N$ 为偶数）或 $\\frac{N-1}{2}$（$N$ 为奇数）。 然后，中位数可以根据 $A, B$ 的堆顶元素计算得到。 算法流程 addNum(num)： 如果 $m = n$ ：需向 $A$ 添加一个元素。实现方法：先将元素 $num$ 插入 $B$ ，然后取出 $B$ 堆顶插入 $A$ ； 如果 $m \\ne n$ ：需向 $B$ 添加一个元素。实现方法：先将元素 $num$ 插入 $A$ ，然后取出 $A$ 堆顶插入 $B$ 。 dindMedian()： 如果 $m = n$ ：$N$ 为偶数，返回 (A.peek() + B.peek()) / 2; 如果 $m \\ne n$ ：$N$ 为奇数，返回 A.peek()。 123456789101112131415161718192021222324class MedianFinder &#123; private Queue&lt;Integer&gt; A, B; /** initialize your data structure here. */ public MedianFinder() &#123; A = new PriorityQueue&lt;&gt;(); // 默认小顶堆 B = new PriorityQueue&lt;&gt;((x, y) -&gt; (y - x)); // 大顶堆 &#125; public void addNum(int num) &#123; if (A.size() == B.size()) &#123; B.offer(num); A.offer(B.poll()); &#125; else &#123; A.offer(num); B.offer(A.poll()); &#125; &#125; public double findMedian() &#123; return A.size() == B.size() ? (A.peek() + B.peek()) / 2.0 : A.peek(); &#125;&#125; 时间复杂度：添加元素 $O(log n)$，获取中位数 $O(1)$。 空间复杂度：$O(N)$，存储数据流中的元素。 42. 连续子数组的最大和输入一个整型数组，数组里有正数也有负数。数组中的一个或连续多个整数组成一个子数组。求所有子数组的和的最大值。 要求时间复杂度为 $O(n)$。 Solution - 动态规划动态规划解析： 状态定义：设动态规划列表为 $dp$ ， $dp[i]$ 代表以元素 $nums[i]$ 为结尾的连续子数组最大和。 转移方程：若 $dp[i-1] \\le 0$ ，说明 $dp[i-1]$ 产生负贡献，即 $dp[i-1]+nums[i]$ 还不如 $nums[i]$ 本身大。 $$dp[i] =\\begin{cases}dp[i-1]+nums[i] &amp;, dp[i-1]&gt;0 \\nums[i] &amp;, dp[i-1]\\le 0\\end{cases}$$ 初始状态：$dp[0]=nums[0]$ ，即以 $nums[0]$ 结尾的连续子数组最大和为 $nums[0]$。 返回值：$dp$ 中的最大值，即全局最大值。 空间复杂度降低： 由于 $dp[i]$ 只与 $dp[i-1]$ 和 $nums[i]$ 有关系，所以可考虑原地修改。 12345678public int maxSubArray(int[] nums) &#123; int res = nums[0]; for (int i = 1; i &lt; nums.length; i++) &#123; nums[i] += Math.max(nums[i - 1], 0); res = Math.max(res, nums[i]); &#125; return max;&#125; 时间复杂度：$O(n)$。 空间复杂度：$O(1)$。 43. 1~n整数中1出现的次数题目描述输入一个整数 $n$ ，求 $1\\sim n$ 这 $n$ 个整数的十进制表示中 $1$ 出现的次数。 12345输入：n = 12输出：5解释：1～12 这些整数中包含 1 的数字有 1、10、11 和 12，1 一共出现了 5 次。1 &lt;= n &lt; 2^31 Solution - 递推将 $1\\sim n$ 的个位、十位、百位、…的 $1$ 出现的次数相加，即为 $1$ 出现的总次数。 设数字 $n$ 是 $x$ 位数，记 $n$ 的第 $i$ 位为 $n_i$ ，则可将 $n$ 表示为 $n_xn_{x-1}\\cdots n_2n_1$ : 称 $n_i$ 为当前位，记为 $cur$ ； 将 $n_{i-1}n_{i-2}\\cdots n_2n_1$ 称为低位，记为 $low$ ； 将 $n_xn_{x-1}\\cdots n_{i+1}n_{i+1}$ ，记为 $high$ ； 将 $10^i$ 称为位因子，记为 $digit$ 。 某位中 $1$ 出现次数的计算方法： 当 $cur=0$ 时，此位 $1$ 的出现次数只由高位 $high$ 决定，表达式为 $high\\times digit$。 当 $cur=1$ 时，此位 $1$ 的出现次数由高位 $high$ 和低位 $low$ 决定，表达式为 $high \\times digit + low + 1$。 当 $cur=2,3,4,\\cdots,9$ 时，此位 $1$ 出现的次数只由高位 $high$ 决定，表达式为 $(high+1)\\times digit$。 1234567891011121314public int countDigitOne(int n) &#123; int digit = 1, res = 0; int high = n / 10, cur = n % 10, low = 0; while (high != 0 || cur != 0) &#123; if (cur == 0) res += high * digit; else if (cur == 1) res += high * digit + low + 1; else res += (high + 1) * digit; low += cur * digit; cur = high % 10; high /= 10; digit *= 10; &#125; return res;&#125; 时间复杂度：$O(\\lg n)$，循环内的计算操作使用 $O(1)$，循环次数为 $O(\\log_{10} n)$。 空间复杂度：$O(1)$，有限的几个变量。 44. 数字序列中某一位的数字题目描述数字以 $0123456789101112131415 \\dots$ 的格式序列化到一个字符序列中。在这个序列中，第 $5$ 位(从下标 $0$ 开始计数)是 $5$，第 $13$ 位是 $1$，第 $19$ 位是 $4$，等等。 Solution - 迭代 将 $101112\\dots$ 中的每一位称为数位，记为 $n$ ； 将 $10,11,12,\\dots$ 称为 数字，记为 $num$ ； 数字 $10$ 是一个两位数，称此数字的位数为 $2$ ，记为 $digit$ ； 每 $digit$ 位数的起始数字（即： $1,10,100,\\dots$ ），记为 $start$。 数字范围 位数 数字数量 数位数量 $1~9$ $1$ $9$ $9$ $10~99$ $2$ $90$ $180$ $100~999$ $3$ $900$ $2700$ $\\dots$ $\\dots$ $\\dots$ $\\dots$ $start \\sim end$ $digit$ $9 \\times start$ $9\\times start\\times digit$ 位数递推公式：$digit=digit+1$； 起始数字递推公式：$start=10\\times start$； 数位数量计算公式：$count=9\\times start \\times digit$； 可推出各 $digit$ 下的数位数量 $count$ 的计算公式：$count=9\\times start \\times digit$。 经过以上分析，可将求解分为三步： 确定 $n$ 所在数字的位数，记为 $digit$ ； 确定 $n$ 所在的数字，记为 $num$ ； 确定 $n$ 是 $num$ 中的哪一数位，并返回结果。 12345678910111213public int findNthDigit(int n) &#123; int digit = 1; long start = 1; long count = 9; while (n &gt; count) &#123; // 1 n -= count; digit += 1; start *= 10; count = 9 * start * digit; &#125; long num = start + (n - 1) / digit; // 2 return Long.toString(num).charAt((n - 1) % digit) - '0'; // 3&#125; 时间复杂度： $O(\\lg n)$ ，所求数位 $n$ 对应数字 $num$ 的位数 $digit$ 最大为 $O(\\lg n)$ ；第一步最多循环 $O(\\lg n)$ 次；第三步中将 $num$ 转化为字符串使用 $O(\\lg n)$ 时间。 空间复杂度： $O(\\lg n)$ ，将数字 $num$ 转化为字符串 $str$ ，占用 $O(\\lg n)$ 的存储空间。 45. 把数组排成最小的数题目描述输入一个非负整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。 Solution - 自定义排序 求拼接起来的最小数字，本质上是一个排序问题。 判断规则：设 $nums$ 任意两数字的字符串格式 $x$ 和 $y$ ，则 若拼接字符串 $x+y&gt;y+x$ ，则 $m&gt;n$ ; 反之，若 $x+y&lt;y+x$ ，则 $n&lt;m$ . 根据以上规则，套用任何排序方法对 $nums$ 排序即可。 算法流程 初始化：字符串列表 $strs$ ，保存各数字的字符串格式； 列表排序：应用以上“排序判断规则”，对 $strs$ 执行排序； 返回值：拼接 $strs$ 的所有字符串 123456789101112131415161718192021222324252627public String minNumber(int[] nums) &#123; String[] strs = new String[nums.length]; for (int i = 0; i &lt; nums.length; i++) strs[i] = String.valueOf(nums[i]); fastSort(strs, 0, strs.length - 1); StringBuilder res = new StringBuilder(); for (String s : strs) res.append(s); return res.toString();&#125;public void fastSort(String[] strs, int l, int r) &#123; if (l &gt;= r) return; int i = l, j = r; String tmp = strs[i]; while (i &lt; j) &#123; while ((strs[j] + strs[l]).compareTo(strs[l] + strs[j]) &gt;= 0 &amp;&amp; i &lt; j) j--; while ((strs[i] + strs[l]).compareTo(strs[l] + strs[i]) &lt;= 0 &amp;&amp; i &lt; j) i++; tmp = strs[i]; strs[i] = strs[j]; strs[j] = tmp; &#125; strs[i] = strs[l]; strs[l] = tmp; fastSort(strs, l, i - 1); fasrSort(strs, i + 1, r);&#125; 时间复杂度：$O(N\\lg N)$，$N$为最终返回值的字符数量 $strs$ 列表的长度 $\\le N$ ；使用快排或内置函数的平均时间复杂度为 $O(N\\lg N)$ ，最差为 $O(N^2)$。 空间复杂度： $O(N)$ ，字符串列表 $strs$ 占用线性大小的额外空间。 46. 把数字翻译成字符串题目描述给定一个数字，我们按照如下规则把它翻译为字符串：$0$ 翻译成 “$a$” ，$1$ 翻译成 “$b$”，$\\dots$，$11$ 翻译成 “$l$”，$\\dots$，$25$ 翻译成 “$z$”。一个数字可能有多个翻译。请编程实现一个函数，用来计算一个数字有多少种不同的翻译方法。 12345输入: 12258输出: 5解释: 12258有5种不同的翻译，分别是\"bccfi\", \"bwfi\", \"bczi\", \"mcfi\"和\"mzi\"0 &lt;= num &lt; 2^31 动态规划解析假设数字序列为 $num=x_1x_2\\dots x_{i-2}x_{i-1}x_i\\dots x_{n-1}{x_n}$ ，例如 $10086=x_1x_2x_3x_4x_5$ 。可以看到的有的数字组合有两种方式，比如 $10086$ 开头的 $1$ 可以单独翻译，但是也可以前两个数字组合 $10$ 一起翻译。 假设 $x_1x_2\\dots x_{i-2}$ 的翻译方案数量为 $f(i-2)$； 假设 $x_1x_2\\dots x_{i-2}x_{i-1}$ 的翻译方案数量为 $f(i-1)$。 所以，从末尾来看， $x_{i-2}$ 可以单独翻译，也可以跟 $x_{i-1}$ 一起作为 $x_{i-2}x_{i-1}$ 一起翻译，由排列组合中的“分类相加，分步相乘”可以得到最后结果为两种情况之和，递归关系为：$$f(i)=\\begin{cases}f(i-2)+f(i-1) &amp;, 若\\ x_{i-1}x_i \\ 可整体翻译 \\f(i-1) &amp;, 若\\ x_{i-1}x_i \\ 不可整体翻译\\end{cases}$$动态规划 状态定义：设动态规划列表为 $dp$ , $dp[i]$ 代表以 $x_i$ 为结尾的数字的翻译方案数量。 状态转移： 若 $x_{i-1}$ 和 $x_i$ 组成的两位数字不可整体翻译，则 $dp[i]=dp[i-1]$； 若 $x_{i-1}$ 和 $x_i$ 组成的两位数字可以整体翻译，则 $dp[i]=dp[i-1]+dp[i-2]$ ，当 $x_{i-1}=0$ 时，组成的两位数是无法被翻译的，例如（$00,01\\dots 09$），因此可翻译的两位数区间为 $[10,25]$。$$dp[i]=\\begin{cases}dp[i-1]+dp[i-2] &amp;, 10x_{i-1}+x_i \\in [10,25] \\dp[i-1] &amp;, 10x_{i-1}+x_i \\in [0,10) \\cup (25,99]\\end{cases}$$ 初始值： $dp[0]=dp[1]=1$ ，当 $num$ 第 $1,2$ 位的数字属于 $[10,25)$ 时显然有两种翻译方案，即 $dp[2]=dp[1]+dp[0]=2$ ，而 $dp[1]=1$ ，所以 $dp[0]=1$ 。 返回值： $dp[n]$。 Solution 1 -字符串遍历 为方便获取数字的各位 $x_i$ ，考虑先将数字 $num$ 转化为字符串 $s$ ，通过遍历 $s$ 实现动态规划。 通过字符串切片 $s[i-2:i]$ 获取数字组合 $10x_{i-1}+x_i$ ，通过对比字符串 $ASCII$ 码判断字符串对应的数字区间。 空间使用优化：由于 $dp[i]$ 只与 $dp[i-1]$ 有关，因此可使用两个变量 $a,b$ 分别记录 $dp[i],dp[i-1]$ ，两变量交替前进即可。此方法可省去 $dp$ 列表使用的 $O(N)$ 的额外空间。 1234567891011public int translateNum(int num) &#123; String s= String.valueOf(num); int a = 1, b = 1; for (int i = 2; i &lt;= s.length(); i++) &#123; String tmp = s.substring(i - 2, i); int c = tmp.compareTo(\"10\") &gt;= 0 &amp;&amp; tmp.compareTo(\"25\") &lt;= 0 ? a + b : a; b = a; a = c; &#125; return a;&#125; 时间复杂度：$O(n)$，需要遍历一遍字符数组。 空间复杂度：$O(n)$。 Solution 2 - 数字求余上述方法虽然已经节省了 $dp$ 列表的空间占用，但是字符串仍然使用了 $O(N)$ 大小的额外存储空间。 空间复杂度优化 利用求余运算 $num%10$ 和求整运算 $num/10$ ，可获取数字 $num$ 的各位数字（获取顺序为个位、十位、百位…） 因此，可通过 求余 和 求整 运算实现从右向左的遍历计算。 字符串 $s$ 的空间占用也被省去，空间复杂度从 $O(N)$ 降至 $O(1)$ 。 12345678910111213public int translateNum(int num) &#123; int a = 1, b = 1, x,y = num % 10; while (nums != 0) &#123; num /= 10; x = num % 10; int tmp = 10 * x + y; int c = (tmp &gt;= 10 &amp;&amp; tmp &lt;= 25) ? a + b : a; b = a; a = c; y = x; &#125; return a;&#125; 47. 礼物的最大价值题目描述在一个 $m\\times n$ 的棋盘的每一格都放有一个礼物，每个礼物都有一定的价值(价值大于 $0$)。你可以从棋盘的左上角开始拿格子里的礼物，并每次向右或者向下移动一格、直到到达棋盘的右下角。给定一个棋盘及其上面的礼物的价值，请计算你最多能拿到多少价值的礼物？ 1234567891011输入:[ [1,3,1], [1,5,1], [4,2,1]]输出: 12解释: 路径 1→3→5→2→1 可以拿到最多价值的礼物。0 &lt; grid.length &lt;= 2000 &lt; grid[0].length &lt;= 200 Solution - 动态规划从题目要求可以发现，某单元格只能从其上方或者左方的单元格到达。设 $f(i,j)$ 为从棋盘左上角走到单元格 $(i,j)$ 的最大累积价值，易得以下地推关系： $f(i,j)$ 等于 $f(i,j-1)$ 和 $f(i-1,j)$ 中的较大值加上当前单元格礼物价值 $grid(i,j)$ .$$f(i,j)=max{f(i,j-1), f(i-1,j)}+grid(i,j)$$ 状态定义：设动态规划矩阵 $dp$ ， $dp(i,j)$ 代表从棋盘的左上角开始，到达单元格 $(i,j)$ 时能拿到礼物的最大累积价值。 状态转移 当 $i=0$ 且 $j=0$ 时，为左上角的起始元素； 当 $i=0$ 且 $j \\ne 0$ 时，为矩阵第一行元素，每个单元格只可由其左边到达； 当 $i\\ne 0$ 且 $j=0$ 时，为矩阵第一列元素，每个单元格只可由其上边到达； 当 $i \\ne 0$ 且 $j\\ne 0$ 时，可从左边或者上边到达。$$dp(i,j)=\\begin{cases}grid(i,j) &amp;, i=0,j=0 \\grid(i,j) + dp(i,j-1) &amp;, i=0,j \\ne 0 \\grid(i,j) + dp(i-1,j)&amp;, i\\ne0,j=0 \\grid(i,j) + dp(i-1,j-1)&amp;, i\\ne0,j\\ne0\\end{cases}$$ 初始状态： $dp[0][0]=grid[0][0]$ ，即初始价值为左上角的元素。 返回值： $dp[m-1][n-1]$ ， $m,n$ 分别为矩阵的行和列，即返回 $dp$ 矩阵右下角的值。 空间复杂度优化：因为 $dp[i][j]$ 只与 $dp[i-1,j,dp[i][j-1]], grid[i][j]$ 有关系，因此可不用申请 $dp$ 矩阵的空间，直接在原矩阵上原地修改即可，从而将空间复杂度从 $O(MN)$ 降低至 $O(1)$。 1234567891011public int maxValue(int[][] grid) &#123; int rows = grid.length, cols = grid[0].length; for (int col = 1; col &lt; cols; col++) // 初始化第一行 grid[0][col] += grid[0][col - 1]; for (int row = 1; row &lt; rows; row++) // 初始化第一列 grid[row][0] += grid[row - 1][0]; for (int row = 1; row &lt; rows; row++) for (int col = 1; col &lt; cols; j++) grid[row][col] += Math.max(grid[row - 1][col], grid[row][col - 1]); return grid[rows - 1][cols - 1];&#125; 时间复杂度：$O(MN)$。 空间复杂度：$O(1)$。 48. 最长不含重复字符的子字符串题目描述从字符串中找出一个最长的不包含重复字符的子字符串，计算该最长子字符串的长度。 123456789输入: \"abcabcbb\"输出: 3解释: 因为无重复字符的最长子串是 \"abc\"，所以其长度为 3。输入: \"bbbbb\"输出: 1解释: 因为无重复字符的最长子串是 \"b\"，所以其长度为 1。s.length &lt;= 40000 动态规划解析 状态定义：设动态规划列表为 $dp$ , $dp[j]$ 代表以字符 $s[j]$ 为结尾的“最长不重复子字符串”的长度。 状态转移：固定右边界 $j$ ，设字符 $s[j]$ 左边距离最近的相同字符为 $s[i]$ ，即 $s[i]=s[j]$ 。 当 $i &lt;0$ ，即 $s[j]$ 左边无相同字符，则 $dp[j] = dp[j-1]+1$ ; 当 $dp[j-1]&lt;j-i$ ，说明 $s[i]$ 在字符串 $dp[j-1]$ 区间之外，则 $dp[j]=dp[j-1]+1$ ; 当 $dp[j-1]\\ge j-i$ ，说明 $s[i]$ 在子字符串 $dp[j-1]$ 区间之内，则 $dp[j]$ 的左边界由 $s[i]$ 来决定，即 $dp[j]=j-i$ 。 当 $i&lt;0$ 时，由于 $dp[j-1]\\le j$ 恒成立，因而 $dp[j-1]&lt;j-i$ 恒成立，因此上述三种情况可以合并为：$$dp[j] =\\begin{cases}dp[j-1]+1 &amp;, dp[j-1] &lt; j-i\\j-i &amp;, dp[j-1]\\ge j-i\\end{cases}$$ 初始状态： $dp[0]=1$ ，即初始不重复子串的长度为 $1$ 。 返回值： $max(dp)$ ，即全局的“最长不重复子串”的长度。 观察状态转移方程，可以发现问题转化为：每轮遍历到字符 $s[j]$ 时，如何计算左边界索引 $i$ ？由于返回值是 $dp$ 中的最大值，因此可借助遍历 $tmp$ 存储 $dp[j]$ ，变量 $res$ 每轮更新最大值即可。 Solution - 1 动态规划 哈希表map：存储字符及其最后一次出现的索引位置。 左边界获取方式：遍历到 $s[j]$ 时，通过 map.get(s[j]) 获取左边界位置。 1234567891011public int lengthOfLongestSubstring(String s) &#123; Map&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); int res = 0, tmp = 0; for (int j = 0; j &lt; s.length(); j++) &#123; int i = map.getOrDefault(s.charAt(j), -1); dic.put(s.charAt(j), j); tmp = tmp &lt; j - i ? tmp + 1 : j - i; // dp[j - 1] -&gt; dp[j] res = Math.max(res, tmp); &#125; return res;&#125; 时间复杂度：$O(n)$，需要遍历一遍字符数组。 空间复杂度：$O(128)=O(1)$，字符的 $ASCII$ 的范围为 $0 \\sim 127$。 Solution - 2 双指针 本质上与方法一类似，不同点在于左边界 $i$ 的定义。 哈希表 map：存储字符及其最后一次出现的索引位置。 左指针 $i$ ：根据上轮左指针 $i$ 和 $map.get(s[j])$ ，每轮更新左边界，保证 $[i+1, j]$ 内无重复字符且最大。$$i= max{ map.get[s[j], i] }$$ 更新结果 $res$ ：取上轮 $res$ 和本轮双指针区间 $[i+1, j]$ 的宽度（即 $j-i$ ）中的最大值$$res = max{res, j-i}$$ 1234567891011public int lengthOfLongestSubstring(String s) &#123; Map&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); int leftIndex = -1, res = 0; for (int j = 0; j &lt; s.length(); j++) &#123; if (map.containsKey(s.charAt(j))) leftIndex = Math.max(leftIndex, map.get(s.charAt(j))); // 更新左指针 i map.put(s.charAt(j), j); // 哈希表记录 res = Math.max(res, j - leftIndex); &#125; return res;&#125; 时间复杂度：$O(n)$，需要遍历一遍字符数组。 空间复杂度：$O(128)=O(1)$，字符的 $ASCII$ 的范围为 $0 \\sim 127$。 49. 丑数题目描述我们把只包含质因子 $2、3$ 和 $5$ 的数称作丑数（$Ugly\\ Number$）。求按从小到大的顺序的第 $n$ 个丑数。 123456输入: n = 10输出: 12解释: 1, 2, 3, 4, 5, 6, 8, 9, 10, 12 是前 10 个丑数。1 是丑数。n 不超过1690。 Solution - 动态规划 丑数只包含因子 $2,3,5$ ，因此有 丑数 $=$ 某较小丑数 $\\times$ 某因子。 有些疑惑 $8$ 为什么也是丑数， $8=2\\times4$ ，这可不是只包含因子 $2,3,5$ 。但是同样的 $8=2\\times2\\times2$ ，此时可以满足因子只包含 $2$ 。(强行解释，所以丑数也许还可以理解为 若干个 $2$ 乘以若干个 $3$ 乘以若干个 $5$，即 $ugly = 2^x \\times 3^y \\times 5^z$)。 设已知长度为 $n$ 的丑数序列 $x_1,x_2,\\dots,x_n$ ，求第 $n+1$ 个丑数 $x_{n+1}$ 。根据递推性质，丑数 $x_{n+1}$ 只可能是以下三种情况之一（索引 $a,b,c$ 为未知数）：$$x_{n+1} =\\begin{cases}x_a \\times 2, &amp; a \\in [1,n] \\x_b \\times 3, &amp; b \\in [1,n] \\x_c \\times 5, &amp; c \\in [1,n]\\end{cases}$$由于 $x_{n+1}$ 是最接近 $x_n$ 的丑数，因此索引 $a,b,c$ 需满足以下条件：$$\\begin{cases}x_a \\times 2 &gt; x_n \\ge x_{a-1} \\times 2, &amp; 即\\ x_a\\ 为首个乘以\\ 2\\ 后大于\\ x_n\\ 的丑数 \\x_b \\times 3 &gt; x_n \\ge x_{c-1} \\times 3, &amp; 即\\ x_b\\ 为首个乘以\\ 3\\ 后大于\\ x_n\\ 的丑数 \\x_c \\times 5 &gt; x_n \\ge x_{c-1} \\times 5, &amp; 即\\ x_c\\ 为首个乘以\\ 5\\ 后大于\\ x_n\\ 的丑数\\end{cases}$$若索引 $a,b,c$ 满足以上条件，则可使用递推公式计算下个丑数 $x_{n+1}$ ，其为三种情况中的最小值，即：$$x_{n+1} = min{x_a\\times2,x_b\\times3,x_c\\times5}$$因此，可设置指针 $a,b,c$ 指向首个丑数(即 $1$ )，循环根据递推公式得到下个丑数，并每轮将对应指针执行 $+1$ 操作。 动态规划解析 状态定义：设动态规划列表为 $dp$ ， $dp[i]$ 代表第 $i+1$ 个丑数。 状态转移： 当索引 $a,b,c$ 满足以下条件时， $dp[i]$ 为三种情况的最小值； 每轮计算 $dp[i]$ 后，需要更新索引 $a,b,c$ 的值，使其始终满足方程条件。实现方法：分别独立判断 $dp[i]$ 和 $dp[a]\\times2,dp[b]\\times3,dp[c]\\times5$ 的大小关系，若相等则将对应索引 $a,b,c$ 加 $1$ 。$$\\begin{cases}dp[a] \\times 2 &gt; dp[i-1] \\ge dp[a-1] \\times 2 \\dp[b] \\times 3 &gt; dp[i-1] \\ge dp[b-1] \\times 3 \\dp[c] \\times 5 &gt; dp[i-1] \\ge dp[c-1] \\times 5\\end{cases} \\dp[i] = min{dp[a]\\times2, dp[b]\\times3,dp[c]\\times5 }$$ 初始状态： $dp[0] = 1$ ，即第一个丑数为 $1$ 。 返回值： $dp[n-1]$ , 即返回第 $n$ 个丑数。 12345678910111213public int nthUglyNumber(int n) &#123; int a = 0, b = 0, c = 0; // 初始下标索引 int[] dp = new int[n]; // dp 数组 dp[0] = 1; // 初始化 for (int i = 1; i &lt; n; i++) &#123; int n2 = dp[a] * 2, n3 = dp[b] * 2, n5 = dp[c] * 5; dp[i] = Math.min(Math.min(n2, n3), n5); if (dp[i] == n2) a++; if (dp[i] == n3) b++; if (dp[i] == n5) c++; &#125; return dp[n - 1];&#125; 时间复杂度：$O(N)$，其中 $N = n$ ，动态规划需要遍历计算 $dp$ 列表。 空间复杂度： $O(N)$ ，长度为 $N$ 的 $dp$ 列表使用 $O(N)$ 的辅助空间。 50. 第一个只出现一次的字符题目描述在字符串 s 中找出第一个只出现一次的字符。如果没有，返回一个单空格。 $s$ 只包含小写字母。 12345s = \"abaccdeff\"返回 \"b\"s = \"\"返回 \" \" Solution - 计数吐槽一句，当时面试映客直播时[电话面]，第一次在面试中问算法，第一道题 $LRU$ 算法，第二道就是这题，然后我几乎不假思索地就说出了这个方法: 先申请一个 $256$ 长度的辅助数组（$256$ 是扩展的 $ASCII$ 码数量）。 扫描字符串，以字符串中的字符的 $ASCII$ 码为下标，每出现一次将下标上的值加 $1$。 再扫描一遍字符数组，仍然用字符 $ASCII$ 码做下标去检查是否为 $1$，是的话则直接返回。扫描结束后无结果则返回空字符串。 如果手写代码的话应该是没问题的，但是当时被面试官一直追着问，没给仔细思考的时间，没有手写代码，然后她给我大概举了个 &quot;cbbadfdf&quot; 的例子。 我本意是按字符的 $ASCII$ 码作为下标去辅助数组中查找的，结果被带的脑子有点乱，在临时讲思路的时候说成了从头到尾扫描，因为&quot;c、a&quot;都出现了一次，&quot;a&quot;的 $ASCII$ 为 $97$ 在 &quot;c&quot; 的 $ASCII$ 码 $99$ 前面，把自己绕进去了。 $NOTE$ : 在面试中问到算法不要急，如果面试官给时间的话最好把代码手写出来，或者写出大概步骤，如果一直追着问不给时间的话多半是已经凉了。 1234567891011public char firstUniqChar(String s) &#123; if (s.length() == 0) return ' '; int[] count = new int[26]; // 题目里说只包含小写字母 char[] chars = s.toCharArray(); for (char c : chars) count[c]++; for (char c : chars) if (count[c] == 1) return c; return ' ';&#125; 时间复杂度：$O(n)$，遍历了一遍字符串。 空间复杂度：将字符串转化成字符数组占用 $O(n)$，辅助数组 $count$ 占用 $O(1)$，总体 $O(n)$。其实也可以不使用 $chars$ 这个辅助数组。 Solution 2 - 无序哈希算法流程 初始化 $HashMap$，记为 $dic$； 遍历字符串 如果字符 $key$ 已在 $dic$ 存在，将其对应的 $value$ 置为 $false$，代表出现了不止一次； 如果字符 $key$ 未在 $dic$ 存在，将其对应的 $value$ 置为 $true$，代表至今只出现了一次。 再次遍历字符串，以字符作为 $key$ 查找 $value$ 是否为 $true$。 为 $true$，代表只出现了一次，返回结果。 为 $false$，代表出现了不止一次；若全为 $false$，则返回 &quot; &quot;。 12345678910public char firstUniqChar(String s) &#123; // 在此选择 Boolean 代替 Integer，减小空间。 Map&lt;Character, Boolean&gt; dic = new HashMap&lt;&gt;(); char[] sc = s.toCharArray(); for(char c : sc) dic.put(c, !dic.containsKey(c)); for(char c : sc) if(dic.get(c)) return c; return ' ';&#125; 时间复杂度：$O(n)$。 空间复杂度：$O(n)$。 Solution 3 - 有序哈希思路与无序哈希相同，时空复杂度也相同，只是用 $LinkedHashMap$ 减少了再次遍历的次数，即时间复杂度的常数项。 12345678910public char firstUniqChar(String s) &#123; Map&lt;Character, Boolean&gt; dic = new LinkedHashMap&lt;&gt;(); char[] sc = s.toCharArray(); for(char c : sc) dic.put(c, !dic.containsKey(c)); for(Map.Entry&lt;Character, Boolean&gt; d : dic.entrySet()) if(d.getValue()) return d.getKey(); return ' ';&#125; 时间复杂度：$O(n)$。 空间复杂度：$O(n)$。 51. 数组中的逆序对题目描述在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组，求出这个数组中的逆序对的总数。 1234输入: [7,5,6,4]输出: 50 &lt;= 数组长度 &lt;= 50000 Solution 1 - 归并排序使用暴力搜索思想的代码很容易写出来，但是看数组规模最大为 $50000$，因为 $O(n^2)$ 的代码不可取。 归并排序「归并排序」是分治思想的典型应用，它包含三个步骤： 分解：待排序的区间为 $[l,\\ r]$，令 $m=\\lfloor \\frac{l+r}{2} \\rfloor$，将区间分为 $[l, m]$ 与 $[m + 1, r]$； 解决：使用归并排序递归地排序两个子序列； 合并：把两个已经排序好的子序列 $[l, m]$ 和 $[m + 1, r]$ 进行合并。在待排序序列长度为 $1$ 时，递归开始回退。 具体思路将「归并排序」与「求逆序对」相关联的关键就在于「归并」中「并」的过程。通过如下实例来理解。假设有两个已排序的序列等待合并，这两个序列分别是 $L={8,12,16,22,100 }$ 和 $R={ 9,26,55,64,91 }$。使用 lPtr 与 rPtr 分别指向 $L、R$ 首部第一个元素。记已经合并好的部分为 $M$。 123L = [8, 12, 16, 22, 100] R = [9, 26, 55, 64, 91] M = [] | | lPtr rPtr 可以发现 $L[lPtr]=8 &lt; R[rPtr]=9$，于是将 $lPtr$ 指向的元素加入 $M$。 123L = [8, 12, 16, 22, 100] R = [9, 26, 55, 64, 91] M = [8] | | lPtr rPtr 把 $8$ 加入 $M$ 之后发现 $R$ 中没有数字比 $8$ 小，所以 $8$ 对于统计数组中「逆序对数量」的贡献为 $0$ 。继续下一次迭代，将 $9$ 加入 $M$ ，此时 $L[lPtr]=12 &lt; R[rPtr]=26$ 。于是将 $12$ 加入 $M$ ，它对「逆序对数量」的贡献为：lPtr 相对于 $R$ 中首位置偏移量，即 $lPtr$ 自身。（此时 $R$ 中位于 rPtr 左边的数字均比 $12$ 小，即构成「逆序对」，而此时只有 $9$ 。） 123L = [8, 12, 16, 22, 100] R = [9, 26, 55, 64, 91] M = [8, 9] | | lPtr rPtr 「贡献度」 的计算方式当前 lPtr 指向的数字比 rPtr 小，但是比 $R$ 中 [0 ... (rPtr - 1)] 的其他数字大，[0 ... (rPtr - 1)] 的其他数字本应当排在 lPtr 对应数字的左边，但是它排在了右边，所以这里就贡献了 rPtr 个逆序对。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public int reversePairs(int[] nums) &#123; if (nums == null || nums.length &lt; 2) return 0; int len = nums.length; int[] copy = Arrays.copyOf(nums, len); int[] tmp = new int[len]; return reversePairs(copy, 0, len - 1, tmp);&#125;private int reversePairs(int[] nums, int left, int right, int[] tmp) &#123; if (left == right) return 0; int mid = (left + right) &gt;&gt;&gt; 1; int leftPairs = reversePairs(nums, left, mid, tmp); int rightPairs = reversePairs(nums, mid + 1, right, tmp); if (nums[mid] &lt;= nums[mid + 1]) return leftPairs + rightPairs; int crossPairs = mergeAndCount(nums, left, mid, right, tmp); return leftPairs + rightPairs + crossPairs;&#125;private int mergeAndCount(int[] nums, int left, int mid, int right, int[] tmp) &#123; for (int i = left; i &lt;= right; i++) tmp[i] = nums[i]; int i = left, j = mid + 1; int count = 0; for (int k = left; k &lt;= right; k++) &#123; if (i == mid + 1) &#123; nums[k] = tmp[j++]; &#125; else if (j == right + 1) &#123; nums[k] = tmp[i++]; &#125; else if (tmp[i] &lt;= tmp[j]) &#123; nums[k] = tmp[i++]; &#125; else &#123; nums[k] = tmp[j++]; count += (mid - i + 1); &#125; &#125; return count;&#125; 时间复杂度： $O(n\\lg n)$ ，利用到归并排序。 空间复杂度： $O(n)$ ，除去递归栈之外，还用到了临时数组 $tmp$ 。 Solution 2 - 离散化树状数组 点击链接自行查看 52. 两个链表的第一个公共节点题目描述输入两个链表，找出它们的第一个公共节点。如下面的两个链表在 $c1$ 相交。 Solution - 双指针 看到这段代码就想起一段话：“听闻远方有你，动身跋涉千里。我吹过你吹过的风，这算不算相拥？我走过你走过的路，这算不算相逢？” 12345678910111213141516171819/** * 实现原理: 如果两个链表要相交，那它们一定走了相同的路程。 * 假设公共部分长度为 C * 假设 A 不相交部分长度为 L1, 则其总长度为 L1 + C * 假设 B 不相交部分长度为 L2, 则其总长度为 L2 + C * 当 A B 走过的长度为 L1 + L2 + C 时将会相遇，而不是 L1 + L2 + 2C, * 因为在双指针交换指向的链表后只需要走到第一个第一个公共结点就可以确定公共部分 */public ListNode getIntersectionNode(ListNode headA, ListNode headB) &#123; if (headA == null || headB == null) return null; // 不直接引用头结点，方便走投无路时重新找到方向。 ListNode romeo = headA, juliet = headB; while (romeo != juliet) &#123; // 走过你来时的路 romeo = romeo == null ? headB : romeo.next; juliet = juliet == null ? headA : juliet.next; &#125; return romeo;&#125; 时间复杂度：$O(L_1+L_2+C)$，解释在注释里。 空间复杂度：$O(1)$。 53 - I. 在排序数组中查找数字 I题目描述统计一个数字在排序数组中出现的次数。 1234输入: nums = [5,7,7,8,8,10], target = 8输出: 20 &lt;= 数组长度 &lt;= 50000 Solution - 二分搜索 继续说，看到一次说一次：看见有序数组，首先想到二分搜索。 对于二分搜索，首先想到如下几个思路： 直接二分搜索，找到 $target$ 的话，记录下索引，然后从索引开始往前搜一搜，再往后搜一搜； 窗口：二分找到比 $target$ 大一点的右边界 $right$，比 $target$ 小一点的左边界 $left$，然后计算窗口的大小。 在此选择第二种思路。因为 $LeetCode$ 的方法名中的参数均为 $int$，所以比 $target$ 大一点与小一点的可以选择小数，作为插入位置，当然，这不是真的要插入，而是要限定区间。 123456789101112131415public int search(int[] nums, int target) &#123; return binarySearch(nums, target + 0.1) - binarySearch(nums, target - 0.1);&#125;private int binarySearch(int[] nums, double target) &#123; int left = 0, right = nums.length - 1; while (left &lt;= right) &#123; // 闭区间 [left, right] int mid = left + ((right - left) &gt;&gt;&gt; 1); if (nums[mid] &lt; target) left = mid + 1; else if (nums[mid] &gt; target) right = mid - 1; &#125; return right; // return left; 也可以&#125; 时间复杂度：$O(\\log_2 n)$。 空间复杂度：$O(1)$。 53 - II. 0～n-1 中缺失的数字题目描述一个长度为 $n-1$ 的递增排序数组中的所有数字都是唯一的，并且每个数字都在范围 $0 \\sim n-1$ 之内。在范围 $0 \\sim n-1$ 内的 $n$ 个数字中有且只有一个数字不在该数组中，请找出这个数字。 1234567输入: [0,1,3]输出: 2输入: [0,1,2,3,4,5,6,7,9]输出: 81 &lt;= 数组长度 &lt;= 10000 Solution 1 - 暴力搜索题目中已经说了是递增排序数组，并且唯一，而数组的下标本身就是从 $0 \\sim n-1$ 递增的。从题目中可以读到数组中的 $index-value$ 应该是相等的，如果某个索引 $index$ 处的 $value$ 不相等的话，则是缺失的数字。 1234567public int missingNumber(int[] nums) &#123; for (int i = 0; i &lt; nums.length; i++) &#123; if (i != nums[i]) return i; &#125; return nums.length;&#125; 时间复杂度：$O(n)$，遍历一遍数组。 空间复杂度：$O(1)$。 Solution 2 - 二分搜索 继续说，看到一次说一次：看见有序数组，首先想到二分搜索。 根据题意，可以将该有序数组划分为有序数组与无序数组两部分： 左子数组: 即 $nums[i] = i$； 右子数组: 即 $nums[i] \\ne i$。 寻找缺失的数字等价于右子数组的首位元素。 123456789101112public int missingNumber(int[] nums) &#123; if (nums == null || nums.length == 0) return int left = 0, right = nums.length - 1; while (left &lt;= right) &#123; // 闭区间 int mid = left + ((right - left) &gt;&gt;&gt; 1); if (nums[mid] == mid) // 说明左子数组有序，缺失的元素 [mid + 1, right] 中 left = mid + 1; else right = mid - 1; // mid 处已经无序，缺失的元素在 mid 之前，即 [left, mid - 1] 中 &#125; // 当循环结束时，left 指向右子数组的首位元素，即第一个 nums[i] != i 的。 return left;&#125; 时间复杂度：$O(n)$，遍历一遍数组。 空间复杂度：$O(1)$。 54(🌲). 二叉搜索树的第 k 大节点题目描述给定一棵二叉搜索树，请找出其中第 $k$ 大的节点。 123456789输入: root = [3,1,4,null,2], k = 1 3 / \\ 1 4 \\ 2输出: 41 ≤ k ≤ 二叉搜索树元素个数 Solution 1 - 中序遍历 $NOTE$: 二叉搜索树的中序遍历结果为递增序列。 既然中序遍历的结果是递增序列，那么遍历一遍返回倒数第 $k$ 个就 $Ok$ 了。 12345678910111213141516public class Solution &#123; private List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); public int kthLargest(TreeNode root, int k) &#123; recur(root); return list.get(list.size() - k); &#125; public void recur(TreeNode root) &#123; if (root == null) return; recur(root.left); list.add(root.val); recur(root.right); &#125;&#125; 时间复杂度：$O(n)$，需要访问每个节点一次。 空间复杂度：$O(n)$，当搜索树退化为链表时，$list$ 存储节点值需要 $O(n)$，递归深度达到 $O(n)$。 Solution 2 - 中序遍历倒序 大佬又有一种提前返回剪枝的做法。 提到中序遍历，可能会有不少人只会想到 左、根、右 这种顺序，但是其实 右、根、左 也是中序遍历。 既然二叉搜索树的中序遍历结果为递增序列，那么二叉搜索树的中序遍历结果的逆序为递减序列。 123456789101112131415// 打印中序遍历void dfs(TreeNode root) &#123; if(root == null) return; dfs(root.left); // 左 System.out.println(root.val); // 根 dfs(root.right); // 右&#125;// 打印中序遍历倒序void dfs(TreeNode root) &#123; if(root == null) return; dfs(root.right); // 右 System.out.println(root.val); // 根 dfs(root.left); // 左&#125; 接下来需要做的就是递减 $k$ 值，每访问一个节点就进行 k-- 操作，等到 $k=0$ 时，自然就是第 $k$ 大的节点了。 1234567891011121314151617181920public class Solution &#123; private int res, k; public int kthLargest(TreeNode roor, int k) &#123; this.k = k; recur(root); return res; &#125; public void recur(TreeNode root) &#123; if (root == null) return; recur(root.right); // ---要对 root 节点的操作开始--- if (k == 0) return; if (--k == 0) return root.val; // ---要对 root 节点的操作结束 --- recur(root.left); &#125;&#125; 时间复杂度：$O(n)$，当搜索树退化为链表时，无论 $k$ 值几何，递归深度均为 $n$。 空间复杂度：$O(n)$，当搜索树退化为链表时，$list$ 存储节点值需要 $O(n)$，递归深度达到 $O(n)$。 55(🌲) - I. 二叉树的深度输入一棵二叉树的根节点，求该树的深度。从根节点到叶节点依次经过的节点（含根、叶节点）形成树的一条路径，最长路径的长度为树的深度。 给定二叉树 [3,9,20,null,null,15,7] 12345678 3 / \\ 9 20 / \\ 15 7返回它的最大深度 3。节点总数 &lt;= 10000 Solution 1 - DFS 树的深度 $=$ 左右子树深度的最大值$ + 1$。 123public int maxDepth(TreeNode root) &#123; return root == null ? 0 : Math.max(maxDepth(root.left), maxDepth(root.right)) + 1;&#125; 时间复杂度：$O(n)$，每个节点访问一次。 空间复杂度：$O(n)$，当树退化为链表时，递归深度达到 $n$。 Solution 2 - BFS其实就是利用树的层次遍历，同时设置一个层高 $level$，每向下一层，$level+1$。 12345678910111213141516public int maxDepth(TreeNode root) &#123; if (root == null) return 0; List&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(), tmp; int level = 0; queue.add(root); while (!queue.isEmpty()) &#123; tmp = new LinkedList&lt;&gt;(); // 存储下一层节点 for (TreeNode node : queue) &#123; if (node.left != null) tmp.add(node.left); if (node.right != null) tmp.add(node.right); &#125; queue = tmp; level++; &#125; return level;&#125; 时间复杂度：$O(n)$，每个节点访问一次。 空间复杂度：$O(n)$，当树为满二叉树时，最底层节点数达到 $n/2$。 55(🌲) - II. 平衡二叉树题目描述输入一棵二叉树的根节点，判断该树是不是平衡二叉树。如果某二叉树中任意节点的左右子树的深度相差不超过 $1$，那么它就是一棵平衡二叉树。 给定二叉树 [3,9,20,null,null,15,7] 123456789 3 / \\ 9 20 / \\ 15 7返回 true1 &lt;= 树的结点个数 &lt;= 10000 Solution 1 - DFS这题其实是上一题的扩展，上一题可以用于求子树的深度，而这一题要做的是判断两棵子树的树高之差，即是否满足：$$|maxDepth(root.left)-maxDepth(root.right)|\\le 1$$ 12345678public boolean isBalanced(TreeNode root) &#123; return root == null ? true : Math.abs(maxDepth(root.left) - maxDepth(root.right)) &lt;= 1 &amp;&amp; isBalanced(root.left) &amp;&amp; isBalanced(root.right);&#125;// 直接拿过来上一题的代码private int maxDepth(TreeNode root) &#123; return root == null ? 0 : Math.max(maxDepth(root.left), maxDepth(root.right)) + 1;&#125; 时间复杂度：$O(n\\lg n)$。 空间复杂度：$O(n)$。 Solution 2 - 后序遍历+剪枝 此方法为本题的最优解法，但剪枝的方法不易第一时间想到。思路是对二叉树做后序遍历，从底至顶返回子树深度，若判定某子树不是平衡树则 “剪枝” ，直接向上返回。 算法流程 recur(root) 返回值： 当节点 $root$ 左 / 右子树的深度差 $\\leq 1$ ：则返回当前子树的深度，即节点 $root$ 的左 / 右子树的深度最大值 $+1$； 当节点 $root$ 左 / 右子树的深度差 $&gt;2$ ：则返回 $-1$ ，代表此子树不是平衡树 。 终止条件： 当 $root$ 为空：说明越过叶节点，因此返回高度 $0$ ； 当左（右）子树深度为 $-1$ ：代表此树的 左（右）子树 不是平衡树，因此剪枝，直接返回 $-1$ 。 isBalanced(root) 若 recur(root) != -1 ，则说明此树平衡，返回 $true$； 否则返回 $false$。 123456789101112public boolean isBalanced(TreeNode root) &#123; return recur(root) != -1;&#125;private int recur(TreeNode root) &#123; if (root == null) return 0; int left = recur(root.left); if (left == -1) return -1; int right = recur(root.right); if (right == -1) return -1; return Math.abs(left - right) &lt; 2 ? Math.max(left, right) + 1 : -1;&#125; 时间复杂度：$O(n)$， $n$ 为树的节点数；最差情况下，需要递归遍历树的所有节点。空间复杂度：$O(n)$，最差情况下（树退化为链表时），系统递归需要使用 $O(n)$ 的栈空间。 56 - I. 数组中数字出现的次数题目描述一个整型数组 $nums$ 里除两个数字之外，其他数字都出现了两次。请写程序找出这两个只出现一次的数字。要求时间复杂度是 $O(n)$，空间复杂度是 $O(1)$。 1234输入：nums = [4,1,4,6]输出：[1,6] 或 [6,1]2 &lt;= nums.length &lt;= 10000 Solution - 分组异或 题目要求：时间复杂度 $O(n)$， 空间复杂度 $O(1)$，因此不能用 $Map$ 以及双重循环。 选择分组异或操作，而在异或操作中有以下几条运算规则： A $\\oplus$ A = $0$ A $\\oplus$ B $\\oplus$ C = A $\\oplus$ (B $\\oplus$ C) 由于数组中存在两个不一样的且只出现一次的数字，其他的数字都是两两出现，而成对的数字异或结果为 $0$ ，所以全体数字异或的结果等于两个只出现一次的数字的异或结果。 12345678910111213141516174 ^ 1 ^ 4 ^ 6 = 1 ^ 6100 ^ 001 ^ 100 ^ 110 = 001 ^ 110 = 111但是此时我们无法通过最后异或的结果 111 去直接得出它是由 001 与 110 异或得出的，也就是说无法得出那两个只出现一次的数字是 1 和 6，而不是 101(5)，而不是 010(2)。所以，这个时候要考虑进行分组，如果能将 1、6 分开到两组，那么它们都是组中出现一次的元素该多好。假设 4、1、4 分在了一组，那么异或后就可以得到一个只出现一次的数字 1，同理 6 单独一组也可以认为它是只出现一次的数字。那么应该如何分组呢？对于成对出现的数字可以考虑使用奇偶分组，而判断奇偶的操作在位运算中可以使用 num &amp; 1 的结果来判断。对于 4 1(001) 4 6(110) 而言可以按照奇偶分组(末位不同)，但是如果是 4 3(011) 4 5(101) 这样的序列就不行了。此时考虑 1、6 的二进制表示，即 001、110，可以看到，1 与 6 的二进制表示三位数字均不相同，而且很显然任何两个不同的数字二进制表示一定也是不同的，所以我们只需要从左到右找出第一个不同的二进制位，如 001 的末位 1 与 110 的末位 0 是不同的，而它们分别与 1 取 &amp; 操作，可以得到 001 &amp; 1 = 001， 110 &amp; 1 = 110. 从而可以达到分组的目的。而对于 001 与 101 这样的 后两位 相同的数字，将它们分别 &amp;1 得到 001 &amp; 1 = 001，101 &amp; 1 = 001，此时是分不了组的。所以考虑将 &amp;1 操作中的 1 左移一位，即 &amp;1 =&gt; &amp; 10，此时 001 &amp; 10 = 000，101 &amp; 10 = 000，此时 &amp; 运算的结果又是相同的，又分不了组，那么将 10 继续左移一位变成 100。此时再进行 &amp; 操作，即 001 &amp; 100 = 000，101 &amp; 100 = 100，此时即可达到分组的目的。所以，对于两个不同的二进制数字，它们至少有一位是不同的，此时可以通过从 &amp;1(掩码) 操作开始进行判断，每当低位相同时，就将上一步的掩码左移一位，这样一定可以找到一位不同的数字，从而获得不同的结果，进而达到分组的目的。 123456789101112131415public int[] singleNumbers(int[] nums) &#123; // 任何数与 0 异或得其本身 int oplus = 0; // oplus 是 latex 中异或的符号 for (int num : nums) oplus ^= num; // 将所有结果异或，假设输入为 4 1 4 6，则结果为 111 int mask = 1; // 掩码 while ((oplus &amp; mask) == 0) mask &lt;&lt;= 1; // 执行完此步骤后，找到掩码 10 int x = 0, y = 0; for (int num : nums) &#123; // 第一组：100 ^ 10 = 000, 001 ^ 10 = 000, 第二组： 110 ^ 10 = 10 if ((num &amp; mask) == 0) x ^= num; // 100 ^ 001 ^ 100 = 001 = 1 else y ^= num; // 110 = 110 &#125; return new int[] &#123;x, y&#125;; // [1, 6]&#125; 时间复杂度：$O(n)$，扫描了两遍数组。 空间复杂度：$O(1)$，只用到了有限个变量。 56 - II. 数组中数字出现的次数 II题目描述在一个数组 $nums$ 中除一个数字只出现一次之外，其他数字都出现了三次。请找出那个只出现一次的数字。 12输入：nums = [3,4,3,3]输出：4 Solution 1 - 位运算解题思路如果一个数字出现了 $3$ 次，那么这个数字的二进制表示的每一位 $0\\ or\\ 1$ 也出现三次。如果把所有出现三次的数字的二进制表示的每一数位上的数字相加（和要么为 $0$ 要么为 $3$ 的倍数），那么每一位的和都能被 $3$ 整除。 如果某一数位上的数字和能被 $3$ 整除，那么那个只出现 $1$ 次的数字的二进制表示中的当前位为 $0$。因为不是 $0$ 的话，那只能是 $1$ 。而在前面已经提到过，所有出现三次的数字的二进制表示的每一数位上的数字和要么为 $0$ 要么为 $3$ 的倍数，而 +1 后则不能被 $3$ 整除。 否则为 $1$ 。 算法流程 使用与运算，可获取二进制数字 $num$ 的最右位 $n_1$，即 n_1 = num &amp; 1; ； 结合无符号右移，可获取 $num$ 所有位的值，即 $n_1 \\sim n_{32}$ ，即 num = num &gt;&gt;&gt; 1; 建立长度为 $32$ 的数组 $counts$ ，用于统计所有数字的二进制位中 $1$ 出现次数。 将 $counts$ 中的各元素对 $3$ 求余，则结果为”只出现 $1$ 次的数字的各二进制位。“ 利用 $左移操作$ 与 $或运算$ ，将 $counts$ 中的 $0\\ or \\ 1$ 恢复成结果。12345678910111213141516171819public int singleNumber(int[] nums) &#123; if (nums == null || nums.length == 0) throw new IllegalArgumentException(\"数组不合法.\"); int[] counts = new int[32]; for (int num : nums) &#123; for (int j = 0; j &lt; 32; j++) &#123; counts[j] += num &amp; 1; num &gt;&gt;&gt;= 1; &#125; &#125; // PS: 在这里使用 mod = 3，只需要修改 mod 的值，就成为解决 // \"除了一个数字以外，其余数字都出现 m 次\" 的通用方法。 int res = 0, mod = 3; for (int i = 0; i &lt; 32; i++) &#123; res &lt;&lt;= 1; res |= counts[31 - i] % mod; &#125; return res;&#125; 时间复杂度： $O(n)$ ，其中 $n = len(nums)$ 。遍历数组一遍，每个数字需要进行常数个位运算，每个操作消耗 $O(1)$ 。 空间复杂度： $O(1)$ ，数组 $counts$ 长度为 $32$ ，常数级别。 Solution 2 - 有限状态自动机 点击链接自行查看 57 - I. 和为 s 的两个数字题目描述输入一个递增排序的数组和一个数字 $s$，在数组中查找两个数，使得它们的和正好是 $s$。如果有多对数字的和等于 $s$，则输出任意一对即可。 12345输入：nums = [2,7,11,15], target = 9输出：[2,7] 或者 [7,2]1 &lt;= nums.length &lt;= 10^51 &lt;= nums[i] &lt;= 10^6 Solution - 对撞指针 之前常说，看到排序数组首先想到二分法，但是这里选择另一种思路，双指针中的对撞指针。本题当然也可以使用 $LeetCode$ 第一题的方法求解，但是空间复杂度为 $O(n)$。 算法流程 初始化：左指针 $left=0$ ，右指针 $right=nums.length-1$ ; 相向而行：判断 $target$ 与 $nums[left] + nums[right]$ 的关系 若 $target = nums[left]+nums[right]$ ，则返回数组 $[nums[left],nums[right]]$ ； 若 $target &gt; nums[left]+nums[right]$ ，和偏小，则左指针向右移动； 若 $target &lt; nums[left]+nums[right]$ ，和偏大，则右指针向左移动。 返回值：如果上一步中没有进行 $return$ 操作的话，说明没有找到符合条件的值，返回空数组。 123456789101112131415public int[] twoSum(int[] nums, int target) &#123; if (nums == null || nums.length == 0) return new int[0]; int left = 0, right = nums.length - 1; while (left &lt; right) &#123; // 跳出条件 left == right, 即区间内只有 1 元素。 // int sum = nums[left] + nums[right]; // 这里没有使用 作和 的方式是为了提示 数值溢出 // 但是 作差 的仍有可能 数值溢出，毕竟都没有强调元素正负 // 但是本题限定了元素的范围，所以都不会溢出。 int tar = target - nums[left]; if (tar == nums[right]) return new int[] &#123;nums[left], nums[right]&#125;; else if (tar &gt; nums[right]) left++; else right--; &#125; return new int[0];&#125; 时间复杂度：$O(n)$，扫描一遍数组。 空间复杂度：$O(1)$，只用到了两个变量。 57 - II. 和为 s 的连续正数序列题目描述输入一个正整数 $target$ ，输出所有和为 $target$ 的连续正整数序列（至少含有两个数）。 序列内的数字由小到大排列，不同序列按照首个数字从小到大排列。 1234输入：target = 9输出：[[2,3,4],[4,5]]1 &lt;= target &lt;= 10^5 Solution - 滑动窗口想必读者能够从头看到这里的话，已经不需要再介绍滑动窗口的概念了。本题所利用到的滑动窗口有以下几点需要关注，假设窗口内的所有数字之和为 $sum$ : 当 $sum &lt; target$时：需要扩大窗口，窗口的右边界需要向右移动。 当 $sum &gt; target$时：需要缩小窗口，窗口的左边界需要向右移动。 当 $sum = target$时：记录此时窗口中的所有元素。 经过上面三步之后可能有人要问：扩大窗口时左边界向左移动不行吗？缩小窗口时右边界向左移动不行吗？这样的话当然是可以的，但是最好不要回溯，这样的话很难保证滑动窗口 $O(n)$ 的复杂度。 12345678910111213141516171819202122232425public int[][] findContinuousSequence(int target) &#123; List&lt;int[]&gt; res = new ArrayList&lt;&gt;(); int i = 1, j = 1, sum = 0; while (i &lt;= target / 2) &#123; if (sum &lt; target) &#123; // 右边界向右移动 增大区间总和 sum += j; j++; &#125; else if (sum &gt; target) &#123; // 左边界向右移动，缩小区间总和 sum -= i; i++; &#125; else &#123; int[] tmp = new int[j - i]; for (int k = i; k &lt; j; k++) tmp[k - i] = k; res.add(tmp); // 当找到了一组满足的结果后 此时是最长区间 继续向右寻找 // 比如 2 + 3 + 4 = 9， 4 + 5 = 9 数值增大，区间缩小 sum -= i; i++; &#125; &#125; return res.toArray(new int[res.size()][]);&#125; 时间复杂度：$O(n)$。 时间复杂度：$O(1)$。 58 - I. 翻转单词顺序题目描述输入一个英文句子，翻转句子中单词的顺序，但单词内字符的顺序不变。为简单起见，标点符号和普通字母一样处理。例如输入字符串 “I am a student.“，则输出”student. a am I“。 1234567891011输入: \" hello world! \"输出: \"world! hello\"解释: 输入字符串可以在前面或者后面包含多余的空格，但是反转后的字符不能包括。输入: \"a good example\"输出: \"example good a\"解释: 如果两个单词间有多余的空格，将反转后单词间的空格减少到只含一个。无空格字符构成一个单词。输入字符串可以在前面或者后面包含多余的空格，但是反转后的字符不能包括。如果两个单词间有多余的空格，将反转后单词间的空格减少到只含一个。 Solution - 双指针「逆序」相关的题目可以使用「栈」来实现，但是这往往需要再去操作「栈」，引入额外的时间开销。这里选择双指针倒序遍历，在一次遍历中完成题目要求。算法流程 前置工作：去除字符串首尾多余空格； 初始化：建立两个指针 $i = j = s.length - 1$ ，指向字符串尾部，用来指示每个单词的左右边界，结果集 $res$。 倒序遍历：指针 $i$ 作为较靠左的指针，向左搜索 空格，在搜索到空格后， 将 $i, j$ 之间的单词加入结果中； 让 $j$ 移动到 $i$ 所在的位置。 返回值：返回结果集 $res$。 1234567891011121314public String reverseWords(String s) &#123; if (s == null || s.length() == 0) return \"\"; s = s.trim(); // 去除首尾空格 int j = s.length() - 1, i = j; StringBuilder res = new StringBuilder(); while (i &gt;= 0) &#123; while (i &gt;= 0 &amp;&amp; s.charAt(i) != ' ') i--; res.append(s.substring(i + 1, j + 1) + \" \"); while (i &gt;= 0 &amp;&amp; s.charAt(i) == ' ') i--; j = i; &#125; return res.toString().trim();&#125; 时间复杂度： $O(n)$ ，其中 $n=s.length()$。 空间复杂度： $O(n)$ ， $StringBuilder$ 并不作为最后结果直接返回，而是中间数据结构。 58 - II. 左旋转字符串题目描述字符串的左旋转操作是把字符串前面的若干个字符转移到字符串的尾部。请定义一个函数实现字符串左旋转操作的功能。比如，输入字符串 “$abcdefg$” 和数字 $2$，该函数将返回左旋转两位得到的结果 “$cdefgab$”。 1234输入: s = \"abcdefg\", k = 2输出: \"cdefgab\"1 &lt;= k &lt; s.length &lt;= 10000 Solution 1 - API1234public String reverseLeftWords(String s, int n) &#123; if (s == null || n &gt; s.length()) return \"\"; return s.substring(n, s.length()) + s.substring(0, n);&#125; 时间复杂度：$O(N)$，其中 $N = s.length()$ 。 空间复杂度：$O(N)$，其中 $N = s.length()$ 。 Solution 2 - 反转首先需要一个交换字符首尾字符的方法 reverse(char[] chars, int i, int j)，它的作用是交换字符数组首尾的字符，并且 $i, j$ 向终点收缩。 这种思想是基于这样一个事实，仍然拿 s = &quot;abcdefg&quot;, n = 2 为例。 123第一次交换：ab cdefg 交换前 2 个字符 =&gt; ba cdefg第一次交换：ba cdefg 交换后 5 个字符 =&gt; ba gfedc第三次交换：ba gfedc 交换共 7 个字符 =&gt; cdefg ab 所以接下来需要做的就是 12345678910111213141516171819202122class Solution &#123; private char[] chars; public String reverseLeftWords(String s, int n) &#123; chars = s.toCharArray(); reverse(0, n - 1); reverse(n, s.length() - 1); reverse(0, s.length() - 1); return String.valueOf(chars); &#125; private void reverse(int i, int j) &#123; while (i &lt; j) &#123; char c = chars[i]; chars[i] = chars[j]; chars[j] = c; ++i; --j; &#125; &#125;&#125; 时间复杂度：$O(N)$，其中 $N = s.length()$ 。需要遍历两遍数组。 空间复杂度：$O(N)$，其中 $N = s.length()$ 。因为 Java 中的 String 不可以直接修改，所以用 chars 辅助存储。 59 - I. 滑动窗口的最大值题目描述给定一个数组 $nums$ 和滑动窗口的大小 $k$，请找出所有滑动窗口里的最大值。 1234567891011121314输入: nums = [1,3,-1,-3,5,3,6,7], 和 k = 3输出: [3,3,5,5,6,7]解释: 滑动窗口的位置 最大值--------------- -----[1 3 -1] -3 5 3 6 7 3 1 [3 -1 -3] 5 3 6 7 3 1 3 [-1 -3 5] 3 6 7 5 1 3 -1 [-3 5 3] 6 7 5 1 3 -1 -3 [5 3 6] 7 6 1 3 -1 -3 5 [3 6 7] 7你可以假设 k 总是有效的，在输入数组不为空的情况下，1 ≤ k ≤ 输入数组的大小。 Solution 1 - 暴力解法设窗口为 $[i, j]$，最大值为 $x_j$ 。当窗口向右移动一格后变为 $[i+1, j+ 1]$ ，即窗口内新增了 $nums[j+1]$ ，消失了 $nums[i]$ 。若仅向窗口内添加了 $nums[j]$ ，则新窗口最大值仅需比较一次即可确定。即：$$x_{j+1} = max(x_j, nums[j+1])$$但是消失的 $nums[i]$ 可能刚好就是最大值 $x_j$ ，因此需要通过遍历的方式再次寻找窗口中的最大值。 12345678910public int[] maxSlidingWindow(int[] nums, int k) &#123; if (nums == null || nums.length == 0 || k &lt;= 0) return new int[0]; int idx = 0, len = nums.length + 1 - k; int[] res = new int[len]; for (int i = 0; i &lt; len; i++) &#123; res[idx++] = Arrays.stream(nums, i, i + k).max().getAsInt(); &#125; return res;&#125; 时间复杂度：$O(nk)$。$k$ 为滑动窗口大小。 空间复杂度：$O(1)$，res 作为最后返回结果，不算入空间复杂度。而每次的窗口仍是在原数组上操作。 Solution 2 - 单调队列不难发现暴力解法中搜索窗口最大值的时间复杂度为 $O(k)$ ，遍历数组又必不可少，所以如果想要继续降低复杂度，突破口就在于降低寻找窗口最大值的复杂度。理想情况是从 $O(k)$ 降低到 $O(1)$ 。在「30. 包含 min 函数的栈」实现了 $O(1)$ 获取最小值，同理，也可以以 $O(1)$ 方式获得窗口中的最大值。窗口对应的数据结构式双端队列，在此使用单调队列。在遍历数组的过程中，每轮保证单调队列 $deque$ ： $deque$ 仅包含窗口内出现过的元素。每当窗口移动，滑过 $nums[i-1]$ 时，将 $deque$ 内 $nums[i]$ 对应的元素一起删除（如果存在的话）。 $deque$ 内的元素非严格递减。每当窗口移动，新增 $nums[j+1]$ 时，需要将 $deque$ 内所有小于 $nums[j+1]$ 的元素删除。 算法流程 初始化：双端队列 $deque$ ，结果集 $res$ ； 滑动窗口：左边界范围 $i \\in [1 - k, n + 1 - k]$，右边界范围 $j \\in [0, n - 1]$ 若 $i &gt; 0$ 且队首元素 $deque[0] = $ 被删除的元素 $nums[i - 1]$ ，将队首元素出队； 删除 $deque$ 内所有小于 $nums[j]$ 的元素，保持 $deque$ 的单调性； 将 $nums[j]$ 添加至 $deque$ 尾部； 若已形成窗口（ $i \\ge 0$ ），将窗口最大值（ $deque[0]$ ）添加至结果集 $res$ 。 返回值：返回结果集 $res$ 。123456789101112131415public int[] maxSlidingWindow(int[] nums, int k) &#123; if (nums == null || nums.length == 0 || k &lt;= 0) return new int[0]; Deque&lt;Integer&gt; deque = new LinkedList&lt;&gt;(); int[] res = new int[nums.length + 1 - k]; for (int j = 0, i = 1 - k; j &lt; nums.length; i++, j++) &#123; if (i &gt; 0 &amp;&amp; deque.peekFirst() == nums[i] - 1) deque.removeFirst(); while (!deque.isEmpty() &amp;&amp; deque.peekLast() &lt; nums[j]) deque.removeLast(); // 保持单调性 if (i &gt;= 0) res[i] = deque.peekFirst(); // 记录窗口的最大值 &#125; return res;&#125; 时间复杂度： $O(n)$ ，其中 $n=len(nums,)$。遍历 $nums$ 需要 $O(n)$ ，每个元素仅入队出队一次，因此单调队列 $deque$ 占用 $O(2n)$ 。 空间复杂度： $O(k)$ ，双端队列最多同时存储 $k$ 个元素。（滑动窗口的大小） 59 - II. 队列的最大值题目描述请定义一个队列并实现函数 max_value 得到队列里的最大值，要求函数 max_value、push_back 和 pop_front 的均摊时间复杂度都是 $O(1)$。 若队列为空，pop_front 和 max_value 需要返回 $-1$。 1234567输入:[\"MaxQueue\",\"push_back\",\"push_back\",\"max_value\",\"pop_front\",\"max_value\"][[],[1],[2],[],[],[]]输出: [null,null,null,2,1,2]1 &lt;= push_back,pop_front,max_value的总操作数 &lt;= 100001 &lt;= value &lt;= 10^5 Solution - 辅助队列 基础的的 push_back 与 pop_front 方法可以用队列 queue 自带的 offer() 与 poll() 以 $O(1)$ 实现 维护一个单调双向队列 deque，保证 max_value() 操作在 $O(1)$ 的时间内完成，此时需要保证： 在向 queue 添加元素时，维护 deque 的单调性，假设 offer() 的元素为 $x$，那么需要移除 deque 中所有小于 x 的元素。 在 queue 出队时，如果出队的元素与 deque 的首元素相同，则移除 deque 中队首元素。 1234567891011121314151617181920212223242526272829class MaxQueue &#123; private Queue&lt;Integer&gt; queue; private Deque&lt;Integer&gt; deque; public MaxQueue() &#123; queue = new LinkedList&lt;&gt;(); deque = new LinkedList&lt;&gt;(); &#125; public int max_value() &#123; return deque.size() &gt; 0 ? deque.peekFirst() : -1; &#125; public void push_back(int value) &#123; queue.offer(value); while (deque.size() &gt; 0 &amp;&amp; deque.peekLast() &lt; value) deque.pollLast(); deque.offer(value); &#125; public int pop_front() &#123; int tmp = queue.size() &gt; 0 ? queue.poll() : -1; if (deque.size() &gt; 0 &amp;&amp; tmp == deque.peekFirst()) deque.pollFirst(); return tmp; &#125;&#125; 时间复杂度：$O(1)$。 空间复杂度：$O(1)$。 60. n 个骰子的点数题目描述把 $n$ 个骰子扔在地上，所有骰子朝上一面的点数之和为 $s$。输入 $n$，打印出 $s$ 的所有可能的值出现的概率。 用一个浮点数数组返回答案，其中第 $i$ 个元素代表这 $n$ 个骰子所能掷出的点数集合中第 $i$ 小的那个的概率。 12输入: 1输出: [0.16667,0.16667,0.16667,0.16667,0.16667,0.16667] Solution - 动态规划先假设投掷两枚骰子，假设 $f(n,k)$ 表示投掷 $n$ 枚骰子 🎲 时，点数和 $k$ 出现的次数，如果只有 $1$ 枚骰子的话，出现 $1-6$ 的概率均为 $1/6$， 这里先来模拟 $n=2$ 来计算 $k=4$ 和 $k=5$ 的情况，那么有：$$\\begin{cases}f(2,4)&amp;=f(1,1)+f(1,2)+f(1,3) \\f(2,5)&amp;=f(1,1)+f(1,2)+f(1,3)+f(1,4)\\end{cases}$$可以看到其中出现了大量的重复计算与子结构，所以考虑使用动态规划的方法来解决。 动态规划解析 状态定义：设 $f(n,k)$ 表示投掷 $n$ 枚骰子时，点数和 $k$ 出现的次数； 状态转移：上面选择了 $f(n,k)$ 作为状态，当投掷 $n$ 枚骰子时需要依靠 $n-1$ 枚骰子时的状态，同时第 $n$ 枚骰子有 $1、2、3、4、5、6$ 六种取值，所以状态转移可以表示为： $$f(n,k)=f(n-1,s-1)+f(n-1,s-2)+f(n-1,s-3)+f(n-1,s-4)+f(n-1,s-5)+f(n-1,s-6)$$ 初始状态：$f(1,k)=1$，其中 $k=1、2、3、4、5、6$。 概率：$p(n,k)=\\frac{f(n,k)}{6^n}$。 123456789101112131415161718192021222324252627public double[] twoSum(int n) &#123; // 1. 初始化 DP Table int[][] dp = new int[n + 1][6 * n + 1]; for(int k = 1; k &lt;= 6; k++) dp[1][k] = 1; // 2. 状态转移 从两枚骰子开始，到 n 枚骰子结束。 for (int i = 2; i &lt;= n; i++) &#123; // 3. 骰子和的范围， i 枚骰子最小为 i (全为1)，最大为 6i (全为 6) for (int k = i; k &lt;= 6 * i; k++) &#123; // 4. 最后一枚骰子的可能取值，即从 1 到 6. for (int last = 1; last &lt;= 6; last++) &#123; // 5. 第 i 个骰子的结果依赖第 i - 1 个骰子的结果 // 而 i - 1 个骰子的最小值最小值就是 i - 1 if (k - last &lt; i - 1) break; dp[i][k] += dp[i - 1][k - last]; &#125; &#125; &#125; double total = Math.pow((double) 6, (double) n); double[] res = new double[5 * n + 1]; for (int i = n; i &lt;= 6 * n; i++) res[i - n] = (double) dp[n][i] / total; return res;&#125; 时间复杂度：$O(n^2)$。 空间复杂度：$O(n^2)$。 61. 扑克牌中的顺子题目描述从扑克牌中随机抽 $5$ 张牌，判断是不是一个顺子，即这 $5$ 张牌是不是连续的。$2\\sim 10$为数字本身，$A$ 为 $1$，$J$ 为 $11$，$Q$ 为 $12$，$K$ 为 $13$，而大、小王为 $0$ ，可以看成任意数字。$A$ 不能视为 $14$。 123456输入: [1,2,3,4,5]输出: True限制：数组长度为 5 数组的数取值为 [0, 13] . 解题思路根据题意，此 $5$ 张牌是顺子的 充分条件 如下： 除大小王外，所有牌无重复； 设此 $5$ 张牌中最大的牌为 $max$ ，最小的为 $min$ (大小王除外)，则需满足： $$max-min&lt;5$$ 因此，可将原问题转化为：此 $5$ 张牌是否满足以上两个条件？ Solution - 集合 Set + 遍历算法流程 遍历五张牌，遇到大小王直接跳过； 判别重复：利用 $Set$ 判重； 获取最大与最小值：借助辅助变量 $max,min$ ，遍历五张牌即可。 12345678910111213public boolean isStraight(int[] nums) &#123; if (nums == null || nums.length == 0) return false; Set&lt;Integer&gt; set = new HashSet&lt;&gt;(); int max = 0, min = 14; for (int num : nums) &#123; if (num == 0) continue; // 跳过大小王 max = Math.max(max, num); min = Math.min(min, num); if (set.contains(num)) return false; // 出现对子肯定不会是顺子 set.add(num); &#125; return max - min &lt; 5;&#125; 时间复杂度： $O(n=5)=O(1)$ 。 空间复杂度： $O(n=5)=O(1)$ 。 62. 圆圈中最后剩下的数字题目描述$0,1,\\dots,n-1$ 这 $n$ 个数字排成一个圆圈，从数字 $0$ 开始，每次从这个圆圈里删除第 $m$ 个数字。求出这个圆圈里剩下的最后一个数字。 例如，$0、1、2、3、4$ 这 $5$ 个数字组成一个圆圈，从数字 $0$ 开始每次删除第 $3$ 个数字，则删除的前 $4$ 个数字依次是 $2、0、4、1$，因此最后剩下的数字是 $3$。 12输入: n = 5, m = 3输出: 3 限制条件：$1 \\le n \\le 10^5, 1 \\le m \\le 10^6$。 Solution 1 - 模拟 这个问题是以弗拉维奥·约瑟夫命名的，他是 $1$ 世纪的一名犹太历史学家。他在自己的日记中写道，他和他的40个战友被罗马军队包围在洞中。他们讨论是自杀还是被俘，最终决定自杀，并以抽签的方式决定谁杀掉谁。约瑟夫斯和另外一个人是最后两个留下的人。约瑟夫斯说服了那个人，他们将向罗马军队投降，不再自杀。约瑟夫斯把他的存活归因于运气或天意，他不知道是哪一个。- 约瑟夫环 - Wikipedia 其实 Wikipedia 已经给出了这道题的两种解决方案，一是链表模拟，二是数学反演。 假设当前删除的索引是 $idx$ ，下一个待删除的数字索引是 $idx+m$ ，由于删除了当前位置的数字 $x$ ， $x$ 之后的数字将会向前移动一位，所以下一个待删除的数字索引其实是 $idx+m-1$ 。因为到达末尾后会从再次从头开始数，有 环 存在，所以需要对环的周长取模，即 $(idx+m-1)\\ mod\\ n$ 。 算法流程 初始化：初始化约瑟夫环； 循环模拟：在这个过程中 找到下一个待删除的索引 $(idx + m - 1)\\ %\\ n$； 从环中移除该索引； 缩小环的周长。 返回值：返回仅剩的数字。 123456789101112public int lastRemaining(int n, int m) &#123; List&lt;Integer&gt; josephCircle = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; n; i++) josephCircle.add(i); int idx = 0; while (n &gt; 1) &#123; idx = (idx + m - 1) % n; josephCircle.remove(idx); n--; &#125; return josephCircle.get(0);&#125; 时间复杂度： $O(n^2)$ ，以 $O(1)$ 定位到删除位置，然后移动元素 $O(n)$ ，需要循环 $n$ 次。 空间复杂度： $O(n)$，用于存储约瑟夫环。 Solution 2 - 数学反演首先以 $m=2$ 为例来推理这个问题，对于 $m \\ne 2$ 的情况，之后将会给出一个具有一般性的解法。 设 $f(n)$ 为在有 $n$ 个人时生还者的位置(最终的生还者只会有一个人)。走完一圈之后，所以偶数号(假设$m=2$)的人被杀。则在第二圈，新的编号为 $2、4、\\dots$ 的人被杀，就像没有第一圈时一样。 如过一开始有偶数个人，则第二圈时位置为 $x$ 的人一开始在 $2x-1$ 的位置，因此位置为 $f(2n)$ 的人的开始时的位置为 $2f(n)-1$ ，递推公式为：$$f(2n)=2f(n)-1 \\tag{1}$$如果一开始有奇数个人，则走了一圈以后，最终是号码为 $1$ 的人被杀。于是同样地，再走第二圈时，新的编号为 $2、4、\\dots$ 的人被杀，等等。在这种情况下，位置为 $x$ 的人原先位置为 $2x+1$，递推公式为：$$f(2n+1)=2f(n)+1 \\tag{2}$$如果把 $n$ 和 $f(n)$ 的值列成表，可以看出这样一个规律： 12n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16f(n) 1 1 3 1 3 5 7 1 3 5 7 9 11 13 15 1 从中可以看出，$f(n)$ 是一个递增的奇数数列，每当 $n$ 为 $2$ 的幂时，便重新从 $f(n)=1$ 开始。因此，如果选择 $m、l$ ，使得 $n=2^m+l$ 且 $0\\le l \\le 2^m$，那么 $f(n)=2l+1$。其中 $2^m$ 是不超过 $n$ 的最大幂， $l$ 是留下的数。上面代码块中的值满足这个方程，下面给出一个定理：$$如果\\ n=2^m+l \\ 且\\ 0 \\le l &lt; 2^m,\\ 则\\ f(n)=2l + 1.$$一般情况下，考虑生还者的号码从 $n-1$ 到的 $n$ 变化, 可以得到以下的递推公式(编号从 $0$ 开始)：$$f(n,k)=(f(n-1,k)+k)\\ mod\\ n,\\quad f(1,k)=0$$ 123456789101112// 非递归public int lastRemaining(int n, int m) &#123; int survivor = 0; for (int i = 2; i &lt;= n; i++) survivor = (survivor + m) % i; return survivor;&#125;// 递归版本public int lastRemaining(int n, int m) &#123; return n &gt; 1 ? (lastRemaining(n - 1, m) + m) % n : 0;&#125; 时间复杂度：$O(n)$。 空间复杂度：非递归版本 $O(1)$，递归栈可能需要 $O(n)$。 63. 股票的最大利润题目描述假设把某股票的价格按照时间先后顺序存储在数组中，请问买卖该股票一次可能获得的最大利润是多少？ 12345输入: [7,1,5,3,6,4]输出: 5解释: 在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。注意利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格 Solution - 动态规划 状态定义：设动态规划列表为 $dp$， $dp[i]$ 代表以 $prices[i]$ 为结尾的子数组的最大利润，即到第 $i$ 天为止的最大利润。 状态转移方程：由于题目限定了一只股票只能买卖一次，因此前 $i$ 日的最大利润等于前 $i-1$ 日的最大利润 $dp[i-1]$ 和第 $i$ 日卖出股票后的利润之中的较大值（此时需要一个变量保存 $0..i-1$ 中的最小值），即：$$dp[i] = Math.max(dp[i-1], prices[i] - minCost)$$ 12345678public int maxProfit(int[] prices) &#123; int cost = Integer.MAX_VALUE, profit = Integer.MIN_VALUE; for (int price : prices) &#123; cost = Math.min(cost, price); profit = Math.max(profit, price - cost); &#125; return profit;&#125; 时间复杂度：$O(n)$，扫描了一遍数组。 空间间复杂度：$O(1)$，只用到了两个变量。 64. 求 1+2+…+n题目描述求 $1+2+\\cdots + n$ ，要求不能使用 $\\times, \\div, for, while, if-else, switch-case, A ? B : C$ 等关键字及三元运算符。 Solution - 逻辑短路求从 $1$ 到 $n$ 的和通常有三种方式: 等差数列的求和公式：此时显然要用到乘法，简单来说不可取，但是有种方法叫做位运算快速乘可以实现 $$\\sum_{i = 1}^n i = \\frac{n\\times (n+1)}{2}$$ 迭代：需要用到 $if、for$ ，不可取。 12345public int sumNums(int n) &#123; int res = 0; for(int i = 1; i &lt; n; i++) res += i; return res;&#125; 递归：但是这样有三元运算符，也可以认为是 $if-else$，简单来看不可取，但是可以利用位运算的短路效应实现 $if-else$ 的功能。 12345public int sumNums(int n) &#123; return n == 1 ? 1 : n += sumNums(n - 1); // A &amp;&amp; B, A 为假则 B 不执行 // A || B， A 为真则 B 不执行&#125; 所以上面方法可以改造为 1234public int sumNums(int n) &#123; boolean flag = n &gt; 1 &amp;&amp; (n += sumNums(n - 1)) &gt; 0; return n;&#125; 时间复杂度：$O(n)$，需要开启 $n$ 个递归函数。 空间间复杂度：$O(n)$，递归深度达到 $n$。 65. 不用加减乘除做加法写一个函数，求两个整数之和，要求在函数体内不得使用 “+”、“-”、“*”、“/” 四则运算符号。 12a, b 均可能是负数或 0结果不会溢出 32 位整数 Solution - 位运算设两个加数二进制表示分别为 $a, b$, 其和为 $s$，$a(i)$ 表示 $a$ 的第 $i$ 位，则分为以下四种情况： $a(i)$ $b(i)$ 无进位和 $n(i)$ 进位 $c(i+1)$ 0 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 观察发现两个规律： 无进位和结果 = 异或结果，即 $n = a \\oplus b$； 进位结果 = 与运算左移一位结果，即 $c = a &amp; b &lt;&lt; 1$。 所以 $s = a + b =&gt; s = n + c$，循环求 $n、c$ ，直至进位 $c=0$ ，此时 $s=n$ ，返回 $n$ 即可。 12345678public int add(int a, int b) &#123; while (b != 0) &#123; int c = (a &amp; b) &lt;&lt; 1; a ^= b; b = c; &#125; return a;&#125; 时间复杂度：$O(1)$，最多循环 $32$ 次，每次循环中操作 $O(1)$。 空间间复杂度：$O(1)$，常数个额外空间。 66. 构建乘积数组题目描述给定一个数组 $A[0,1,…,n-1]$，请构建一个数组 $B[0,1,…,n-1]$，其中 $B$ 中的元素 $B[i]=A[0]×A[1]×…×A[i-1]×A[i+1]×…×A[n-1]$。不能使用除法。 12输入: [1,2,3,4,5]输出: [120,60,40,30,24] Solution本题难点在于不能使用除法，观察 $B[i]$ 的定义发现缺少一项，即 $A[i]$，但是其实我们可以将 $A[i]$ 也乘入其中，如果 $A[i] = 1$ 的话。基本思路就是使用 $A[i] = 1$ 这个桥梁，将 $B[i]$ 的乘积表达式分成两部分，即 $A[i]$ 左侧与 $A[i]$ 右侧。这个时候可以考虑建立一个 $n \\times n$ 的方阵，且其主对角线为 $1$。 算法流程 初始化：数组 $B[0] = 1$，中间变量 $tmp = 1$; 分别计算下三角各同行元素乘积，存入 $B[i]$； 将上三角同行($i$ 行)元素与 $B[i]$ 相乘，并更新 $B[i]$ 为此结果； 返回 $B$。 12345678910111213public int[] constructArr(int[] a) &#123; if (a == null || a.length == 0) return new int[0]; int[] B = new int[a.length]; B[0] = 1; int tmp = 1; for (int i = 1; i &lt; a.length; i++) B[i] = B[i - 1] * a[i - 1]; for (int i = a.length - 2; i &gt;= 0; i--) &#123; tmp *= a[i + 1]; B[i] *= tmp; &#125; return B;&#125; 时间复杂度：$O(n)$，需要扫描两次数组。 空间间复杂度：$O(1)$，中间变量 $tmp$。 67. 把字符串转换成整数题目描述写一个函数 StrToInt()，实现把字符串转换成整数这个功能。不能使用 atoi 或者其他类似的库函数。 该函数具有如下功能： 根据需要丢弃无用的开头空格字符，直到寻找到第一个非空格的字符为止； 当寻找到的第一个非空字符为正或者负号时，则将该符号与之后面尽可能多的连续数字组合起来，作为该连续数字的正负号； 当寻找到的第一个非空字符是数字，则直接将其与之后连续的数字字符组合起来，形成整数。 该字符串除了有效的整数部分之后也可能会存在多余的字符，这些字符可以被忽略，它们对于函数不应该造成影响。 假如该字符串中的第一个非空格字符不是一个有效整数字符、字符串为空或字符串仅包含空白字符时，函数不需要进行转换； 在任何情况下，若函数不能进行有效的转换时，返回 $0$； 数字转换范围为：$[-2^{31}, 2^{31}-1]$，如果超过这个范围则返回 Integer.MIN_VALUE 或 Integer.MAX_VALUE。 本题与主站第 8 题相同 Solution根据题意，有以下几种情况需要考虑： 左起第一个符号或数字位之前为空格：直接删除即可； 符号位：可能为 $+-$、或者无符号，保存一个变量代表符号位，$-1$ 代表 $-$ 号，$1$ 代表 $+$ 或无符号； 非数字字符：直接返回，因为已经不是数字了，例如 $123x$ 或者 $-x123$； 数字字符：从左向右遍历时，设当前位的字符为 $c$ ，当前位的字符对应的数字为 $x$ ，当前位置时的结果为 $res$。 字符转数字：将此数字的 $ASCII$ 码与 $0$ 的 $ASCII$ 码相减，获得该字符的数值表示 $x = ascii(c) - ascii(0)$； 字符拼接：$res = res \\times 10 + x$。 数字越界处理：题目要求返回的值的范围在 $[-2^{31}, 2^{31}-1]$，观察 $-2147483648$ 与 $2147483647$ 可以发现它们的除去符号位的前九位都是相同的，即 $boundry = 214748364$，只有最后一位分别是 $7、8$ ，而且又因为它们的符号相反，所以在符号为 $-$ 时可以判断最后一个字符是否小于 $7$，而符号为正时的判断也是最后一位是否 $&lt; 7$。 12345678910111213141516public int strToInt(String str) &#123; // 不能一开始就这么判断，如果输入是 \" \" 的话经转化后的 char 数组长度为 0，判断 c[0] 时下标越界。 // if (str.length() == 0) return 0; char[] c = str.trim().toCharArray(); // trim() 用于删除首尾的空字符串 int res = 0, sign = 1, boundry = Integer.MAX_VALUE / 10; int index = 1; // 如果首位是正负号时的起始下标 if (c[0] == '-') sign = -1; else if (c[0] != '+') index = 0; // 如果首字符不为 +，说明是数字字符，从头开始 for (int j = index; j &lt; c.length(); j++) &#123; if (c[j] &lt; '0' || c[j] &gt; '9') break; // 不是数字字符 if (res &gt; boundry || res == boundry &amp;&amp; c[j] &gt; '7') // 如果已经越界 return sign == 1 ? Integer.MAX_VALUE : Integer.MIN_VALUE; res = 10 * res + c[j] - '0'; &#125; return sign * res;&#125; 时间复杂度：$O(n)$，需要扫描一遍字符数组。 空间间复杂度：$O(n)$，存储字符数组。 68(🌲) - I. 二叉搜索树的最近公共祖先题目描述给定一棵二叉树, 找到该树中两个指定节点的最近公共祖先。 最近公共祖先的定义为：“对于有根树 $T$ 的两个节点 $p、q$，最近公共祖先表示为一个节点 $x$，满足 $x$ 是 $p、q$ 的祖先且 $x$ 的深度尽可能大（一个节点也可以是它自己的祖先）。” 例如，给定如下二叉搜索树: $root = [6,2,8,0,4,7,9,null,null,3,5]$ 123456输入: root = [6,2,8,0,4,7,9,null,null,3,5], p = 2, q = 8输出: 6解释: 节点 2 和节点 8 的最近公共祖先是 6。所有节点的值都是唯一的。p、q 为不同节点且均存在于给定的二叉搜索树中。 解析祖先的定义：若节点 $p$ 在节点 $root$ 的左/右子树中，或 $p = root$，则称 $root$ 是 $p$ 的祖先。 最近公共祖先的定义: 设节点 $root$ 为节点 $p、q$ 的某公共祖先，若其左子节点 $root.left$ 和右子节点都不是 $p、q$ 的公共祖先，则称 $root$ 是最近公共祖先。 因此：若 $root$ 是 $p、q$ 的最近公共祖先，则只能为以下情况之一： $p、q$ 在 $root$ 的子树中，且分列 $root$ 的左右两侧； $p = root$，且 $q$ 在 $root$ 左子树或右子树中； $q = root$，且 $p$ 在 $root$ 左子树或右子树中。 12345678两个重要条件- 🌲为二叉搜索🌲- 所有结点都是唯一的判断 p、q 与 root 的关系root.val &lt; p.val: p 在 root 的右子树中root.val &gt; p.val: p 在 root 的左子树中root.val == p.val: p、root 指向同一结点 Solution 1 - 迭代 循环搜索：当结点 $root$ 为空时跳出 当 $p、q$ 都在 $root$ 的右子树中，则到 $root.right$ 遍历; 当 $p、q$ 都在 $root$ 的左子树中，则到 $root.left$ 遍历; 否则，说明找到了最近公共祖先，跳出。 返回值：最近公共祖先 $root$。 12345678910public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; while(root != null) &#123; if(root.val &lt; p.val &amp;&amp; root.val &lt; q.val) // p,q 都在 root 的右子树中 root = root.right; // 到右子节点遍历 else if(root.val &gt; p.val &amp;&amp; root.val &gt; q.val) // p,q 都在 root 的左子树中 root = root.left; // 到左子节点遍历 else break; &#125; return root;&#125; 时间复杂度：$O(n)$, 其中 $n$ 为二叉树结点个数。每循环一轮排除掉一层，二叉搜索树的最坏情况为退化为链表。 空间复杂度：$O(1)$。 Solution 2 - 递归 递推工作： 当 $p、q$ 都在 $root$ 的右子树中，则开启递归 $root.right$ 并返回。 否则，当 $p、q$ 都在 $root$ 的左子树中，则开启递归 $root.left$ 并返回。 返回值：最近公共祖先 $root$。 1234567public TreeNode lowestCommandAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if (root.val &lt; p.val &amp;&amp; root.val &lt; q.val) return lowestCommandAncestor(root.right, p, q); if (root.val &gt; p.val &amp;&amp; root.val &gt; q.val) return lowestCommandAncestor(root.left, p, q); return root;&#125; 时间复杂度：$O(n)$。 空间复杂度：$O(n)$，最坏情况下，树退化为链表，递归深度达到 $n$。 68(🌲) - II. 二叉树的最近公共祖先题目描述给定一棵二叉树, 找到该树中两个指定节点的最近公共祖先。 最近公共祖先的定义为：“对于有根树 $T$ 的两个节点 $p、q$，最近公共祖先表示为一个节点 $x$，满足 $x$ 是 $p、q$ 的祖先且 $x$ 的深度尽可能大（一个节点也可以是它自己的祖先）。” 例如，给定如下二叉树: $root = [3,5,1,6,2,0,8,null,null,7,4]$。 123456789Case 1:输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 1输出: 3解释: 节点 5 和节点 1 的最近公共祖先是节点 3。Case 2:输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 4输出: 5解释: 节点 5 和节点 4 的最近公共祖先是节点 5。因为根据定义最近公共祖先节点可以为节点本身。 Solution - 后序遍历最近公共祖先的定义: 设节点 $root$ 为结点 $p、q$ 的某公共祖先，若其左子节点 $root.left$ 和右子节点 $root.right$ 都不是 $p、q$ 的公共祖先，则称 $root$ 为最近公共祖先。且仅有以下三种情况： $p、q$ 在 $root$ 的子树中，且分列 $root$ 两侧； $p = root$，且 $q$ 在 $root$ 左子树或右子树中； $q = root$，且 $p$ 在 $root$ 左子树或右子树中。 使用递归对二叉树进行后序遍历，当遇到结点 $p$ 或 $q$ 时返回，自底向上回溯，当结点 $p、q$ 在结点 $root$ 的两侧时，结点 $root$ 即为最近公共祖先，向上返回 $root$。 递归解析： 终止条件 越过叶结点，返回 $null$。 root == p || root == q，返回 root. 递推工作 开启递归左子结点，返回值记为 left。 开启递归右子结点，返回值记为 right。 返回值： 根据 left和 right，可具体分为四种情况 left == null || right == null：说明 root的左右子树都不包含 p、q，返回 null； left != null || right != null: 说明 p、q 分列 root两侧，因此 root为最近公共祖先，返回 root； left == null &amp;&amp; right != null：说明 p、q 都不在 root的左子树中，直接返回 right。具体又可分为两种情况： p、q 其中一个在 root的右子树中，也就是p = root || q == root，此时假设 right指向 p； p、q 两结点都在 root的右子树中，此时的 right指向最近公共祖先结点。 left != null &amp;&amp; right == null: 与上种情况类似。 12345678public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if (root == null || root == p || root == q) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); if (left == null) return right; if (right == null) return left; return root;&#125; 时间复杂度：$O(n)$, 最坏情况下需要遍历所有的树结点。 空间复杂度：$O(n)$, 最坏情况下，递归深度达到 $n$。","categories":[{"name":"算法","slug":"算法","permalink":"http://raymond-zhao.top/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"剑指 Offer","slug":"剑指-Offer","permalink":"http://raymond-zhao.top/tags/%E5%89%91%E6%8C%87-Offer/"}]},{"title":"使用Mac的程序员也许会用到的内容","slug":"2020-04-27-Utils-Mac","date":"2020-04-27T14:43:21.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/04/27/2020-04-27-Utils-Mac/","link":"","permalink":"http://raymond-zhao.top/2020/04/27/2020-04-27-Utils-Mac/","excerpt":"","text":"iTerm2终端 安装 通过 Oh My Zsh 1$ git clone --depth=1 https://github.com/romkatv/powerlevel10k.git $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/themes/powerlevel10k 中国用户 - 码云加速 1$ git clone --depth=1 https://gitee.com/romkatv/powerlevel10k.git $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/themes/powerlevel10k Set ZSH_THEME=&quot;powerlevel10k/powerlevel10k&quot; in ~/.zshrc. 重启终端，根据提示操作。 Homebrew官网 Homebrew中文官网 官方命令文档 安装1$ /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\" 常用命令 命令 解释 brew help 查看帮助信息 brew update 更新 brew outdated 查看可更新的包 brew upgrade [formula] 升级所有或指定的可升级的软件包 brew install/uninstall [formula] 安装/卸载包 brew clean [formula] 清除所有/指定包的旧版本 brew list 查看已安装的包 brew search [formula] 搜索可用的包 Services 控制123456$ brew services list # 查看使用brew安装的服务列表$ brew services run formula|--all # 启动服务（仅启动不注册）$ brew services start formula|--all # 启动服务，并注册$ brew services stop formula|--all # 停止服务，并取消注册$ brew services restart formula|--all # 重启服务，并注册$ brew services cleanup # 清除已卸载应用的无用的配置 配置环境变量 在此假设读者已具备基本的 vim 操作技能，如果不具备的话可以使用右键选择常用的文本编辑软件进行编辑。 vim基本使用：i + 编辑内容 + esc + :wq Visual Studio Code方式一 打开 Visual Studio Code 同时按下Command+Shift+P，在命令框中输入Shell Command:Install &#39;code&#39; command in PATH 卸载的话: 执行上面那一步时就应该已经注意到了。 方式二 如果 Visual Studio Code 是在 Applications (英文操作系统)或者应用程序(中文操作系统)的话 编辑配置文件 vi ~/.bash_profile 添加Visual Studio Code环境变量 12# Setting PATH for Visual Studio Codeexport PATH=/Applications/Visual\\ Studio\\ Code.app/Contents/Resources/app/bin:$PATH 保存并退出:wq 重新加载配置文件 source ~/.bash_profile Sublime 如果 Sublime Text 是在 Applications(英文操作系统)或者应用程序(中文操作系统)的话 建立软连接，将 Sublime 的启动文件与 /use/local/bin 建立链接。 1$ ln -s /Applications/Sublime\\ Text.app/Contents/SharedSupport/bin/subl /usr/local/bin/. 编辑配置文件 vi ~/.bash_profile 添加 Sublime 环境变量，并重命名命令缩写，我在此选择的是subl。 123# Setting PATH for Sublimeexport PATH=\"/usr/local/bin:$PATH\"alias subl='/Applications/Sublime\\ Text.app/Contents/SharedSupport/bin/subl' 保存并退出:wq 重新加载配置文件 source ~/.bash_profile Typora方式一 如果 Typora 是在 Applications (英文操作系统)或者应用程序(中文操作系统)的话 建立软连接，将 Typora 的启动文件与 /use/local/bin 建立链接。 1$ ln -s /Applications/Typora.app/Contents/MacOS/Typora /usr/local/bin/. 编辑配置文件 vi ~/.bash_profile 添加 Typora 环境变量，并重命名命令缩写，我在此选择的是typo。 1alias typo='open -a Typora' 保存并退出:wq 重新加载配置文件 source ~/.bash_profile 细心的同学可能已经发现了，Sublime与Typora的环境变量配置方式均可，可以选择自己喜欢的。 Java 可以快速查看JDK安装位置的命令 1$ /usr/libexec/java_home -V 假设你的JDK是默认安装环境下的 编辑配置文件 vi ~/.bash_profile 添加JDK环境变量 12# Settings for Javaexport JAVA_HOME=$(/usr/libexec/java_home) 保存并退出:wq 重新加载配置文件 source ~/.bash_profile 如果安装了多个版本的JDK，并且需要切换的话，可以使用下面的方式。 123export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)export JAVA_HOME=$(/usr/libexec/java_home -v 11)export JAVA_HOME=$(/usr/libexec/java_home -v 13) Python312# 通过 Homebrew 安装的 Python3.8export PATH=\"/usr/local/opt/python@3.8/bin:$PATH\" MySQL12# Set PATH for MySQL@5.7export PATH=\"/usr/local/opt/mysql@5.7/bin:$PATH\" Node.js12# Set PATH for Node.js@12export PATH=\"/usr/local/opt/node@12/bin:$PATH\" 命令重命名 觉得终端内容太多，想清空的时候 clear 命令有点长？ 其他命令重命名也类似 12345678# Self Defined Clear Screenalias cls=\"clear\"# Self Defined Commandsalias ll='colorls -l --sd --gs --group-directories-first'alias ls='colorls --group-directories-first'alias tree=\"find . -print | sed -e 's;[^/]*/;|____;g;s;____|; |;g'\"alias doc=\"docsify\" iTerm2 命令标签页 新建标签页：command + T 关闭当前标签页：command + W 标签页切换：command + 数字(1，2...)或者command+左右方向键 分屏 垂直分屏：command + D 水平分屏：command + shift + D 窗口切换：command + option + 上下左右方向键 命令(常用) 跳到行首：control + A 跳到行尾：control + E 查看历史命令：command + ; 搜索历史命令：control + R 查看剪切板历史：command + shift + H 清除本行：control + U，再按 上下方向键 命令将不会再出现，下同。 清除光标之前的单词：control + W 清除光标之前的字符：control + H 清除当前光标的字符：control + D 清除当前光标到末尾：control + K 目录跳转：输入d后显示目录，再输入目录前的编号可以直接进入，省略cd ..命令 窗口 清屏：control + L/R，更推荐使用clear重命名为如cls的方式 查找：command + F 复制：光标选中即复制 自定义提示样式默认情况下, 提示符显示为 user@hostnam，比如xxxxx@xxxxdeMBP，可以手动去除。 whoami查看当前用户 vim ~/.zshrc，增加 DEFAULT_USER=&quot;user&quot; ，其中的user比如和 whoami 中的输出匹配 重载文件 source ~/.zshrc 但是如果跟我的终端一样就不需要配置了。 配置传送门：编程和酒 Vim 命令在这颗蔚蓝色的星球上，有一群程序员，在这一群程序员中有两个广为流传的工具，一个是编辑器之神 Vim，一个是神之编辑器 emacs。 插入模式 命令 作用（解释) i 插入到光标前面 I 插入到行的开始位置 a 插入到光标的后面 A 插入到行的最后位置 o, O 新开一行 esc 关闭插入模式 光标移动 命令 作用（解释） h,j,k,l h表示往左，j表示往下，k表示往右，l表示往上 Ctrl+f 上一页 Ctrl+b 下一页 w, e, W, E 跳到单词的后面，小写包括标点 b, B 以单词为单位往前跳动光标，小写包含标点 O 开启新的一行 ^ 一行的开始 $ 一行的结尾 gg 文档的第一行 [N]G 文档的第N行或者最后一行 复制与删除 命令 作用（解释) dd 删除一行 [N]dd 删除连续的多行，[N]为数字，1,2,3…表示要删除的行数 dw 删除一个单词 x 删除当前光标的字符 X 删除前一个字符 D 删除当前光标及之后的字符(整行) [N]yy 复制一行或者N行 yw 复制一个单词 p 粘贴 搜索模式 命令 作用（解释) /pattern 搜索（非插入模式) pattern代表要搜素的内容 ?pattern 往后搜索 n 光标到达搜索结果的前一个目标 N 光标到达搜索结果的后一个目标 编辑模式 命令 作用（解释） r 在插入模式替换光标所在的一个字符 J 合并下一行到上一行 s 删除光标所在的一个字符, 光标还在当行 S 删除光标所在的一行，光标还在当行，不同于dd u 撤销上一步操作 control + r 恢复上一步操作 . 重复最后一个命令 ~ 变换为大写 [N]&gt;&gt; 一行或N行往右移动一个tab [N]&lt;&lt; 一行或N行往左移动一个tab 窗口操作 命令 作用（解释) :split 水平方向分割出一个与含有原窗口相同内容的窗口 :vsplit 垂直方向分割出一个与含有原窗口相同内容的窗口 :close 关闭窗口 Ctrl+W 切换窗口, h到左边窗口，j到下方窗口，k到上方窗口，l到右边窗口 关闭 命令 作用（解释) :w 保存 :wq, :x 保存并关闭 :q 关闭（已保存） :q! 强制关闭 ! 强制，与上边的命令复合使用 参考 用命令行启动 Sublime 和 Typora","categories":[{"name":"Utils","slug":"Utils","permalink":"http://raymond-zhao.top/categories/Utils/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://raymond-zhao.top/tags/Mac/"}]},{"title":"Java面试里常见的比较","slug":"2020-04-17-Interview-Comparsions","date":"2020-04-17T09:07:20.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/04/17/2020-04-17-Interview-Comparsions/","link":"","permalink":"http://raymond-zhao.top/2020/04/17/2020-04-17-Interview-Comparsions/","excerpt":"","text":"前言在准备面试的过程中发现很多会需要相互比较的内容，为了方便自己与大家学习与比较，遂整理。 很多知识点的比较可能并不全面，甚至可能会出现错误，但是也只能暂列出来，等待以后慢慢考证，修改，尽量做到最准确、最细致。如果有读者看到出错的地方，还请不吝指出。 表格是 Typora 自动调整的格式，所以有些排版有些丑陋。 网络篇说说TCP和UDP的区别吧 TCP UDP 首部至少20字节，开销大 首部8字节，开销小 面向连接，应用程序在使用 TCP 协议之前，必须先建立 TCP 连接。在传送数据完毕后，必须释放已经建立的 TCP 连接。 无连接的，即发送数据之前不需要建立连接，发送数据之后也不需要断开连接，因此减少了开销和发送数据之前的时延。 面向字节流、TCP 中的 “流”(stream) 指的是流入到进程或从进程流出的字节序列。 面向报文 通过 TCP 连接传送的数据，无差错、不丢失、不重复，并且按序到达。 尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的连接状态表。 滑动窗口，流量控制，拥塞控制，停止等待协议，连续 ARQ 协议 没有拥塞控制，因此网络出现的拥塞不会使源主机的发送速率降低。这 传输效率低 传输效率高 一对一、点对点传输 支持一对一、一对多、多对多的传输 说说Cookie和Session的区别吧Cookie 简介 由服务器发给客户端的特殊信息，以文本的形式存放在客户端 客户端再次请求的时候，会把 Cookie 回发 服务器收到后，会解析 Cookie 生成与客户端相对应的内容 Session 简介 服务器端的机制，在服务器上保存的信息 解析客户端请求并操作 Session Id ，按需保存状态信息 实现方式 使用 Cookie 来实现 使用 URL 回写来实现 Cookie 和 Session 的区别 Cookie 数据存放在客户的浏览器上， Session 数据存放在服务器上 Session 相对于 Cookie 更安全 若考虑减轻服务器负担，应当使用 Cookie 说说 HTTP 和 HTTPS 的区别吧 方面 HTTP HTTPS 开发目的 发布和接收 HTML 页面浏览器和网站服务器之间传递信息 提供对网站服务器的身份认证保护交换数据的隐私与完整性。 默认端口 80 443 安全协议 无 SSL/TLS 加密方式 无 对称加密、非对称加密、哈希算法、数字签名 安全性 明文传输、相对较低 加密传输、相对较高 建立连接 TCP三次握手交换3个包 TCP3个包+SSL9个包 响应速度 相对较快 相对较慢 所需费用 免费 可能需要购买证书 HTTP与HTTPS-菜鸟教程 说说GET和POST的区别吧 比较方面 GET POST 请求主体(Body) 只有URL，无请求主体 有请求主体 响应主体 有 有 后退按钮/刷新 无副作用 数据会被重新提交 书签 可收藏为书签 不可收藏为书签 缓存 可被缓存 不能缓存 编码类型 application/x-www-form-urlencoded application/x-www-form-urlencodedmultipart/form-datatext/plain 历史 参数保留在浏览器历史中 参数不会保存在浏览器历史中 URL长度 有限制，但不同浏览器限制又不同常说的2KB其实是指IE8，Chrome和Apache为8KB 无限制(Post请求本身不限制长度，但是服务器是会限制的。) 对数据类型的限制 只允许 ASCII 字符 没有限制、也允许二进制数据，Percent Encoding编码 安全性 与 POST 相比，GET 的安全性较差，因为所发送的数据是 URL 的一部分。在发送密码或其他敏感信息时绝不要使用 GET POST 比 GET 更安全，因为参数不会被保存在浏览器历史或 web 服务器日志中。 幂等性 支持 不支持 可见性 数据在 URL 中对所有人都是可见的 数据不会显示在 URL 中 操作系统篇 有待完善 数据库篇说说 MyISAM 和 InnoDB 的区别吧 比较方面 MyISAM InnoDB 锁级别 只支持表级锁 支持表级锁、行级锁 事务 不支持事务 支持事务 外键支持 不支持外键 支持外键 索引支持 非聚簇索引、索引和数据是分开的 聚簇索引、数据文件本身就是索引文件 辅助索引 和主索引没有多大区别 data域存储相应记录主键的值而不是地址 全文索引 支持 FULLTEXT类型的全文索引 不支持FULLTEXT类型的全文索引但是InnoDB可以使用sphinx插件支持全文索引，并且效果更好。 索引数据结构 索引利用B+Tree实现 索引利用B+Tree树实现 统计行数 变量保存整个表的行数 select count() 需要select count(1) from tb 扫描表获取总行数 存储结构 表定义文件、数据文件、索引文件分开存放 所有的表都保存在同一个数据文件 自增支持 增长列必须是索引，如果是组合索引，自动增长可以不是第一列，可以根据前面几列进行排序后递增。 必须包含只有该字段的索引。引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列。 CRUD 查询性能极高 查询、修改、插入、删除总体比较高 说说 Redis 和 memcached 的区别吧 比较方面 Redis Memcached 数据结构 更丰富的数据结构支持String, Hash, List, Set, ZSet 支持简单的数据类型 String 数据持久化 支持数据持久化 不支持 集群支持 具有原生集群 没有原生集群 线程IO模型 单线程的IO多路复用模型 多线程非阻塞IO复用的网络模型 附加功能 发布订阅模式、主从分区、序列化支持、脚本支持 多线程服务支持 事件库 自封装简易事件库AeEvent LibEvent事件库 说说 B+ 树和 B-树的区别吧对于 $m$ 阶 $B-、B+$ 树 比较方面 B-树 B+树 结点与分支 具有 $n$ 个关键字的结点有 $n+1$ 个分支 具有 $n$ 个关键字的结点有 $n+1$ 个分支 根节点关键字个数范围 $1 \\le n \\le m-1$ $2 \\le n \\le m$ 非根节点关键字个数范围 $\\lceil \\frac{m}{2}\\rceil - 1 \\le n\\le m-1$ $\\lceil \\frac{m}{2}\\rceil \\le n\\le m$ 叶子结点 存储关键字 包含信息，并且包含全部关键字，叶子结点引出的指针指向记录。 非叶子结点 每个关键字对应一条记录的存储地址 仅起索引作用，结点中的每个索引项仅含有对应子树的最大关键字和指向该子树的指针，不含有关键字对应记录的存储地址。 额外指针 无 有一个指针指向关键字最小的叶子结点，所有叶子结点连接成一个线性表 说说跳表和红黑树的区别吧 比较方面 跳表 红黑树 实现难易程度 相对简单 相对复杂 插入、删除结点 仅需局部调整 可能需要全局调整 时间复杂度 总体概率上 $O(\\lg n)$ $O(\\lg n)$ 非并发性能 所差无几 所差无几 并发性能 相对较高 相对较低 Java基础篇说说String、StringBuffer、StringBuilder的区别吧JDK 1.8 中 比较方面 String StringBuilder StringBuffer 底层实现 final char[] char[] char[] 对象可变性 不可变 可变 可变 继承自 无 AbstractStringBuilder AbstractStringBuilder 实现接口 Comparable&lt;String&gt;, CharSequence CharSequence CharSequence 线程安全 否 否 是 对象操作： String: 如果字符串常量池不存在的话会生成新对象，然后将指针指向新的 String 对象 StringBuilder, StringBuffer: 对对象本身进行操作，而不是生成新的对象并改变对象引用。 说说 Overload 和 Override 的区别吧 比较方面 重载方法 重写方法 发生范围 同一个类中 子类重写父类，或者实现接口定义方法 参数列表 必须修改 一定不能修改 返回类型 可修改 不可修改 异常 可修改 可以减少或删除，一定不能抛出新的或者更广的异常 访问修饰符 可修改 一定不能做更严格的限制（可以降低限制） 发生阶段 运行期 编译器 说说接口和抽象类的区别吧 比较方面 接口 抽象类 方法修饰符 默认 public, 使用 default 的话需要添加方法体 可以被各种修饰符修饰 与类的关系 一个类可以实现多个接口 只能继承一个抽象类 变量 仅可有static、final 变量 可有各种变量 设计目的 对类的行为进行约束 代码复用 说说成员变量和局部变量的区别吧 比较方面 成员变量 局部变量 所属对象 类 类中的方法 可用修饰符 private、public、static、final final 存储位置 被static修饰，成员变量属于类，存储在运行时数据区的方法区，未被static修饰存在堆 存储在运行时数据区的Java虚拟机栈 生存时间 随着对象的创建而存在 随着方法的调用而自动消失 初始化值 未被赋初值则为类型的默认值，final修饰的需要显式赋值 不会自动赋值 说说 for 和 foreach 的区别吧 比较方面 for foreach 更适合作用于 数组 集合 实现基础 数组下标索引 迭代器 Iterator 删除/修改元素 可以实现 出现 ConcurrentModificationException 遍历不修改元素 效率高 效率低 在测试时中向 ArrayList、LinkedList 添加100万个元素，发现总是 for 循环的遍历时间比较快，只是遍历，未做修改添加删除。 说说 BIO、NIO、AIO的区别吧JavaGuide-GitHub 说说 == 和 equals 的区别吧== : 它的作用是判断两个对象的地址是不是相等。即，判断两个对象是不是同一个对象(基本数据类型==比较的是值，引用数据类型==比较的是内存地址)。 equals() : 它的作用也是判断两个对象是否相等。但它一般有两种使用情况： 情况 1：类没有覆盖 equals() 方法。则通过 equals() 比较该类的两个对象时，等价于通过“==”比较这两个对象。 情况 2：类覆盖了 equals() 方法。一般，我们都覆盖 equals() 方法来比较两个对象的内容是否相等；若它们的内容相等，则返回 true。 说说 Error 和 Exception 的区别吧Error类和Exception类的父类都是Throwable类，他们的区别是： Error类一般是指与虚拟机相关的问题，如系统崩溃，虚拟机错误，内存空间不足，方法调用栈溢等。对于这类错误的导致的应用程序中断，仅靠程序本身无法恢复和预防，遇到这样的错误，建议让程序终止。 Exception类表示程序可以处理的异常，可以捕获且可能恢复。遇到这类异常，应该尽可能处理异常，使程序恢复运行，而不应该随意终止异常。 Exception类又分为运行时异常（Runtime Exception）和受检查的异常(Checked Exception )，运行时异常编译能通过，但是一运行就终止了，程序不会处理运行时异常，出现这类异常，程序会终止。而受检查的异常，要么用 try-catch 捕获，要么用 throws 字句声明抛出，交给它的父类处理，否则编译不会通过。 Java集合框架篇说说 List、Set、Map 的区别吧 比较方面 List Set Map 存储数据 同类型数据 同类型数据 键值对 重复元素 允许重复 不可重复 HashMap不允许key 重复，允许value重复 顺序性 有序 无序 TreeMap有序、HashMap 无序 对null支持 不支持 最多一个null HashMap可允许一个null key、value 主要实现类 ArrayList,LinkedList,Vector HashSet,LinkedHashSet HashMap 说说 ArrayList、LinkedList、Vector 的区别吧 比较方面 ArrayList Vector LinkedList 继承的类 AbstractList AbstractList AbstractSequentialList 实现的接口 List&lt;E&gt;, RandomAccess, Cloneable List&lt;E&gt;, RandomAccess, Cloneable List&lt;E&gt;, Deque&lt;E&gt;, Cloneable 底层实现 对象数组transient Object[] elementData; 对象数组 protected Object[] elementData; 双向链表 初始容量 10 10 0 扩容操作 有 有 无 线程安全 否 是 否 最适合操作 查询 查询 增加、删除 说说 HashMap、HashTable、ConcurrentHashMap的区别吧 比较方面 HashMap Hashtable ConcurrentHashMap 是否线程安全 否 是 是 线程安全采用的方式 无 采用synchronized类锁，效率低 CAS + synchronized，锁住的只有当前操作的bucket，不影响其他线程对其他bucket的操作，效率高 数据结构 数组+链表+红黑树(链表长度超过8则转红黑树) 数组+链表 数组+链表+红黑树(链表长度超过8则转红黑树) 是否允许null键值 是 否 否 哈希地址算法 (h=key.hashCode())^(h&gt;&gt;&gt;16 key.hashCode() (h=key.hashCode())^(h&gt;&gt;&gt;16)&amp;0x7fffffff 定位算法 (n-1)&amp;hash (hash&amp;0x7fffffff)%n (n-1)&amp;hash 扩容算法 当键值对数量大于阈值，则容量扩容到原来的2倍 当键值对数量大于等于阈值，则容量扩容到原来的2倍+1 当键值对数量大于等于sizeCtl，单线程创建新哈希表，多线程复制bucket到新哈希表，容量扩容到原来的2倍 链表插入 将新节点插入到链表尾部 将新节点插入到链表头部 将新节点插入到链表尾部 继承的类 继承abstractMap抽象类 继承Dictionary抽象类 继承abstractMap抽象类 实现的接口 实现Map接口 实现Map接口 实现ConcurrentMap接口 默认容量大小 16 11 16 默认负载因子 0.75 0.75 0.75 统计size方式 直接返回成员变量size 直接返回成员变量count 遍历CounterCell数组的值进行累加，最后加上baseCount的值即为size 说说 JDK1.7 与 JDK 1.8 中 HashMap 的变化吧HashMap源码笔记 说说 JDK1.7 与 JDK 1.8 中 ConcurrentHashMap 的变化吧ConcurrentHashMap源码笔记 说说TreeMap、HashMap、LindedHashMap、HashTable的区别吧 比较方面 HashMap TreeMap LinkedHashMap HashTable 迭代顺序 随机 根据键的顺序 根据插入顺序 随机 Get、Put、Remove、ContainsKey效率 $O(1)$ $O(\\lg n)$ $O(1)$ $O(1)$ Null Keys/Keys 允许 不允许 允许 不允许 接口 Map Map、SortedMap、NavigableMap Map Map 线程安全 否 否 否 是 实现 桶+链表+红黑树 红黑树 HashTable、LinkedList、桶的双向链表 桶 备注 高效 维持树需要付出代价 无额外开销的具有TreeMap的优势 过时 多线程与并发篇说说进程与线程的区别吧进程是一个独立的运行环境，而线程是在进程中执行的一个任务。他们两个本质的区别是是否单独占有内存地址空间及其它系统资源(比如I/O）： 进程单独占有一定的内存地址空间，所以进程间存在内存隔离，数据是分开的，数据共享复杂但是同步简单，各个进程之间互不干扰；而线程共享所属进程占有的内存地址空间和资源，数据共享简单，但是同步复杂。 进程单独占有一定的内存地址空间，一个进程出现问题不会影响其他进程，不影响主程序的稳定性，可靠性高；一个线程崩溃可能影响整个程序的稳定性，可靠性较低。 进程单独占有一定的内存地址空间，进程的创建和销毁不仅需要保存寄存器和栈信息，还需要资源的分配回收以及页调度，开销较大；线程只需要保存寄存器和栈信息，开销较小。 另外一个重要区别是，进程是操作系统进行资源分配的基本单位，而线程是操作系统进行调度的基本单位，即CPU分配时间的单位 。 说说 run() 和 start() 的区别吧 start() : 通过 start () 方法来启动的新线程，处于就绪(可运行)状态，并没有运行，一旦得到 cpu 时间片，就开始执行相应线程的 run () 方法，这里方法 run () 称为线程体，它包含了要执行的这个线程的内容，run 方法运行结束，此线程随即终止。start () 不能被重复调用。用 start 方法来启动线程，真正实现了多线程运行，即无需等待某个线程的 run 方法体代码执行完毕就直接继续执行下面的代码。这里无需等待 run 方法执行完毕，即可继续执行下面的代码，即进行了线程切换。 run() : run() 就和普通的成员方法一样，可以被重复调用。如果直接调用 run 方法，并不会启动新线程！程序中依然只有主线程这一个线程，其程序执行路径还是只有一条，还是要顺序执行，还是要等待 run 方法体执行完毕后才可继续执行下面的代码，这样就没有达到多线程的目的。 总结：调用 start() 方法会创建一个新的子线程并启动， run() 方法只是 Thread 的一个方法调用，还是在原线程里运行。 说说 wait() 和 sleep() 的区别吧 sleep() 是 Thread 类的方法， wait() 是 Object 类的方法 sleep() 方法可以在任何地方使用 wait() 方法只能在 synchronized 方法或 synchronized 块中使用 Thread.sleep() : 只会让出 CPU，不会导致锁行为的改变 Object.wait() : 不仅让出 CPU，还会释放已经占有的同步资源锁 说说 Thread 和 Runnable 的区别吧 Thread 是实现了 Runnable 接口的类，使得 run() 支持多线程 因为类的 单一继承原则，推荐多使用 Runnable 接口 说说 synchronized 和 volatile 的区别吧 Synchronized Volatile 实例方法、类方法(静态方法)、代码块 只修饰变量 保证可见性和原子性 保证可见性，不保证原子性 可能会造成线程阻塞 不会造成线程阻塞 实现基于对象头中的Mark Word 实现基于JMM 锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住(加锁) (不是加锁)本质是在告诉 JVM 当前变量在寄存器（工作内存）中的值是不确定的，需要从主存中读取 解决对个线程之间访问资源的同步性 解决变量在多个线程之间的可见性 说说 synchronized 和 ReentrantLock 的区别吧 synchronized ReentrantLock 关键字 类 可重入 可重入 基于JVM，JVM层面 java.util.concurrent.locks包下的类、JDK层面 基于对象头中的Mark Word实现 基于AQS实现 非公平锁 可实现公平锁 不可中断 等待可中断、超时等待、选择性通知 使用完后自动释放锁 需要手动加锁与释放锁 说说乐观锁与悲观锁的区别吧 比较方面 乐观锁 悲观锁 核心思想 总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁， 总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是更新时会检查一下 主要实现方式 版本号机制、CAS synchronized、Lock、排它锁 适用场景 多读少写 多写少读 缺点 ABA 问题、循环时间长开销大、只能保证一个共享变量的原子操作 容易造成阻塞 说说类锁与对象锁的区别吧 获取对象锁的两种方法 同步代码块 ( synchronized(this), synchronized(类实例对象) )，锁的是小括号中的实例对象 同步非静态方法 ( synchronized method )，锁的是当前对象的实例对象 获取类锁的两种方法 同步代码块 ( synchronized(类.class) )，锁的是小括号中的类对象 (Class 对象) 同步静态方法 (synchronized static method)，锁的是当前对象的类对象 对象锁和类锁的总结 有线程访问对象的同步代码块时，另外的线程可以访问对象的非同步代码块 若锁住的是同一个对象，一个线程在访问对象同步代码块时，另一个访问对象的同步代码块的线程会被阻塞 若锁住的是同一个对象，一个线程在访问对象同步方法时，另一个访问对象的同步方法的线程会被阻塞 若锁住的是同一个对象，一个线程在访问对象同步代码块时，另一个访问对象的同步方法的线程会被阻塞；反之亦然 同一个类的不同对象的对象锁互不干扰 类锁由于也是一种特殊的对象锁，因此表现和前 4 条一致，而由于一个类只有一把对象锁，所以同一类的不同对象使用类锁将会是同步的 类锁和对象锁互不干扰 JVM与GC篇说说几种类加载器之间的区别吧说说主内存与工作内存的区别吧 主内存 存储 Java 实例对象 包括成员变量，类信息，常量，静态变量等 属于数据共享的区域，多线程并发操作时会引发线程安全问题 工作内存 存储当前方法的所有本地变量信息，本地变量对其他线程不可见 字节码行号指示器，Native 方法信息 属于线程私有数据区域，不存在线程安全问题 JMM 与 Java 内存区域划分 JMM 与 Java 内存区域划分是不同的概念层次。 JMM 描述的是一组规则，围绕原子性，有序性，可见性展开。 它们的相似点是都存在共享区域与私有区域。 主内存与工作内存的数据存储类型以及操作方式 方法里的基本数据类型本地变量将直接存储在工作内存的栈帧结构中 引用类型的本地变量：引用存储在工作内存中，实例存储在主内存中 成员变量、static 变量、类信息均会存储在主内存中 主内存共享的方式是线程各自拷贝一份数据到工作内存，操作完成后刷新回主内存 说说 CMS 与 G1 的区别吧 比较方面 CMS G1 设计目标 获取最短回收停顿时间 可预测垃圾回收的停顿时间 作用空间 老年代 不再区分新生代与老年代, 划分为Region 清除算法 标记-清除 整体上标记-清除、局部上复制 回收过程 初始标记、并发标记、重新标记、并发清除 初始标记、并发标记、最终标记、筛选回收 STW的阶段 初始标记、重新标记 初始标记、最终标记、筛选回收 空间利用率 相对较低 相对较高 优点 并发收集、低停顿 分区收集、空间整合、可预测停顿 缺点 资源敏感，内存碎片，浮动垃圾 分配大对象时可能难以找到空间 常用框架篇说说 @Component 和 @Bean的区别吧 比较方面 @Component @Bean 作用对象 类 方法 目的 注册bean到Spring容器中 注册bean到Spring容器中 作用方式 扫描类路径自动侦测以及自动装配到Spring容器中 告诉Spring这是某个类的实例 自定义性 一般 较强，引用第三方库只能使用@Bean 说说 JDK Proxy 和 Cglib 的区别吧JDK动态代理是面向接口，在创建代理实现类时比CGLib要快，创建代理速度快。 CGLib动态代理是通过字节码技术底层生成一个继承代理类的类来实现，然后重写代理类的方法(如果被代理类被final关键字所修饰，将会失败，因为被final修饰的类无法被集成; 如果代理类中方法被final修饰，该方法无法代理成功, 因为被final修饰的方法无法被重写)，但是运行速度比JDK动态代理要快。 Spring CloudNginx与RibbonFeign与OpenFeignEureka与ZookeeperHystrix与Sentinel","categories":[{"name":"面试","slug":"面试","permalink":"http://raymond-zhao.top/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://raymond-zhao.top/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"JVM-运行时数据区","slug":"2020-04-04-JVM-Memory","date":"2020-04-04T11:00:02.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/04/04/2020-04-04-JVM-Memory/","link":"","permalink":"http://raymond-zhao.top/2020/04/04/2020-04-04-JVM-Memory/","excerpt":"","text":"Java内存区域与内存溢出异常Java虚拟机在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些区域有各自的用途，以及创建和销 毁的时间，有的区域随着虚拟机进程的启动而一直存在，有些区域则是依赖用户线程的启动和结束而建立和销毁。Java虚拟机所 管理的内存包括以下几个运行时数据区域： 由图中可以看出，线程私有的区域为虚拟机栈、本地方法栈、程序计数器，而线程共享的区域为堆，方法区。 运行时数据区程序计数器程序计数器(Program Counter Register)是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，它是程序控制流的指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 由于 JVM 的多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器(对于多核处理器来说是一个内核)都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，这类内存区域为“线程私有”的内存。 如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址。 如果正在执行的是本地(Native)方法，这个计数器值则应为空(Undefined)。此内存区域是唯一一个没有规定任何 OOM 情况的区域。 虚拟机栈线程私有的，它的生命周期与线程相同。 虚拟机栈描述的是Java方法执行的线程内存模型：每个方法被执行的时候，Java虚拟机都会同步创建一个栈帧用于存储局部变量表、操作数栈、动态连接、方法出口等信息。每一个方法被调用直至执行完毕的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 局部变量表存放了各种基本数据类型(boolean、byte、char、short、int、float、long、double)、对象引用(reference类型，它并不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或者其他与此对象相关的位置)和return Address类型(指向了一条字节码指令的地址)。 这个内存区域有两类异常状况： 如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常； 如果Java虚拟机栈容量可以动态扩展，当栈扩展时无法申请到足够的内存会抛出OutOfMemoryError异常。 本地方法栈本地方法栈(Native Method Stacks)与虚拟机栈所发挥的作用非常相似，区别只是虚拟机栈为虚拟机执行Java方法(也就是字节码服务，而本地方法栈则是为虚拟机使用到的本地(Native)方法服务。 与虚拟机栈一样，本地方法栈也会在栈深度溢出或者栈扩展失败时分别抛出StackOverflowError和OutOfMemoryError异常。 堆(Heap)Java堆(Java Heap)是虚拟机所管理的内存中最大的一块，是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，在《Java虚拟机规范》中对Java堆的描述是:“所有的对象实例以及数组都应当在堆上分配”. Java 堆也被称作“GC堆”(Garbage Collected Heap)。从回收内存的角度看，由于现代垃圾收集器大部分都是基于分代收集理论设计的，所以Java堆中经常会出现“新生代”“老年代”“永久代”“Eden空间”“From Survivor空间”“To Survivor空间”等名词。从分配内存的角度看，所有线程共享的Java堆中可以划分出多个线程私有的分配缓冲区(ThreadLocalAllocationBuffer，TLAB)，以提升对象分配时的效率。 Java堆可以处于物理上不连续的内存空间中，但在逻辑上应该被视为连续的。 Java堆既可以被实现成固定大小的，也可以是可扩展的(通过参数-Xmx和-Xms)。如果在Java堆中没有内存完成实例分配，并且堆也无法再扩展时，Java虚拟机将会抛出 OutOfMemoryError 异常。 方法区(Method Area)方法区(Method Area)与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。 《Java虚拟机规范》对方法区的约束是非常宽松的，除了和 Java 堆一样不需要连续的内存和可以选择固定大小或者可扩展外，甚至还可以选择不实现垃圾收集。 这区域的内存回收目标主要是针对常量池的回收和对类型的卸载。 如果方法区无法满足新的内存分配需求时，将抛出OutOfMemoryError异常。 运行时常量池运行时常量池(Runtime Constant Pool)是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池表(ConstantPoolTable)，用于存放编译期生成的各种字面量与符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。 直接内存直接内存(Direct Memory)并不是虚拟机运行时数据区的一部分，也不是《Java虚拟机规范》中定义的内存区域。但是这部分内存也被频繁地使用，而且也可能导致OutOfMemoryError异常出现。 new 的过程发生了什么？对象的创建当Java虚拟机遇到一条字节码 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 在类加载检查通过后，接下来虚拟机将会在堆(Heap)上为新生对象分配内存，分配内存的方式可以选择指针碰撞或者空闲列表。 指针碰撞：假设Java堆中内存是绝对规整的，所有被使用过的内存都被放在一边，空闲的内存被放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间方向挪动一段与对象大小相等的距离。 空闲列表：如果Java堆中的内存并不规整，已被使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录。 解决对象创建时可能出现的线程不安全这个问题有两种可选方案: 一种是对分配内存空间的动作进行同步处理—实际上虚拟机是采用CAS配上失败重试的方式保证更新操作的原子性； 一种是把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲(Thread Local Allocation Buffer，TLAB)，哪个线程要分配内存，就在哪个线程的本地缓冲区中分配，只有本地缓冲区用完了，分配新的缓存区时才需要同步锁定。虚拟机是否使用TLAB，可以通过-XX:+/-UseTLAB参数来设定。 内存分配完成之后，虚拟机必须将分配到的内存空间(但不包括对象头)都初始化为零值。 接下来，Java虚拟机还要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码(实际上对象的哈希码会延后到真正调用Object::hashCode()方法时才计算)、对象的GC分代年龄等信息。这些信息存放在对象的对象头(Object Header)之中。 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了。但是从Java程序的视角看来，对象创建才刚刚开始，构造函数还没有执行，即Class文件中的&lt;init&gt;()方法还没有执行，所有的字段都为默认的零值，对象需要的其他资源和状态信息也还没有按照预定的意图构造好。 new指令之后会接着执行&lt;init&gt;()方法，按照程序员的意愿对对象进行初始化，这样一个真正可用的对象才算完全被构造出来。 总结一下，对象的创建大致为： 类加载检查 分配内存 将新分配的内存初始化为零值 设置对象头 执行Class文件中的&lt;init&gt;() 方法 对象的内存布局在HotSpot虚拟机里，对象在堆内存中的存储布局可以划分为三个部分:对象头(Header)、实例数据(InstanceData)和对齐填充(Padding)。 对象头包括两部分： Mark Word: 用于存储对象自身的运行时数据，如哈希码(HashCode)、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等。 如对象未被同步锁锁定的状态下，MarkWord的32个比特存储空间中的25个比特用于存储对象哈希码，4个比特用于存储对象分代年龄，2个比特用于存储锁标志位，1个比特固定为0，在其他状态(轻量级锁定、重量级锁定、GC标记、可偏向)下对象的存储内容如下表所示。 类型指针：即对象指向它的类型元数据的指针，Java虚拟机通过这个指针来确定该对象是哪个类的实例。 对象的访问定位Java程序会通过栈上的reference数据来操作堆上的具体对象。 句柄访问：Java堆中将可能会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自具体的地址信息，其结构如图所示。 指针访问：Java堆中对象的内存布局就必须考虑如何放置访问类型数据的相关信息，reference中存储的直接就是对象地址，如果只是访问对象本身的话，就不需要多一次间接访问的开销。使用直接指针来访问最大的好处就是速度更快，它节省了一次指针定位的时间开销。 总览图片来源于：Process On - JVM 内存模型完整版 参考 《深入理解Java虚拟机：JVM高级特性与最佳实践》（第3版） - 周志明","categories":[{"name":"JVM","slug":"JVM","permalink":"http://raymond-zhao.top/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://raymond-zhao.top/tags/JVM/"},{"name":"GC","slug":"GC","permalink":"http://raymond-zhao.top/tags/GC/"}]},{"title":"常用类库与技巧","slug":"2020-04-03-Java-ClassLib","date":"2020-04-03T07:03:00.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/04/03/2020-04-03-Java-ClassLib/","link":"","permalink":"http://raymond-zhao.top/2020/04/03/2020-04-03-Java-ClassLib/","excerpt":"","text":"1 异常处理1.1 异常处理机制 异常处理机制主要回答了三个问题 What：异常类型回答了什么被抛出 Where：异常堆栈跟踪回答了在哪抛出 Why：异常信息回答了为什么被抛出 Java 异常体系 RuntimeException : 不可预知的，程序应当自行避免 非 RuntimeException ：可预知的，从编译器校验的异常 从概念角度解析 Java 的异常处理机制 Error：程序无法处理的系统错误，编译器不做检查 Exception：程序可以处理的异常，捕获后可能恢复 总结：前者是程序无法处理的错误，后者是可以处理的异常 从责任角度解析 Java 的异常处理机制 Error 属于 JVM 需要承担的责任 RuntimeException 是程序应该承担的责任 Checked Exception 可检查异常是 Java 编译器应该负担的责任 1.2 常见 Error 及 Exception RuntimeException NullPointerException ：空指针引用异常 ClassCastException ：类型强制转换异常 IllegalArgumentException IndexOutOfBoundsException NumberFormatException 非 RuntimeException ClassNotFoundException IOException Error NoClassDefFoundError StackOverflowError 深度递归导致栈被耗尽而抛出的异常 OutOfMemoryError 内存溢出 1.3 异常处理方法 抛出异常：创建异常对象，交由运行时系统处理 捕获异常：寻找合适的异常处理器处理异常，否则终止运行 1.4 异常处理原则 具体明确：抛出的异常应能通过类名和 message 准确说明异常的类型和产生异常的原因 提早抛出：应尽可能早的发现并抛出异常，便于精确定位问题 延迟捕获：异常的捕获和处理应尽可能延迟，让掌握更多信息的作用域来处理异常 1.5 高效主流的异常处理框架在用户看来，应用系统发生的所有异常都是应用系统内部的异常 设计一个通用的继承自 RuntimeException 的异常来统一处理 其余异常都统一转移为上述异常 AppException 在 catch 之后，抛出上述异常的子类，并提供足以定位的信息 由前端接收 AppException 做统一处理 1.6 try-catch 的性能 Java异常处理消耗性能的地方 try-catch 块影响 JVM 的优化 异常对象实例需要保存堆栈快照等信息，开销较大 2 集合框架 数据结构考点 数组和链表的区别 链表的操作，如反转，链表环路检测，双向链表，循环链表相关操作 队列，栈的应用 二叉树的遍历方式及其递归和非递归的实现 红黑树的旋转 算法考点 内部排序：如递归排序，交换排序(冒泡，快排)，选择排序，插入排序 外部排序：应掌握如何利用有限的内存配合海量的外部存储来处理超大的数据集， 考点扩展 哪些排序是不稳定的，稳定意味着什么？ 不同数据集，各种排序最好或最坏的情况 如何优化算法 2.1 集合框架 2.2 集合之 List 与 Set 2.3 集合之 Map 见文章HashMap 3 J.U.C 知识点梳理 java.util.concurrent : 提供了并发编程的解决方案 CAS 是 java.util.concurrent.atomic 包的基础 AQS 是 java.util.concurrent.locks 包以及一些常用类如 Semophore、ReentrantLock 等类的基础 3.1 包的分类 线程执行器： executor 锁： locks 原子变量类： atomic 并发工具类 ：tools 并发集合类： collections 3.2 并发工具类 闭锁： CountDownLatch 让主线程等待一组事件发生后继续执行 栅栏： CyclicBarrier 阻塞当前线程，等待其它线程，所有线程必须同时到达栅栏的位置后才能继续执行 所有线程到达栅栏处，可以触发执行另外一个预先设置的线程 信号量： Semaphore 控制某个资源可被同时访问的线程个数 交换器： Exchanger 两个线程到达同步点后，相互交换数据 4 IO4.1 BIO BIO - InputStream、OutputStream、Reader 和 Writer 4.2 NIONonBlock-IO：构建多路复用的，同步非阻塞 IO 4.2.1 NIO的核心 Channels 通道 FileChannel DatagramChannel SocketChannel ServerSocketChannel Buffers 缓冲区 ByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer LongBuffer ShortBuffer MappedByteBuffer Selectors 选择器 NIO有、而IO没有 选择器用于使用单个线程处理多个通道。因此，它需要较少的线程来处理这些通道。线程之间的切换对于操作系统来说是昂贵的。 因此，为了提高系统效率选择器是有用的。 4.2.2 NIO读写数据方式通常来说NIO中的所有IO都是从 Channel（通道） 开始的。 从通道进行数据读取 ：创建一个缓冲区，然后请求通道读取数据。 从通道进行数据写入 ：创建一个缓冲区，填充数据，并要求通道写入数据。 4.2.3 IO 多路复用调用系统级别的 select/poll/epoll 名称 描述 效率问题 消息传递方式 select 单个进程所能打开的最大连接数由 FD_SETSIZE 宏定义 因为每次调用时都会对连接进行线性遍历，所以随着 FD 的增加会造成遍历速度的线性下降的性能问题 内核需要将消息传递到用户空间，需要内核的拷贝动作 poll 本质上与select没有区别，但是它没有最大连接数的限制，原因是它是基于链表来存储的 同上 同上 epoll 虽然连接数有上限，但是很大，1G内存的机器上可以打开10万左右连接 由于epoll是根据每个FD上的 callback 函数来实现的，只有活跃的 socket 才会主动调用callback，所以性能与socket 的活跃程度有关 通过内核和用户空间共享一块内存来实现，性能较高 4.3 AIOAsynchronous IO: 基于事件和回调机制 AIO如何进一步处理结果： 基于回调：实现 CompletionHandler 接口，调用时触发回调函数 返回 Future : 通过 isDone() 查看是否准备好，通过 get() 等待返回数据 4.2 BIO、NIO、AIO对比 属性、模型 BIO NIO AIO blocking 阻塞并同步 非阻塞但同步 非阻塞并异步 线程数 1:1 1:N 0:N 复杂度 简单 较复杂 复杂 吞吐量 低 高 高","categories":[{"name":"Java","slug":"Java","permalink":"http://raymond-zhao.top/categories/Java/"}],"tags":[{"name":"Java类库","slug":"Java类库","permalink":"http://raymond-zhao.top/tags/Java%E7%B1%BB%E5%BA%93/"}]},{"title":"Linux常用知识","slug":"2020-03-30-OS-Linux","date":"2020-03-30T14:44:09.000Z","updated":"2022-05-07T03:07:02.000Z","comments":true,"path":"2020/03/30/2020-03-30-OS-Linux/","link":"","permalink":"http://raymond-zhao.top/2020/03/30/2020-03-30-OS-Linux/","excerpt":"","text":"Linux 的体系结构 体系结构主要分为用户态(用户上层活动)和内核态 内核：本质上是一段管理计算机硬件设备的程序 系统调用：内核的访问接口，是一种能再简化的操作。 公用函数库：系统调用的组合拳 Shell: 命令解释器，可编程 如何操作特定的文件检索文件名 find12345678910find path [options] params# 作用：在指定目录下查找文件# 不加 path 作用域时的默认路径为当前目录及其子目录find -name \"helloworld.java\"# linux 系统中 / 代表 root 根目录 ~ 代表 home 目录find / -name \"helloworld.java\"# 支持模糊查询find ~ -name \"hello*\"# -iname 忽略大小写find ~ -iname \"hello*\" 检索文件内容 grep123456# grep : global regular expression print# 用于查找文件里符合条件的字符串 可以使用正则表达式 最终输出到终端grep [options] pattern file# 从以 hello 开头的文件中寻找含 hello 的字符串grep \"hello\" \"hello*\" 管道操作符 |123456# 可将指令连接起来 前一个指令的输出(stdout)作为后一个指令的输入(stdin)# 只处理前一个命令的正确输出 不处理错误输出# 右边的命令必须能够接收标准输入流 否则传递过程中会被抛弃find ~ | grep \"hello\"# grep -o# grep -v 对文件内容做统计 awk123456789101112131415awk [options] 'cmd' file# 一次读取一行文本 按输入分隔符进行切片 切成多个组成部分# 将切片直接保存在内建的变量中 $1, $2... ($0 表示行的全部)# 支持对耽搁切片的判断，支持循环判断，默认分隔符为空格。# 打印 netstat.txt 文件中第一列，第四列的内容awk '&#123;print $1, $4&#125;' netstat.txt# 打印 netstat.txt 文件中 第一列中为 tcp 第二列为 1 的所有内容# NRawk '$1==\"tcp\" &amp;&amp; $2=1 &#123;print $0&#125;' netstat.txt# enginearr[] 为变量名# 可以先用 grep 过滤数据，然后用 awk 来整理awk '&#123;enginearr[$1]++&#125;END&#123;for(i in enginearr) print i \"\\t\" enginearr[i]&#125;' 批量替换掉文档中的内容 sed123456// replace.javaStr a = \"The girl's boyfriend is Jack\".Str b = \"The girl often chats with Jack and Jack is Jack\".Str c = \"The girl loves Jack so muck\". Integer bf = new Integer(2); 12345678910111213141516171819202122232425262728sed [option] 'sed command' filename# 全名 stream editor 流编辑器# 适合用于对文本的行内容进行处理# 将 replace.java 中所有 Str 字符串替换为 String# 第一个 / 之前的 s 代表对字符串进行操作 # 第一个 / 与第二个 / 之间的代表要操作的内容# 第二个 / 与第三个 / 之间的内容表示要操作后的内容sed 's/^Str/String/' replace.java# 上面的命令指示将修改后的内容输出到 终端# 如果需要输出到文件中需要加 -ised -i 's/^Str/String/' replace.java# \\ 为转义字符# 将 replace.java 中所有以 . 作为结尾的变为以 ; 结尾的sed -i 's/\\.$/\\;/' replace.java# 默认只替换每行文件中第一个出现等于 Jack 的字符串sed -i 's/Jack/me/' replace.java# 全局替换sed -i 's/Jack/me/g' replace.java# 删除 3-4 行的空行 注意 ^ 与 *$ 之间的空格代表空行 /d 代表删除sed -i '/^ *$/d' replace.java# 删除含有 Integer 的行sed -i '/Integer/d' replace.java 查看或者编辑文件1234567891011$ cat 猫一眼, 可以合并文件$ head -n N 头几行$ tail -n N 尾几行$ more 较大文件, 分页显示, 提示文件显示百分比$ less less 可以随意浏览文件, 而 more 仅能向前移动, 而且 less 在查看之前不会加载整个文件$ vi $ vim# 如何查看大文件# 就跟所有操作大文件的方法一样: 分治# vim默认加载整个文件, 但是可以按需取用, 使用 less 最好, 可以把命令直接加进去. 3 如何查看系统的负载1234567891011121314151617$ uptime # 获取主机运行时间和查询linux系统负载等信息$ w # 和uptime差不多$ top # 实时监控系统状态$ iostat -x 1 10 # 监控IO, -x表示显示所有参数信息, 1表示每秒监控一次, 10 表示工监控10次$ vmstat # 获得有关进程、虚存、页面交换空间及CPU活动的信息$ free -m # 查看系统内存的使用情况, -m参数表示按照兆字节展示$ sar -n DEV 1 # 查看网络设备的吞吐率$ mpstat -P ALL 1 # 显示每个CPU的占用情况, 如果CPU占用率特别高, 可能是单线程应用程序引起$ dmesg | tail # 输出系统日志的最后10行","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://raymond-zhao.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://raymond-zhao.top/tags/Linux/"}]},{"title":"好好学 Redis","slug":"2020-03-30-Redis-Basic","date":"2020-03-30T10:52:49.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/03/30/2020-03-30-Redis-Basic/","link":"","permalink":"http://raymond-zhao.top/2020/03/30/2020-03-30-Redis-Basic/","excerpt":"","text":"1. 缓存技术1.1 主流应用架构 缓存穿透 熔断 1.2 缓存中间件1.2.1 Memcache 支持简单数据类型 不支持持久化存储 不支持主从 不支持分片 1.2.2 Redis 数据类型丰富 支持数据磁盘持久化存储 支持主从 支持分片 1.3 为什么Redis能这么快？ 完全基于内存，绝大部分请求是纯粹的内存操作，执行效率高 数据结构简单，对数据操作也简单 采用单线程，单线程也能处理高并发请求，多核也可启动多实例 使用多路 I/O 复用模型，非阻塞I/O 1.4 多路 I/O 复用模型 传统的阻塞 I/O 模型 Select 系统调用 Redis采用的 I/O 多路复用函数：epoll/kqueue/evport/select 因地制宜 优先选择时间复杂度为 $O(1)$ 的 I/O 多路复用函数作为底层实现 以时间复杂度为 $O(n)$ 的select作为保底 基于 react 设计模式监听 I/O 事件 2 Redis的数据类型3 从海量 key 里查询固定前缀的 key从一亿条数据里取出拥有固定前缀的10万个 key 留意细节 摸清数据规模，问清楚边界 KEYS pattern： 查找所有符合给定模式 pattern 的 key： keys + 前缀 keys k1* KEYS指令一次性返回所有匹配的 key 键的数量过大会使服务卡顿 SCAN cursor [MATCH pattern] [COUNT count] scan 0 match k1* count 10 基于游标的迭代器，需要基于上一次的游标延续之前的迭代过程 以 0 作为游标开始一次新的迭代，直到命令返回 0 完成一次遍历 不保证每次执行都返回某个给定数量的元素，支持模糊查询 一次返回的数量不可控，只能是大概率符合 count 4 如何通过 Redis 实现分布式锁？ 分布式锁需要解决的问题 互斥性 安全性 死锁 容错 SETNX key value : 如果 key 不存在，则创建并赋值 时间复杂度： $O(1)$ 返回值：成功为 1，失败为 0 如何解决 setnx 长期有效的问题 expire key seconds : 设置 key 的生存时间，当 key 过期时，会被自动删除 expire 的缺点：原子性得不到满足 set key value [ex seconds] [px milliseconds] [nx|xx] redis 2.6.12之后 ex second : 设置键的过期时间为 second 秒 px millisecond : 设置键的过期时间为 millisecond 毫秒 nx : 只在键不存在时，才会对键进行设置操作 xx : 只在键存在时，才会对键进行设置操作 set 操作成功完成时，返回 ok ，否则返回 nil 大量的 key 同时过期的注意事项 集中过期，由于清楚大量的 key 很耗时，会出现短暂的卡顿现象 解决方案：在设置 key 的过期时间的时候，给每个 key 加上随机值 5 如何使用 Redis 做异步队列？使用 List 作为队列， RPUSH 生产消息， LPOP 消费消息 缺点：没有等待队列里有值就直接消费 弥补：可以通过在应用层引入 Sleep 机制去调用 LPOP 重试 面试官：如果我不想用 sleep 呢？ BLPOP key [key ...] timeout : 阻塞直到队列有消息或者超时 缺点：只能供一个消费者消费 12345# 等待 testlist 30s 的时间，如果有值就消费blpop testlist 30# 在执行完上面的命令后，可以另开一个 redis-cli 输入rpush testlist helloworld# 那么 blpop 这边就会输出 rpush 刚刚输入的内容 pub/sub : 主题订阅者模式 发送者 (pub) 发送消息，订阅者 (sub) 接收消息 订阅者可以订阅任意数量的频道 缺点：消息的发布是无状态的，无法保证可达 6 Redis持久化RDB持久化：保存某个时间点的全量数据快照 save ：阻塞 redis 的服务器进程，知道 rdb 文件被创建完毕 bgsave ： fork 处一个子进程来创建 rdb 文件，不阻塞服务器进程 自动触发 RDB 持久化的方式 根据 redis.conf 配置里的 SAVE m n 定时触发(用的是 BGSAVE) 主从复制时，主节点自动触发 执行 debug reload 执行 shutdown 且没有开启 AOF 持久化 BGSAVE原理SegmentFault copy-on-write : 写时复制 如果有多个调用者（callers）同时请求相同资源（如内存或磁盘上的数据存储），他们会共同获取相同的指针指向相同的资源，直到某个调用者试图修改资源的内容时，系统才会真正复制一份专用副本（private copy）给该调用者，而其他调用者所见到的最初的资源仍然保持不变。这过程对其他的调用者都是透明的（transparently。此作法主要的优点是如果调用者没有修改该资源，就不会有副本（private copy）被创建，因此多个调用者只是读取操作时可以共享同一份资源。 缺点 内存数据的全量同步，数据量大会由于 I/O 而严重影响性能 可能会因为 redis 挂掉而丢失从当前至最后一次快照期间的数据 AOF持久化：保存写状态 记录下除了查询意外的所有变更数据库状态的指令 以 append 的形式追加保存到 AOF 文件中 日志重写解决 AOF 文件不断增大的问题 调用 fork() ，创建一个子进程 子进程把新的 aof 文件写到一个临时文件里，不依赖原来的 aof 文件 主进程只需把新的变动同时写到内存和原来的 aof 里 主进程获取子进程重写 aof 的完成信号，往新 aof 同步增量变动 使用新的 aof 文件替换掉旧的 aof 文件 Redis 数据的恢复Redis持久化与恢复 RDB 和 AOF 文件共存情况下的恢复流程 RDB 和 AOF 的优缺点 RDB AOF 优点 全量数据小，文件小，恢复快 可读性高，适合保存增量数据，数据不易丢失 缺点 无法保存最近一次快照之后的数据 文件体积大，恢复时间长 RDB-AOF 混合持久化 BGSAVE 做镜像全量持久化， AOF 做增量持久化 Pipeline及主从同步使用Pipeline的好处 Pipeline与Linux的管道类似 Redis基于请求/响应模型，单个请求处理需要一一应答 Pipeline批量执行指令，节省多次IO往返的时间 有顺序依赖的指令建议分批发送 Redis的同步机制 主从、哨兵、集群的配置 一个master用来写，多个slave用来读 全量同步过程 Salve发送sync命令到Master Master启动一个后台进程，将Redis中的数据快照保存到文件中 Master将保存数据快照期间接收到的写命令缓存起来 Master完成写文件操作后，将该文件发送给Salve 使用新的AOF文件替换掉旧的AOF文件 Master将这期间收集的增量写命令发送给Salve端 增量同步过程 Master接收到用户的操作指令，判断是否需要传播到Slave 将操作记录追加到AOF文件 将操作传播到其他Slave：1 对齐主从库；2 往响应缓存写入指令 将缓存中的数据发送给Slave Redis Sentinel (哨兵)解决主从同步Master宕机后的主从切换问题： 监控：检查主从服务器是否运行正常 提醒：通过API向管理员或者其他应用程序发送故障通知 自动故障迁移：主从切换 流言协议Gossip在杂乱无章中寻求一致 每个节点都随机地与对方通信，最终所有节点的状态达成一致 种子节点定期随机向其他节点发送节点列表以及需要传播的消息 不保证消息一定会传递给所有的节点，但是最终会趋于一致 Redis 集群原理如何从海量数据里快速找到所需？ 分片：按照某种规则去划分数据，分散存储在多个节点上 常规的按照哈希划分无法实现节点的动态增减 一致性哈希算法 对 $2^{32}$ 取模，将哈希值空间组织成虚拟的圆环，可以理解为钟表圆环。 将数据key使用相同的函数Hash计算出哈希值 例如我们有四个服务器，四个服务器Node ABCD在环上的位置如下图所示： 根据我们的一致性Hash算法，数据Object A离Node A最近，所以该数据就会存在Node A上，以此类推。 假设，Node C宕机，此时Node A、B、D并不会受到影响，只有Node C（这里指的是Object C）的对象会重新定位到Node D上。当一台服务器宕机时，受影响的数据，只有该服务器逆时针行走到上一个服务器节点的数据，其它的数据是不会受影响的，而且受影响的数据会存储在该宕机服务器的顺时针行走的下一个服务器上。 新增一台服务器Node X，同上面原理一样。 综上所述，一致性哈希算法对于节点的增减，只需要重新定位环空间中的一小部分数据，具有较好的容错性和扩展性。 但是一致性Hash算法也不是十全十美的。 Hash环的数据倾斜问题： 在服务器节点较少的情况下，容易因为节点分布不均匀而造成数据倾斜问题。 数据倾斜：指的是被缓存的对象，大部分集中缓存在某一台服务器上。 为了解决数据倾斜问题，引入了虚拟节点。 虚拟节点：对每一个节点计算多个Hash，计算结果位置都放置一个此服务器位置。 一致性哈希算法","categories":[{"name":"Redis","slug":"Redis","permalink":"http://raymond-zhao.top/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://raymond-zhao.top/tags/Redis/"}]},{"title":"Redis基础","slug":"2020-03-24-Redis-RedisInAction","date":"2020-03-24T15:15:24.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/03/24/2020-03-24-Redis-RedisInAction/","link":"","permalink":"http://raymond-zhao.top/2020/03/24/2020-03-24-Redis-RedisInAction/","excerpt":"","text":"第一章 初始Redis1.1 Redis简介Redis 是一个远程内存数据库，它不仅性能强劲，而且还具有复制特性以及为解决问题而生 的独一无二的数据模型。Redis 提供了 5 种不同类型的数据结构，各式各样的问题都可以很自然 地映射到这些数据结构上:Redis 的数据结构致力于帮助用户解决问题，而不会像其他数据库那 样，要求用户扭曲问题来适应数据库。除此之外，通过复制、持久化(persistence)和客户端分 片(client-side sharding)等特性，用户可以很方便地将 Redis 扩展成一个能够包含数百 GB 数据、 每秒处理上百万次请求的系统。 Redis是一个速度非常快的非关系数 据库(non-relational database)，它可以存储键(key)与 5 种不同类型的值(value)之间的 映射(mapping)，可以将存储在内存的键值对数据持久化到硬盘，可以使用复制特性来扩 展读性能，还可以使用客户端分片来扩展写性能。 1.1.1 Redis与其他数据库的对比 1.1.2 附加特性在使用类似 Redis 这样的内存数据库时，一个首先要考虑的问题就是“当服务器被关闭时， 服务器存储的数据将何去何从呢?”Redis 拥有两种不同形式的持久化方法，它们都可以用小而 紧凑的格式将存储在内存中的数据写入硬盘： 时间点转储(point-in-time dump)：转储操作既可以在“指定时间段内有指定数量的写操作执行”这一条件被满足时执行， 又可以通过调用两条转储到硬盘(dump-to-disk)命令中的任何一条来执行。 将所有修改了数据库的命令都写入一个只追加(append-only)文件里面，用户可以根据数据的重 要程度，将只追加写入设置为从不同步(sync)、每秒同步一次或者每写入一个命令就同步一次。 为了扩展 Redis 的读性能，并为 Redis 提供故障转移（failover)支持，Redis 实现了 主从复制 特性:执行复制的从服务器会连接上主服务器，接收主服务器发送的整个数据库的初始副本(copy); 之后主服务器执行的写命令，都会被发送给所有连 接着的从服务器去执行，从而实时地更新从服务器的数据集。因为从服务器包含的数据会不断地 进行更新，所以客户端可以向任意一个从服务器发送读请求，以此来避免对主服务器进行集中式 的访问。 1.2 Redis数据结构简介 第二章 Redis命令本章主要内容： 字符串命令、列表命令和集合命令 散列命令和有序集合命令 发布命令与订阅命令 其他命令 2.1 字符串在Redis里面，字符串可以存储以下三种类型的数据 字符串(byte string) 整数：整数的取 值范围和系统的长整数(long integer)的取值范围相同 浮点数：浮点数的取值范围和精度与 IEEE 754 标准的双精度浮点数(double)相同 下表为Redis中字符串的自增与自减命令 命令 用例和描述 INCR INCR key-name—将键存储的值加上1 DECR DECR key-name—将键存储的值减去1 INCRBY INCRBY key-name amount—将键存储的值加上整数amount DECRBY DECRBY key-name amount—将键存储的值减去整数amount INCRBYFLOAT INCRBYFLOAT key-name amount—将键存储的值加上浮点数amount 除了自增操作和自减操作之外，Redis 还拥有对字节串的其中一部分内容进行读取或者写入 的操作(这些操作也可以用于整数或者浮点数，但这种用法并不常见) 命令 用例和描述 APPEND APPEND key-name value—将值value追加到给定键key-name当前存储的值的末尾 GETRANGE GETRANGE key-name start end— 获取一个由偏移量 start 至偏移量 end 范围内所有字符组成 的子串，包括 start 和 end 在内 SETRANGE SETRANGE key-name offset value—将从start偏移量开始的子串设置为给定值 GETBIT GETBIT key-name offset— 将字节串看作是二进制位串(bit string)，并返回位串中偏移量为 offset 的二进制位的值 SETBIT SETBIT key-name offset value— 将字节串看作是二进制位串，并将位串中偏移量为 offset 的二进制位的值设置为 value BITCOUNT BITCOUNT key-name [start end]— 统计二进制位串里面值为 1 的二进制位的数量，如果给定 了可选的 start 偏移量和 end 偏移量，那么只对偏移量指定范围内的二进制位进行统计 BITOP BITOP operation dest-key key-name [key-name …]— 对一个或多个二进制位串执行包 括并(AND)、或 (OR)、异或(XOR)、非 (NOT)在内的任意一种按位运算操作(bitwise operation)， 并将计算得出的结果保存在 dest-key 键里面 2.2 列表Redis 的列表允许用户从序列的两端推入或者弹出元素，获取列表元 素，以及执行各种常见的列表操作。除此之外，列表还可以用来存储任务信息、最近浏览过的文章或者常用联系人信息。 命令 用例和描述 RPUSH RPUSH key-name value [value …]—将一个或多个值推入列表的右端 LPUSH LPUSH key-name value [value …]—将一个或多个值推入列表的左端 RPOP RPOP key-name—移除并返回列表最右端的元素 LPOP LPOP key-name—移除并返回列表最左端的元素 LINDEX LINDEX key-name offset—返回列表中偏移量为offset的元素 LRANGE 返回列表从 start 偏移量到 end 偏移量范围内的所有元素 LTRIM 对列表进行修剪，只保留从start偏移量到end偏移量范围，包括两端 有几个列表命令可以将元素从一个列表移动到另一个列表，或者阻塞(block)执行命令的 客户端直到有其他客户端给列表添加元素为止。 命令 用例和描述 BLPOP BLPOP key-name [key-name …] timeout— 从第一个非空列表中弹出位于最左端的元素， 或者在 timeout 秒之内阻塞并等待可弹出的元素出现 BRPOP BRPOP key-name [key-name …] timeout— 从第一个非空列表中弹出位于最右端的元素， 或者在 timeout 秒之内阻塞并等待可弹出的元素出现 RPOPLPUSH RPOPLPUSH source-key dest-key— 从 source-key 列表中弹出位于最右端的元素，然后 将这个元素推入 dest-key 列表的最左端，并向用户返回这个元素 BRPOPLPUSH BRPOPLPUSH source-key dest-key timeout—从source-key列表中弹出位于最右端的 元素，然后将这个元素推入 dest-key 列表的最左端，并向用户返回这个元素;如果 source-key 为空，那么在 timeout 秒之内阻塞并等待可弹出的元素出现 2.3 集合Redis 的集合以无序的方式来存储多个各不相同的元素，用户可以快速地对集合执行添加 元素操作、移除元素操作以及检查一个元素是否存在于集合里。 命令 用例和描述 SADD SADD key-name item [item …]— 将一个或多个元素添加到集合里面，并返回被添 加元素当中原本并不存在于集合里面的元素数量 SREM SREM key-name item [item …]— 从集合里面移除一个或多个元素，并返回被移除 元素的数量 SISMEMBER SISMEMBER key-name item—检查元素item是否存在于集合key-name里 SCARD SCARD key-name—返回集合包含的元素的数量 SMEMBERS SMEMBERS key-name—返回集合包含的所有元素 SRANDMEMBER SRANDMEMBER key-name [count]— 从集合里面随机地返回一个或多个元素。当 count 为正数时，命令返回的随机元素不会重复;当 count 为负数时，命令返回的随机元素可能会 出现重复 SPOP SPOP key-name—随机地移除集合中的一个元素，并返回被移除的元素 SMOVE SMOVE source-key dest-key item— 如果集合 source-key 包含元素 item，那么从 集合 source-key 里面移除元素 item，并将元素 item 添加到集合 dest-key 中;如果 item 被成功移除，那么命令返回 1，否则返回 0 通过使用上面展示的命令，我们可以将各不相同的多个元素添加到集合里面，集合真正厉害的地方在于组合 和关联多个集合。 命令 用例和描述 SDIFF SDIFF key-name [key-name …]—返回那些存在于第一个集合、但不存在于其他集合中的元素(数学上的差集运算) SDIFFSTORE SDIFFSTORE dest-key key-name [key-name …]— 将那些存在于第一个集合但并不存在于其他集合中的元素(数学上的差集运算)存储到 dest-key 键里面 SINTER SINTER key-name [key-name …]— 返回那些同时存在于所有集合中的元素(数学上的交集运算) SINTERSTORE SINTERSTORE dest-key key-name [key-name …]— 将那些同时存在于所有集合的元素(数学上的交集运算)存储到 dest-key 键里面 SUNION SUNION key-name [key-name …]— 返回那些至少存在于一个集合中的元素(数学上的并集计算) SUNIONSTORE SUNIONSTORE dest-key key-name [key-name …]— 将那些至少存在于一个集合中的元素(数学上的并集计算)存储到 dest-key 键里面 2.4 散列Redis 的散列可以让用户将多个键值对存储到一个 Redis 键里面。从功能上 来说，Redis 为散列值提供了一些与字符串值相同的特性，使得散列非常适用于将一些相关的数 据存储在一起。我们可以把这种数据聚集看作是关系数据库中的行，或者文档数据库中的文档。 命令 用例和描述 HMGET HMGET key-name key [key …]— 从散列里面获取一个或多个键的值 HMSET HMSET key-name key value [key value …]—为散列里面的一个或多个键设置值 HDEL HDEL key-name key [key …]— 删除散列里面的一个或多个键值对，返回成功找到并删除的 键值对数量 HLEN HLEN key-name— 返回散列包含的键值对数量 下面列出了散列的其他几个批量操作命令，以及一些和字符串操作类似的散列命令。 命令 用例和描述 HEXISTS HEXISTS key-name key—检查给定键是否存在于散列中 HKEYS HKEYS key-name—获取散列包含的所有键 HVALS HVALS key-name—获取散列包含的所有值 HGETALL HGETALL key-name—获取散列包含的所有键值对 HINCRBY HINCRBY key-name key increment—将键key存储的值加上整数increment HINCRBYFLOAT HINCRBYFLOAT key-name key increment—将键key存储的值加上浮点数increment 2.5 有序集合和散列存储着键与值之间的映射类似，有序集合也存储着成员与分值之间的映射，并且提供 了分值处理命令，以及根据分值大小有序地获取(fetch)或扫描(scan)成员和分值的命令。 命令 用例和描述 ZADD ZADD key-name score member [score member …]—将带有给定分值的成员添加到有序集合里面 ZREM ZREM key-name member [member …]—从有序集合里面移除给定的成员，并返回被移除成员的数量 ZCARD ZCARD key-name—返回有序集合包含的成员数量 ZINCRBY ZINCRBY key-name increment member—将member成员的分值加上increment ZCOUNT ZCOUNT key-name min max—返回分值介于min和max之间的成员数量 ZRANK ZRANK key-name member—返回成员member在有序集合中的排名 ZSCORE ZSCORE key-name member—返回成员member的分值 ZRANGE ZRANGE key-name start stop [WITHSCORES]—返回有序集合中排名介于start和stop 之间的成员，如果给定了可选的 WITHSCORES 选项，那么命令会将成员的分值也一并返回 有序集合的范围型数据获取命令和范围型数据删除命令，以及并集命令和交集命令。 命令 用例和描述 ZREVRANK ZREVRANK key-name member— 返回有序集合里成员 member 的排名，成员按照分值 从大到小排列 ZREVRANGE ZREVRANGE key-name start stop [WITHSCORES]— 返回有序集合给定排名范围内 的成员，成员按照分值从大到小排列 ZRANGEBYSCORE ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]—返回 有序集合中，分值介于 min 和 max 之间的所有成员 ZREVRANGEBYSCORE ZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset count]— 获取有序集合中分值介于 min 和 max 之间的所有成员，并按照分值从大到小的顺序来返 回它们 ZREMRANGEBYRANK ZREMRANGEBYRANK key-name start stop— 移除有序集合中排名介于 start 和 stop 之间的所有成员 ZREMRANGEBYSCORE`` ZREMRANGEBYSCORE key-name min max— 移除有序集合中分值介于 min 和 max 之 间的所有成员 ZINTERSTORE ZINTERSTORE dest-key key-count key [key …] [WEIGHTS weight [weight …]] [AGGREGATE SUM ZUNIONSTORE ZUNIONSTORE dest-key key-count key [key …] [WEIGHTS weight [weight …]] [AGGREGATE SUM 2.6 发布订阅发布与订阅(又称 pub/sub)的特 点是订阅者(listener)负责订阅频道(channel)，发送者(publisher)负责向频道发送二进制字 符串消息(binary string message)。每当有消息被发送至给定频道时，频道的所有订阅者都会收 到消息。 命令 用例和描述 SUBSCRIBE SUBSCRIBE channel [channel …]—订阅给定的一个或多个频道 UNSUBSCRIBE UNSUBSCRIBE [channel [channel …]]—退订给定的一个或多个频道，如果执行时没有给定任何频道，那么退订所有频道 PUBLISH PUBLISH channel message—向给定频道发送消息 PSUBSCRIBE PSUBSCRIBE pattern [pattern …]—订阅与给定模式相匹配的所有频道 PUNSUBSCRIBE PUNSUBSCRIBE [pattern [pattern …]]— 退订给定的模式，如果执行时没有给定任何 模式，那么退订所有模式 对于旧版 Redis 来说，如果一个客户端订阅了某个 或某些频道，但它读取消息的速度却不够快的话，那么不断积压的消息就会使得 Redis 输出缓冲 区的体积变得越来越大，这可能会导致 Redis 的速度变慢，甚至直接崩溃。也可能会导致 Redis 被操作系统强制杀死，甚至导致操作系统本身不可用。新版的 Redis 不会出现这种问题，因为它 会自动断开不符合 client-output-buffer-limit pubsub 配置选项要求的订阅客户端。 任何网络系统在执行操作时都可能会遇上断线情况， 而断线产生的连接错误通常会使得网络连接两端中的其中一端进行重新连接。 2.7 其他命令2.7.1 排序负责执行排序操作的 SORT 命令可以根据字符串、列表、集合、有序集合、散 列这 5 种键里面存储着的数据，对列表、集合以及有序集合进行排序。 使用 SORT 命令提供的选项可以实现以下功能:根据降序而不是默认的升序来排序元素; 将元素看作是数字来进行排序，或者将元素看作是二进制字符串来进行排序(比如排序字符 串’110’和’12’的结果就跟排序数字 110 和 12 的结果不一样);使用被排序元素之外的其 他值作为权重来进行排序，甚至还可以从输入的列表、集合、有序集合以外的其他地方进行 取值。 命令 用例和描述 SORT SORT source-key [BY pattern] [LIMIT offset count] [GET pattern [GETpattern …]] [ASC|DESC] [ALPHA] [STORE dest-key] 根据给定的选项，对输入 列表、集合或者有序集合进行排序，然后返回或者存储排序的结果 2.7.2 基本的Redis事务尽管 Redis 有几个可以在 两个键之间复制或者移动元素的命令，但却没有那种可以在两个不同类型之间移动元素的命令 (虽然可以使用 ZUNIONSTORE 命令将元素从一个集合复制到一个有序集合)。为了对相同或者不 同类型的多个键执行操作，Redis 有 5 个命令可以让用户在不被打断(interruption)的情况下对多个键执行操作，它们分别是 WATCH、MULTI、EXEC、UNWATCH 和 DISCARD。 Redis 的基本事务(basic transaction)需要用到 MULTI 命令和 EXEC 命令，这种事务可以让 一个客户端在不被其他客户端打断的情况下执行多个命令。和关系数据库那种可以在执行的过程 中进行回滚(rollback)的事务不同，在 Redis 里面，被 MULTI 命令和 EXEC 命令包围的所有命 令会一个接一个地执行，直到所有命令都执行完毕为止。当一个事务执行完毕之后，Redis 才会 处理其他客户端的命令。 2.7.3 键的过期时间在使用 Redis 存储数据的时候，有些数据可能在某个时间点之后就不再有用了，用户可以使 用 DEL 命令显式地删除这些无用数据，也可以通过 Redis 的过期时间(expiration)特性来让一个 键在给定的时限(timeout)之后自动被删除。 对于列表、集合、散列和有序集合这样的容器(container)来说，键过期命令只能为整个键设 置过期时间，而没办法为键里面的单个元素设置过期时间。 命令 用例和描述 PERSIST PERSIST key-name—移除键的过期时间 TTL TTL key-name—查看给定键距离过期还有多少秒 EXPIRE EXPIRE key-name seconds—让给定键在指定的秒数之后过期 EXPIREAT EXPIREAT key-name timestamp—将给定键的过期时间设置为给定的UNIX时间戳 PTTL PTTL key-name—查看给定键距离过期时间还有多少毫秒 PEXPIRE PEXPIRE key-name milliseconds— 让给定键在指定的毫秒数之后过期 PEXPIREAT PEXPIREAT key-name timestamp-milliseconds—将一个毫秒级精度的UNIX时间戳设置 为给定键的过期时间， 第三章 数据安全性与性能保障本章主要内容 将数据持久化到硬盘 将数据复制至其他机器 处理系统故障 Redis 事务 非事务型流水线(non-transactional pipeline) 诊断性能问题 3.1 持久化选项Redis 提供了两种不同的持久化方法来将数据存储到硬盘里面。一种方法叫快照(snapshotting)，它可以将存在于某一时刻的所有数据都写入硬盘里面。另一种方法叫只追加文件(append-only file， AOF)，它会在执行写命令时，将被执行的写命令复制到硬盘里面。这两种持久化方法既可以同 时使用，又可以单独使用，在某些情况下甚至可以两种方法都不使用，具体选择哪种持久化方法 需要根据用户的数据以及应用来决定。 将内存中的数据存储到硬盘的一个主要原因是为了在之后重用数据，或者是为了防止系统 故障而将数据备份到一个远程位置。另外，存储在 Redis 里面的数据有可能是经过长时间计算 得出的，或者有程序正在使用 Redis 存储的数据进行计算，所以用户会希望自己可以将这些数 据存储起来以便之后使用，这样就不必再重新计算了。 12通过 Homebrew 配置文件地址/usr/local/etc/redis.conf 3.1.1 快照持久化Redis 可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。在创建快照 之后，用户可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本，还可以将快照留在原地以便重启服务器时使用。 12345save 60 1000stop-writes-on-bgsave-error yesrdbcompression yesdbfilename dump.rdbdir /usr/local/var/db/redis/ 根据配置，快照将被写入 dbfilename 选项指定的文件里面，并储存在 dir 选项指定的路径上面。如果在新的快照文件创建完毕之前，Redis、系统或者硬件这三者之中的任意一个崩溃 了，那么 Redis 将丢失最近一次创建快照之后写入的所有数据。 创建快照的方法 客户端可以通过向Redis发送BGSAVE命令来创建一个快照。对于支持BGSAVE命令的平台来说(基本上所有平台都支持，除了Windows平台)，Redis会调用fork1来创建一个子进程，然后子进程负责将快照写入硬盘，而父进程则继续处理命令请求。 客户端还可以通过向 Redis 发送 SAVE 命令来创建一个快照，接到 SAVE 命令的 Redis 服 务器在快照创建完毕之前将不再响应任何其他命令。SAVE 命令并不常用，通常只会在没有足够内存去执行 BGSAVE 命令的情况下，又或者即使等待持久化操作执行完毕也无所谓的情况下，才会使用这个命令。 如果用户设置了 save 配置选项，比如 save 60 10000，那么从 Redis 最近一次创建快照之后开始算起，当“60 秒之内有 10 000 次写入”这个条件被满足时，Redis 就会自动 触发 BGSAVE 命令。如果用户设置了多个 save 配置选项，那么当任意一个 save 配置 选项所设置的条件被满足时，Redis 就会触发一次 BGSAVE 命令。 当 Redis 通过 SHUTDOWN 命令接收到关闭服务器的请求时，或者接收到标准 TERM 信号 时，会执行一个 SAVE 命令，阻塞所有客户端，不再执行客户端发送的任何命令，并在 SAVE 命令执行完毕之后关闭服务器。 当一个 Redis 服务器连接另一个 Redis 服务器，并向对方发送 SYNC 命令来开始一次复 制操作的时候，如果主服务器目前没有在执行 BGSAVE 操作，或者主服务器并非刚刚执 行完 BGSAVE 操作，那么主服务器就会执行 BGSAVE 命令。 在只使用快照持久化来保存数据时，一定要记住:如果系统真的发生崩溃，用户将丢失最近一次生成快照之后更改的所有数据。因此，快照持久化只适用于那些即使丢失一部分数据也 不会造成问题的应用程序，不能接受这种数据损失的应用程序则可以考虑使用 AOF 持久化。 几个使用快照持久化的场景 个人开发 对日志进行聚合计算 大数据 3.1.2 AOF持久化AOF 持久化会将被执行的写命令写到 AOF 文件的末尾，以此来记录数据发生的变化。因此，Redis 只要从头到尾重新执行一次 AOF 文件包含的所有写命令，就可以恢复 AOF 文件所记录的数据集。 文件同步 在向硬盘写入文件时，至少会发生3件事。当调用 file.write()方法 ( 或者其他编程语言里面的类似操作)对文件进行写入时，写入的内容首先会被存储到缓冲区，然后操作系统会在将来的某个时候将缓冲区存储的内容写入硬盘，而数据只有在被写入硬盘之后，才算是真正地保存到了硬盘里面。用户可以通过调用 file.flush()方法来请求操作系统尽快地将缓冲区存储的数据写入硬盘里，但具体何时执行写入操作仍然由操作系统决定。除此之外，用户还可以命令操作系统将文件同步(sync)到硬盘，同步操作会一直阻塞直到指定的文件被写入硬盘为止。当同步操作执行完毕之后，即使系统出现故障也不会对被同步的文件造成任何影响。 123456789appendonly noappendfsync everysec 每秒执行一次同步，显式地将多个写命令同步到硬盘# appendfsync always 每个 Redis 写命令都要同步写入硬盘。这样做会严重降低 Redis 的速度# 使用固态硬盘的用户请谨慎使用 appendfsync always 选项，因为这个选项让 Redis 每次只写入一个命令# appendfsync no 让操作系统来决定应该何时进行同步no-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mbdir /usr/local/var/db/redis/ 3.1.3 重写/压缩 AOF 文件AOF 持久化既可以将丢失数据的时间窗口降低至 1 秒(甚至不丢失任何数据)，又可以在极短的时间内完成定期的持久化操作，那么我们有什么理由不使用 AOF 持久化呢? 因为 AOF 文件体积会不断增大。 为了解决 AOF 文件体积不断增大的问题，用户可以向 Redis 发送 BGREWRITEAOF 命令，这个命令会通过移除 AOF 文件中的冗余命令来重写(rewrite)AOF 文件，使 AOF 文件的体积变得尽可能地小。 3.2 复制复制可以让其他服务器拥有一个不断地更新的数据副本，从而使得拥有数据副本的服务器可以用于处理客户端发送的读请求。关系数据库通常会使用一个主服务器(master)向多个从服务器(slave)发送更新，并使用从服务器来处理所有读请求。Redis 也采用了同样的方法来实现 自己的复制特性，并将其用作扩展性能的一种手段。 3.2.1 对 Redis 的复制相关选项进行配置当从服务器连接主服务器的时候，主服务器会执行 BGSAVE 操作。 因此为了正确地使用复制特性，用户需要保证主服务器已经正确地设置了dir 选项和 dbfilename 选项，并且这两个选项所指示的路径和文件对于 Redis 进程来说都是可写的(writable)。 尽管有多个不同的选项可以控制从服务器自身的行为，但开启从服务器所必须的选项只有 slaveof一个。如果用户在启动Redis服务器的时候，指定了一个包含slaveof host port 选项的配置文件，那么 Redis 服务器将根据该选项给定的 IP 地址和端口号来连接主服务器。对于一个正在运行的Redis服务器，用户可以通过发送SLAVEOF no one命令来让服务器终止复制操作，不再接受主服务器的数据更新;也可以通过发送SLAVEOF host port命令来让服务器 开始复制一个新的主服务器。 3.2.2 Redis 复制的启动过程从服务器在连接一个主服务器的时候，主服务器会创建一个快照文件并 将其发送至从服务器，但这只是主从复制执行过程的其中一步。 步骤 主服务器操作 从服务器操作 1 (等待命令进入) 连接(或者重连接)主服务器，发送 SYNC 命令 2 开始执行 BGSAVE，并使用缓冲区记录 BGSAVE之后执行的所有写命令 根据配置选项来决定是继续使用现有的数据(如果有的话)来处理客户端的命令请求，还是向发送请求的客户端返回错误 3 BGSAVE 执行完毕，向从服务器发送快照文件， 并在发送期间继续使用缓冲区记录被执行的写命令 丢弃所有旧数据(如果有的话)，开始载入主服务器发来的快照文件 4 快照文件发送完毕，开始向从服务器发送存储在缓冲区里面的写命令 完成对快照文件的解释操作，像往常一样开始接受命令请求 5 缓冲区存储的写命令发送完毕;从现在开始， 每执行一个写命令，就向从服务器发送相同的 写命令 执行主服务器发来的所有存储在缓冲区里面的写命 令;并从现在开始，接收并执行主服务器传来的每个写命令","categories":[{"name":"Redis","slug":"Redis","permalink":"http://raymond-zhao.top/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://raymond-zhao.top/tags/Redis/"}]},{"title":"《软件工程》思维导图","slug":"2020-03-21-SE-SoftwareEngineering","date":"2020-03-21T08:31:11.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/03/21/2020-03-21-SE-SoftwareEngineering/","link":"","permalink":"http://raymond-zhao.top/2020/03/21/2020-03-21-SE-SoftwareEngineering/","excerpt":"","text":"取用时若能注明出处，我将不胜感激。","categories":[{"name":"架构师软技能","slug":"架构师软技能","permalink":"http://raymond-zhao.top/categories/%E6%9E%B6%E6%9E%84%E5%B8%88%E8%BD%AF%E6%8A%80%E8%83%BD/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://raymond-zhao.top/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"},{"name":"设计模式","slug":"设计模式","permalink":"http://raymond-zhao.top/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"《设计模式》思维导图","slug":"2020-03-21-SE-DesignPatterns","date":"2020-03-21T08:27:55.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/03/21/2020-03-21-SE-DesignPatterns/","link":"","permalink":"http://raymond-zhao.top/2020/03/21/2020-03-21-SE-DesignPatterns/","excerpt":"","text":"取用时若能注明出处，我将不胜感激。","categories":[{"name":"架构师软技能","slug":"架构师软技能","permalink":"http://raymond-zhao.top/categories/%E6%9E%B6%E6%9E%84%E5%B8%88%E8%BD%AF%E6%8A%80%E8%83%BD/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://raymond-zhao.top/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"},{"name":"设计模式","slug":"设计模式","permalink":"http://raymond-zhao.top/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"好看又精良的美剧以及图书","slug":"2020-03-21-Life-BooksAndMovies","date":"2020-03-21T06:15:19.000Z","updated":"2022-05-08T04:50:27.000Z","comments":true,"path":"2020/03/21/2020-03-21-Life-BooksAndMovies/","link":"","permalink":"http://raymond-zhao.top/2020/03/21/2020-03-21-Life-BooksAndMovies/","excerpt":"","text":"休闲一刻力荐 中文译名 英文原名 Season 剧集详情 总剧集 单集时长(分) 《我们这一天》 《This Is Us》 六季 18*4+15+15 102 40 《绝命毒师》 《Breaking Bad》 五季 7+13+13+13+16 62 60 《权力的游戏》 《Game of Thrones》 八季 10*6+7+6 73 60 《法官大人》 《Your Honor》 一季 7 7 50 《黄石》 《Yellowstone》 三季 10*3 30 40 《亿万》 《Billions》 五季 10+12*3+7 53 50 《西部世界》 《Westworld》 三季 10*2+8 28 60 《为了全人类》 《For All Mankind》 两季 10 * 2 20 60 英伦 中文译名 英文原名 Season 剧集详情 总剧集 单集时长(分) 《唐顿庄园》 《Downton Abbey》 六季 8*6 48 60 《夜班经理》 《The Night Manager》 一季 6 6 40 《神探夏洛克》 《Sherlock》 四季 3*4 12 90 《梅尔罗斯》 《Melrose》 一季 5 5 50 《名姝》 《Harlots》 两季 8*2 16 40 《浴血黑帮》 《Peaky Blinders》 六季 6*6 36 60 《布里奇顿》 《Bridgerton》 一季 1 战争 中文译名 英文原名 Season 剧集详情 总剧集 单集时长(分) 《杀戮一代》 《Generation Kill》 一季 7 7 40 《斯巴达克斯》 《Spartacus》 三季 10*3 30 50 《太平洋战争》 《The Pacific》 一季 10 10 50 政治与经济 中文译名 英文原名 Season 剧集详情 总剧集 单集时长(分) 《纸牌屋》 《House of Cards》 六季 13*5+8 73 50 《国土安全》 《Homeland》 七季 12*7 84 50 《狼厅》 《Wolf Hall》 一季 6 6 60 《王冠》 《The Crown》 四季 10*4 40 60 《指定幸存者》 《Designated Survivor》 三季 21+22+10 53 40 《中央公园五人案》 《When They See US》 一季 1 1 40 惊悚犯罪 中文译名 英文原名 Season 剧集详情 总剧集 单集时长(分) 《汉尼拔》 《Hannibal》 三季 13*3 39 40 《越狱》 《Prison Break》 五季 22+22+13+22+9 88 50 《大西洋帝国》 《The Atlantic City》 五季 12*4+8 56 40 《美国犯罪故事》 《American Crime Story》 两季 10+9 19 50 《清道夫》 《Ray Donovan》 五季 12*5 60 40 《侍女的故事》 《The Handmaid’s Tale》 两季 10+13 23 60 《黑名单》 《The Blacklist》 一季 22 22 40 《蛇蝎美人》 《Femme Fatales》 一季 13 13 20 《逍遥法外》 《How To Get Away With Murder》 五季 15*5 75 40 《龙战士》 《Warrior》 两季 10*2 20 60 《毒枭-哥伦比亚》 《Narcos》 三季 10*3 30 40 《毒枭-墨西哥》 《Narcos-Mexico》 一季 3 3 40 《风骚律师》 《Better Call Saul》 五季 10*5 50 40 《空乘危机》 《The Flight Attendant》 一季 8 8 40 《血疫》 《The Hot Zone》 一季 6 6 40 《边境》 《Frontier》 三季 6*3 18 40 《无神》 《Godless》 一季 7 * 1 7 70 科幻 中文译名 英文原名 Season 剧集详情 总剧集 单集时长(分) 《真实的人类》 《Humans》 三季 8*3 24 40 《黑镜》 《Black Mirror》 五季 3+4+6+6+3 22 60 《穹顶之下》 《Under The Dome》 三季 13*3 39 40 《命运航班》 《Manifest》 一季 16 16 40 《曼达洛人》 《The Mandalorian》 两季 8*2 16 50 《猎魔人》 《The Witcher》 一季 8 8 60 平民生活 中文译名 英文原名 Season 剧集详情 总剧集 单集时长(分) 《马男波杰克》 《BoJack Horseman》 五季 12*5 60 20 《女王的棋局》 《Queen’s Gambit》 一季 7 7 50 《广告狂人》 《Mad Men》 五季 13*5 65 40 《我的天才女友》 《My Brilliant Girlfriend》 两季 8*2 16 50 《摩登情爱》 《Modern Love》 一季 4 4 30 纪录片 中文译名 英文原名 Season 剧集详情 总剧集 单集时长(分) 《天才》 《Genius》 两季 10*2 20 50 《末代沙皇》 《Last Casar》 一季 6 6 50 西式超能力 中文译名 英文原名 Season 剧集详情 总剧集 单集时长(分) 《吸血鬼日记》 《The Vampire Diaries》 八季 22*7+16 170 40 《初代吸血鬼》 《The Originals》 五季 22*3+13*2 92 40 《吸血鬼后裔》 《Legacies》 一季 13 13 40 《传教士》 《Priest》 一季 1 1 40 《这样不OK》 《I‘m Not Okay With This》 一季 7 7 40 DC 中文译名 英文原名 Season 剧集详情 总剧集 单集时长(分) 《明日传奇》 《Legends of Tomorrow》 两季 16+17 33 40 《绿箭侠》 《The Arrow》 七季 23*7 161 40 《超女》 《Supergirl》 两季 22*2 44 40 《闪电侠》 《The Flash》 七季 23*5 + 19 + 4 138 40 《路西法》 《Lucifer》 五季 18+18+26+10+8 80 50 Marvel 中文译名 英文原名 Season 剧集详情 总剧集 单集时长(分) 《神盾局特工》 《Agents of S.H.I.L.E.D》 七季 22*5+13+4 127 40 《特工卡特》 《Agent Carter》 两季 8+10 18 40 《杰西卡琼斯》 《Jessica Jones》 一季 4 4 40 《月光骑士》 《Moon Knight》 一季 6 6 60 #### 喜剧 中文译名 英文原名 Season 剧集详情 总剧集 单集时长(分) :————: :———————–: :—-: :——–: :—-: :———-: 《生活大爆炸》 《The BigBang Theory》 十二季 24*12 288 20 《硅谷》 《Silicon Vally》 五季 8*2+10*3 46 40 《小谢尔顿》 《Young Sheldon》 四季 22*2+18+5 67 20 《朋友也上床》 《Friends with Benefits》 一季 12 12 20 《初来乍到》 《Fresh Off The Boat》 一季 10 10 20 《破产姐妹》 《2 Broke Girls》 六季 22*6 132 20 《太空部队》 《Space Force》 一季 10 10 20 《性爱自修室》 《Sex Education》 两季 8*2 16 40 《老友记》 《Friends》 十季 236 236 20 《极品老妈》 《Mom》 八季 22*6+20+5 157 20 《再造淑女》 《Selfie》 一季 14 14 20 太空冒险 中文译名 英文原名 Season 剧集详情 总剧集 单集时长(分) 《迷失太空》 《Lost in Space》 两季 10*2 20 40 《太空无垠》 《The Expanse》 五季 13*3+10+8 57 50 《外星生命》 《Another Life》 两季 10*2 20 40 缤纷思想大二大三期间的书单，研究生入学后就真的没时间读书了。 仍然记得高中时在课外时间看本《百年孤独》都要被班主任威胁要叫家长时的可怕场景。 书名 作者 作者国籍 《盗墓笔记》 南派三叔 🇨🇳中国 《一个人的朝圣》 瑞秋·乔伊斯 🇬🇧英国 《百年孤独》 加西亚·马尔克斯 🇨🇴哥伦比亚 《霍乱时期的爱情》 加西亚·马尔克斯 🇨🇴哥伦比亚 《国富论》 亚当·斯密 🇬🇧英国 《资本论》 卡尔·马克思 🇩🇪德国 《就业利息和货币通论》 约翰·梅纳德·凯恩斯 🇬🇧英国 《雪国》 川端康成 🇯🇵日本 《春琴抄》 谷崎润一郎 🇯🇵日本 《平凡的世界》 路遥 🇨🇳中国 《麦田里的守望者》 J·D·塞林格 🇺🇸美国 《追风筝的人》 卡勒德·胡塞尼 🇺🇸美国 《苦难辉煌》 金一南 🇨🇳中国 《世界大格局 中国有话说》 金一南 🇨🇳中国 《美的历程》 李泽厚 🇨🇳中国 《大国崛起》 唐晋 🇨🇳中国 《梦的解析》 西格蒙德·弗洛伊德 🇦🇹奥地利 《时间简史》 史蒂芬·霍金 🇬🇧英国 《果壳中的宇宙》 史蒂芬·霍金 🇬🇧英国 《人类简史》 尤瓦尔·赫拉利 🇮🇱以色列 《未来简史》 尤瓦尔·赫拉利 🇮🇱以色列 《中国历代政治得失》 钱穆 🇨🇳中国 《前世今生》 布莱恩·维斯 🇺🇸美国 《活着》 余华 🇨🇳中国 《三体-地球往事》 刘慈欣 🇨🇳中国 《三体-黑暗森林》 刘慈欣 🇨🇳中国 《三体-死神永生》 刘慈欣 🇨🇳中国 《民主的细节》 刘瑜 🇨🇳中国 《人间词话》 王国维 🇨🇳中国 《偷影子的人》 马克·李维 🇫🇷法国 《伊斯坦布尔假期》 马克·李维 🇫🇷法国 《文明的冲突》 塞缪尔·亨廷顿 🇺🇸美国 《战争与和平》 列夫·托尔斯泰 🇷🇺俄国 《毛泽东选集》 毛泽东 🇨🇳中国 《巨人的陨落》 肯·福莱特 🇬🇧英国 《简·爱》 夏洛蒂·勃朗特 🇬🇧英国 《傲慢与偏见》 简·奥斯汀 🇬🇧英国 《大道之行 中国共产党与中国社会主义》 京沪五学者 🇨🇳中国 《了不起的盖茨比》 弗·菲茨杰拉德 🇺🇸美国 《希特勒传 从乞丐到元首》 约翰·托兰 🇺🇸美国 《第二次世界大战回忆录》 温斯顿·丘吉尔 🇬🇧英国","categories":[{"name":"读书观影","slug":"读书观影","permalink":"http://raymond-zhao.top/categories/%E8%AF%BB%E4%B9%A6%E8%A7%82%E5%BD%B1/"}],"tags":[{"name":"读书观影","slug":"读书观影","permalink":"http://raymond-zhao.top/tags/%E8%AF%BB%E4%B9%A6%E8%A7%82%E5%BD%B1/"}]},{"title":"排序算法","slug":"2020-03-19-Algo-Sorting","date":"2020-03-19T08:31:11.000Z","updated":"2022-09-13T01:59:16.000Z","comments":true,"path":"2020/03/19/2020-03-19-Algo-Sorting/","link":"","permalink":"http://raymond-zhao.top/2020/03/19/2020-03-19-Algo-Sorting/","excerpt":"","text":"十大排序算法比较与总结下列表格主要来源于学习数据结构时的总结，在《Algorithm to Algorithm》一书中涉及的算法将以该书内容为准，因为Markdown并不能完美支持 $LaTex$，所以这里就不能手写伪代码(Pseudocode)了 。 在此默认排序方式为升序，即从小到大。 排序算法可以分为内部排序和外部排序，内部排序是数据记录在内存中进行排序，而外部排序是因排序的数据很大，一次不能容纳全部的排序记录，在排序过程中需要访问外存。 分类 算法名称 平均 最好 最坏 空间复杂度 稳定性 插入 直接插入 $O(n^2)$ $O(n)$ $O(n^2)$ $O(1)$ $Y$ 插入 折半插入 $O(n^2)$ $O(n\\lg n)$ $O(n^2)$ $O(1)$ $Y$ 插入 希尔排序 $O(n\\lg n)$ $O(n^{1.3})$ $O(n^2)$ $O(1)$ $N$ 交换 冒泡排序 $O(n^2)$ $O(n)$ $O(n^2)$ $O(1)$ $Y$ 交换 快速排序 $O(n\\lg n)$ $O(n\\lg n)$ $O(n^2)$ $O(\\lg n)$ $N$ 选择 简单选择排序 $O(n^2)$ $O(n^2)$ $O(n^2)$ $O(1)$ $N$ 选择 堆排序 $O(n\\lg n)$ $O(n)$ $O(n\\lg n)$ $O(1)$ $N$ 归并 二路归并 $O(n\\lg n)$ $O(n\\lg n)$ $O(n\\lg n)$ $O(n)$ $Y$ 线性 基数排序 $O(d(n+r))$ $O(d(n+r))$ $O(d(n+r))$ $O(r+n)$ $Y$ 线性 计数排序 $\\Theta(n+k)$ $O(n+k)$ $O(n+k)$ $O(k)$ $Y$ 线性 桶排序 $O(n+k)$ $O(n)$ $O(n^2)$ $O(n*k)$ $Y$ 平均而言，快速排序最佳，最坏情况下不如堆排序与归并排序 直接插入排序，折半插入排序，简单选择排序，冒泡排序等时间复杂度为 $O(n^2)$ 的排序不适用与 $n$ 较大的情况 一趟排序至少能保证一个关键字到达最终位置的排序：交换类(冒泡排序，快速排序)，选择类排序(简单选择排序，堆排序) 关键字比较次数和原始序列无关的排序：简单选择排序，折半插入排序，归并排序，基数排序 排序趟数和原始序列有关的排序：交换类排序(冒泡排序，快速排序) 排序趟数与原始序列无关的排序：直接插入排序，折半插入排序，希尔排序，堆排序，二路归并排序，简单选择排序，基数排序 借助于比较进行排序的算法，最坏时至少为 $O(n\\lg n)$ 直接插入和折半插入的区别仅在于查找插入位置的方式不同 元素的移动次数与原始序列无关的排序：基数排序 不稳定排序：快速排序，希尔排序，简单选择排序，堆排序 时间复杂度为 $O(n\\lg n)$ 的排序：快速排序，希尔排序，二路归并排序，堆排序 计数排序不是比较排序，因此不受 $\\Omega(n\\lg n)$ 的限制 TODO 时间复杂度的计算方法 未完成的算法 相应算法的动图 1. 插入类排序1.1 直接插入《Introduction to Algorithm》这本经典著作中的第一个算法例子就是 $Insertion Sort$ ，作者也是在这个例子中举出了此书中分析算法的核心名词 Loop Invariant ，即循环不变量，使用循环不变量能够帮助我们明白一个算法为什么是正确的。循环不变量包括三个要素 Initialization: It is true prior to the first iteration of the loop. 初始化：在循环迭代中第一步是正确的 Maintenance: If it is true before an iteration of the loop, it remains true before the next iteration. 保持：如果循环中前一次的迭代结果时正确的，那么在下一次迭代过程中也应该是正确的。 Termination: When the loop terminates, the invariant gives us a useful property that helps show that the algorithm is correct. 终止：当循环结束时，不变量可以给我们一个能证明算法是正确的有用的特性。 就像打扑克牌补牌时的方式一样，第一张牌肯定是有序的，从二张牌开始，每次将补到的牌按大小顺序插入手牌中，这样每次能保证每次插入后的手牌都是有序的，但是很可能会存在的问题是补到的牌不大不小，正处中间，这就会导致比这张牌大的手牌将会逐个后移。 基本思想：将待插入的元素逐个插入到已经有序的数组中。 循环不变量：在插入之前已经有序的数组，在插入后数组仍然是有序的。 12345678910111213141516171819202122232425/** * 基本思想：从数组中的第二个元素起，保存当前元素， * 拿当前元素之前的所有元素跟当前元素比较，如果之前元素 &gt; 当前元素，将之前元素后移一位， * 如果遇到之前的某个元素不比当前元素大了，或者是走到头了，表示这个之前元素是不大于当前元素的， * 那当前元素就应该排在这个之前元素的屁股后边，这里就是它的最终归宿。 */public static int[] insertionSort(int[] nums) &#123; if (Objects.isNull(nums) || nums.length &lt;= 1) &#123; return nums; &#125; for (int i = 1; i &lt; nums.length; i++) &#123; int currentNum = nums[i]; // 当前值 int j = i - 1; // 当前值前边的值的索引 // j &gt;= 0 至关重要，这个值为数组中第一个值的下标， // 如果是 j &gt; 0 的话，进入循环的条件少了个数（num[0]），第一个值不会参与比较。 while (j &gt;= 0 &amp;&amp; nums[j] &gt; currentNum) &#123; nums[j + 1] = nums[j]; --j; &#125; // 归位元素 nums[j + 1] = currentNum; &#125; return nums;&#125; 现在，利用 循环不变量 来证明这个算法是正确的。 初始化：在第一次执行的时候，仅有元素 array[0]，很显然一个数字无论从哪种排序方式看都是有序的。 保持：接下来需要证明在每一次循环中都能保持循环不变量成立。从非形式化的方式上看，在for循环中，可能要将array[i-1], array[i-2]... 的元素向后移动一个位置，直到找到array[i]要插入的位置为止。 终止：当外层for循环结束的时候，在外层for循环将i 替换为 n的时候，就有子数组 array[0..n-1]已经排好序了，而子数组其实正是整个数组，说明整个数组也已经排好序了，说明算法是正确的。 渐进记号在《Introduction to Algorithm》一书中，出现最多的其实并不是大 $O$ 记号，而是$\\Theta$ 记号，使用 $\\Theta$ 记号的意思是紧确界，而简单的使用大 $O$ 记号则表示该问题的一个规模的上界，表示该问题的最坏情况，另外有该问题的下界记号 $\\Omega$ ，表示该问题的最好情况, 而 $\\Theta$ 记号则表示介于二者之间。 图像比具体的数学定义更直观一些。 在插入排序中最好的情况是指该数组已经有序，此时代码中的while循环条件之中不满足，则不用交换元素，只需要遍历一遍数组，即达到下确界 $\\Omega(n)$ 最坏情况则是该数组已有序但是与要求的排序规则相比则是逆序，此时while循环每次满足， 而移动元素的次数为 $1+2+3+\\dots + (n-1) = \\frac{n(n-1)}{2}$ 次，即达到上确界 $O(n^2)$ . 所以通常情况下，该问题的规模函数介于二者之间，为 $\\Theta(n^2)$ 更加细致入微的分析还请参考《Introduction to Algorithm》一书英文版26页，下面是一张书中的截图 这里可能有人要问，为什么介于二者之间就是 $\\Theta(n^2)$ ，而不是 $\\Theta(n)$ 呢？具体的下面再证明。 在证明之前首先了解《Introduction to Algorithm》一书中三种求解问题规模的方法： 对于问题形如$$T(n)=2T(n/2)+\\Theta(n)$$一般有三种方法，分别为代入法，递归树法，主定理法 substitution method：we guess a bound and then use mathematical in- duction to prove our guess correct. recursion-tree method：converts the recurrence into a tree whose nodes represent the costs incurred at various levels of the recursion. We use techniques for bounding summations to solve the recurrence. master method：rovides bounds for recurrences of the form$$T(n) = aT(n/b)+f(n), \\quad a \\ge 1, b&gt;1$$$ f(n)$ 为已知函数。 1.2 折半插入折半插入排序（Binary Insertion Sort）是对插入排序算法的一种改进，由于排序算法过程中，就是不断的依次将元素插入前面已排好序的序列中。由于前半部分为已排好序的数列，这样我们不用按顺序依次寻找插入点，可以采用折半查找的方法来加快寻找插入点的速度。 123456789101112131415161718192021222324public static int[] binaryInsertionSort(int[] array) &#123; if (array.length == 0) return array; for (int i = 1; i &lt; array.length; i++) &#123; // 先保存要插入的值 int key = array[i]; int left = 0, right = i - 1; // 然后寻找它需要插入的位置 while (left &lt;= right) &#123; int middle = (left + right) / 2; if (array[middle] &lt; key) &#123; left = middle + 1; &#125; else &#123; right = middle - 1; &#125; &#125; // 将 left 后边的数组整体后移一位 for (int j = i; j &gt; left; j--) &#123; array[j] = array[j - 1]; &#125; array[left] = key; &#125; return array;&#125; 分析：与直接插入相比，折半插入仅仅是对查找插入位置的方式进行了优化，减少了查找插入位置的时间，但是元素的移动次数并没有减少，所以总的来说时空复杂度与插入排序相同。 循环不变量：与插入排序一样，循环不变量为在插入数字之前已有序的数组。 1.3 希尔排序希尔排序(Shellsort) ，也称递减增量排序算法，是插入排序的一种更高效的改进版本，但是希尔排序在高效的同时也破坏了排序元素的稳定性，是非稳定排序算法。 希尔排序通过将比较的全部元素分为几个区域来提升插入排序的性能。这样可以让一个元素可以一次性地朝最终位置前进一大步。然后算法再取越来越小的步长进行排序，算法的最后一步就是普通的插入排序，但是到了这步，需排序的数据几乎是已排好的了（此时插入排序较快）。 假设有一个很小的数据在一个已按升序排好序的数组的末端。如果用复杂度为 $O(n^2)$ 的排序（冒泡排序或插入排序），可能会进行 $n$ 次的比较和交换才能将该数据移至正确位置。而希尔排序会用较大的步长移动数据，所以小数据只需进行少数比较和交换即可到正确位置。 一个更好理解的希尔排序实现：将数组列在一个表中并对列排序（用插入排序）。重复这过程，不过每次用更长的列来进行。最后整个表就只有一列了。将数组转换至表是为了更好地理解这算法，算法本身仅仅对原数组进行排序（通过增加索引的步长，例如是用i += stepSize, 而不是 i++）。 例如，假设有这样一组数[ 13 14 94 33 82 25 59 94 65 23 45 27 73 25 39 10 ]，如果我们以步长为5开始进行排序(其实就是按索引下标分组，如下标为0，5，10… 一组，而下标1，6，11…一组，下标 2，7，12… 一组)，我们可以通过将这列表放在有5列的表中来更好地描述算法，这样他们就应该看起来是这样： 123456789101112131415161718192021222324252627282930313233343536// 步长为5得到13 14 94 33 8225 59 94 65 2345 27 73 25 3910// 对每列进行排序，也就是组内排序。10 14 73 25 2313 27 94 33 3925 59 94 65 8245// 第一次排序结束后 从左往右，从上到下穿起来得到[ 10 14 73 25 23 13 27 94 33 39 25 59 94 65 82 45 ]// 步长为3得到10 14 7325 23 1327 94 3339 25 5994 65 8245// 再次对每列进行排序，即再一次组内排序。10 14 1325 23 3327 25 5939 65 7345 94 8294// 再次穿起来得到[ 10 14 13 25 23 33 27 25 59 39 65 73 45 94 82 94 ]// 此时数组已经基本有序了，再次选取步长为 1 ，也就成为了实际意义上的插入排序。// 需要注意的是，步长尽量选择为互质的数，这样就可以减小最小公倍数出现的情况，从而避免一个数字被分到不同分组里 已知的最好步长序列是由Sedgewick提出的(1, 5, 19, 41, 109,…)，该序列的项来自 $9 \\times (4^i - 2^i)$ 和 $2^{i+2}(2^{i+2}-3)+1$ 这两个算式。这项研究也表明“比较在希尔排序中是最主要的操作，而不是交换。”用这样步长序列的希尔排序比插入排序要快，甚至在小数组中比快速排序和堆排序还快，但是在涉及大量数据时希尔排序还是比快速排序慢。 另一个在大数组中表现优异的步长序列是(斐波那契数列除去0和1将剩余的数以黄金分割比的两倍的幂进行运算得到的数列):(1, 9, 34, 182, 836, 4025, 19001, 90358, 428481, 2034035, 9651787, 45806244, 217378076, 1031612713,…) 在此选择 Shell 本人推荐使用的步长为 n/2, 其他的步长实现待补充。 12345678910111213141516171819public static int[] shellSort(int[] array) &#123; int length = array.length; if (length == 0) return array; int temp; for (int step = length / 2; step &gt;= 1; step /= 2) &#123; // 步长每次取为分组长度的 1/2 for (int i = step; i &lt; length; i++) &#123; // 组内排序 temp = array[i]; int j = i - step; while (j &gt;= 0 &amp;&amp; array[j] &gt; temp) &#123; array[j + step] = array[j]; j -= step; &#125; array[j + step] = temp; &#125; &#125; return array;&#125; 分析：显然可以看到外层循环次数为 $\\lfloor \\log_2n \\rfloor$ ，而内层循环为 $n$ ，从而该排序方法的时间复杂度为 $O(n\\log_2 n)$ . 循环不变量：元素在每一个分组之内都是有序的 2. 交换类排序2.1 冒泡排序冒泡排序（英语：Bubble Sort）又称为泡式排序，是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 冒泡排序算法的运作如下： 比较相邻的元素。如果第一个比第二个大，就交换它们的位置。 对每一对相邻元素做同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数，即 冒泡排序每轮结束后至少能保证一个元素处于最终位置上。 针对除最后一个元素外的所有元素重复以上的步骤。 1234567891011121314151617public int[] bubbleSort(int[] array) &#123; int temp; for (int i = 0; i &lt; array.length - 1; i++) &#123; // 从第一个到倒数第二个元素 for (int j = 0; j &lt; array.length - 1 - i; j++) &#123; // 每轮冒泡结束后至少有一个元素到达最终位置，所以比较的元素个数依次为 // n - 1、n - 2、n - 3 ... if (array[j] &gt; array[j + 1]) &#123; // 每次取得两个数之间较大的数 temp = array[j]; array[j] = array[j + 1]; array[j + 1] = temp; &#125; &#125; &#125; return array;&#125; 冒泡排序是与插入排序拥有相等的运行时间，但是两种算法在需要的交换次数却很大地不同。在最坏的情况(逆序)，冒泡排序需要 $O(n^2)$ 次交换，而插入排序只要最多 $O(n)$ 交换。 冒泡排序如果能在内部循环第一次运行时，使用一个旗标来表示有无需要交换的可能，也可以把最优情况下的复杂度降低到 $O(n)$。在这个情况，已经排序好的数列就无交换的需要。若在每次走访数列时，把走访顺序反过来，也可以稍微地改进效率。 2.2 快速排序2.2.1 快速排序基础 上图是《Introduction to Algorithm》一书第七章对于快速排序的介绍，大意是：快速排序在问题规模为 $n$ 的情况下最坏情况达到 $\\Theta(n^2)$ ，但是由于它平均情况下出色的效率，时间规模为 $\\Theta(n\\lg n)$ ，而且在 $\\Theta(n\\lg n)$ 中所隐含的常数因子比较小，而且可以很好的就地排序。 像归并排序一样，快速排序也是基于分治(Divide And Conquer)思想的。下面是对一个典型子数组A[p..r]排序的分治过程的的三个步骤。 分解：找到一个枢轴 q将原数组划分为两个子数组 A[p..q-1],A[q+1, r]，使得前一个子数组中的所有元素都小于等于 A[q]，而后一个数组中的所有元素都大于 A[q]，下标 q 也在这个划分过程中参与计算； 解决：通过递归调用快速排序，对子数组 A[p..q-1] 和 A[q+1,r]排序； 合并：因为两个子数组是就地排序的，将它们的合并不需要操作，整个数组已排好序。 PARTITION 总是选择数组中最右边的元素作为 枢轴 Pivot，并围绕它来划分子数组，随着该过程的进行，数组被划分为四个区域(可能有空的)，在第 $3-6$ 行的 for 循环每次迭代的开始，每一个区域都满足特定的性质。这些性质就是我们从插入排序时就开始提到的 Loop Invariant，循环不变量。 在上图中，有 $A[p..i]$ 中的值均小于等于 $x$ ，而 $A[i+1,j-1]$ 之间均大于 $x$ ，而 $A[j,r-1]$ 则没有限制， $A[r]=x$ 。 下面来证明循环不变量： 初始化：在循环的第一轮迭代开始之前，有 $i=p-1$ 和 $j=p$ ，在 $p$ 和 $i$ 之间没有值，在 $i+1$ 和 $j-1$ 之间也没有值。因此，循环不变量的头两个条件显然满足。第1行中的赋值操作满足第三个条件。 保持：需要考虑下图中的两种情况，具体取决于第 $4$ 行中测试的结果。 第一种情况显示当 $A[j]&gt;x$ 时所做的处理；循环中的唯一操作是 $j++$ ，在 $j++$ 后，条件 $2$ 对 $A[j-1]$ 成立，且所有其他项保持不变。 第二种情况显示当 $A[j]\\le x$ 时所做的处理：将 $i$ 加 $1$，交换 $A[i]$ 和 $A[j]$ ，再将 $j$ 加1。因为进行了交换，现在 $A[i]\\le x$ ，因而条件 $1$ 满足。类似的，还有 $A[j-1]&gt;x$ ，因为根据循环不变量，被交换进 $A[j-1]$ 的项目是大于 $x$ 的。 终止：当终止时， $j=r$ ，于是数组中的每个元素都在循环不变量所描述的三个集合的某一个之中。也就是已经把数组中的所有元素划分到了三个集合中：一个集合包含了小于等于 $x$ 的元素，第二个集合中包含了大于 $x$ 的元素，还有一个只包含了 $x$ 的集合。 此过程的运行时间为 $\\Theta(n)$ ，其中 $n=r-p+1$ ，暂不证明。 2.2.2 快速排序性能快速排序的运行时间与划分是否对称有关，而是否对称又与选择哪一个元素为 $pivot$ 有关，如果划分是对称的，那么从渐进意义上来讲就与 归并算法 一样快；如果划分是不对称的，那么从渐进意义上来讲就与 插入排序 一样慢。 最坏情况：每次划分的两个区域分别包含 $n-1$ 和 $1$ 个元素，运行时间递归式为： $$T(n) = T(n-1)+T(0)+\\Theta(n)=T(n-1)+\\Theta(n)$$ 最好情况：每次划分的两个子问题的大小都不可能大于 $n/2$ ，其中的一个问题大小为 $\\lfloor n/2 \\rfloor$ ，另一个子问题的大小为 $\\lceil n/2 \\rceil - 1$ ，运行时间递归式为： $$T(n) = 2T(n/2)+\\Theta(n)$$ 123456789101112131415161718192021222324252627282930313233343536373839// 对自身的两次递归调用public int[] quickSort(int[] array, int p, int r) &#123; if (p &lt; r) &#123; int q = partition(array, p, r); quickSort(array, p, q - 1); quickSort(array, q + 1, r); &#125; return array;&#125;// 尾递归调用public int[] tailRecursiveQuickSort(int[] array, int left, int right) &#123; while (left &lt; right) &#123; int middle = partition(array, left, right); tailRecursiveQuickSort(array, left, middle - 1); left = right + 1; &#125; return array;&#125;// 由 N.Lomuto 提出private int partition(int[] array, int p, int r) &#123; int x = array[r]; int i = p - 1; for (int j = p; j &lt; r; j++) &#123; if (array[j] &lt;= x) &#123; i++; swap(array, i, j); &#125; &#125; swap(array, i + 1, r); return i + 1;&#125;private void swap(int[] a, int i, int j) &#123; int temp = a[i]; a[i] = a[j]; a[j] = temp;&#125; 最初的快排数组划分方式，由 C.R.Hoare 提出，也是国内本科教学中最常见的划分方法。 12345678910111213141516171819202122232425262728/*** 这是 C.R.Hoare 最早提出的分区算法，选最右元素为枢轴，* 1. 从左向右找到第一个大于等于 key 的值 array[i]；* 2. 从右向左找到第一个小于 key 的值 array[j]；* 3. 交换找到的这两个值 exchange(array, i, j)* 4. 循环结束后，交换 i 和最右元素的值，复位 array[right] 的最终位置* 5. 返回 array[right] 的新位置* @param arr 待排序数组* @param l 数组左边界* @param r 数组右边界* @return 枢轴索引*/public static int hoarePartition(int[] array, int left, int right) &#123; int key = array[right]; int i = left; int j = right; while (i &lt; j) &#123; while (i &lt; j &amp;&amp; array[i] &lt; key) &#123; i++; &#125; while (i &lt; j &amp;&amp; array[j] &gt;= key) &#123; j--; &#125; exchange(array, i, j); &#125; exchange(array, i, right); return i;&#125; 无论选择何种方式实现 partition，一定要确保在 pivot 左边的 &lt;= pivot，pivot 右边的 &gt; pivot。 2.2.3 分析与具体证明 如上图所示，我们假设在每次划分时按 $9:1$ 的比例划分两个子数组，生成的递归树如上图所示，运行时间递归式为$$T(n)\\le T(9n/10)+T(n/10)+cn$$收敛速度更快的是 $\\log_{10}n$ ，因为每次都减小为上一次的 $\\frac{1}{10}$ ，但是为了让全部的子数组都收敛到 $1$ ，收敛时间则取决于 $\\log_{10/9}n = \\Theta(n)$ ,而把每一水平行加起来的结果都小于等于 $cn$ ，再乘以树的高度 $\\Theta(n)$ ，得到总的运行时间为 $O(n\\lg n)$ ，这与划分长度是每次在正中间的效果是一样的。 事实上，即使是按照 $99:1、999:1、9999:1、 \\dots$ 这种常数比例进行的划分都会产生深度为 $\\Theta(n)$ 的递归树，其中每一层的代价为 $O(n)$，那么总的运行时间也就是 $O(n\\lg n)$ 了。伪代码中每次选取子数组中最右边的元素作为 $pivot$ 也是以此为依据的。 3. 选择类排序3.1 简单选择排序选择排序（Selection sort）是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 选择排序的主要优点与数据移动有关。如果某个元素位于正确的最终位置上，则它不会被移动。 选择排序每次交换一对元素，它们当中至少有一个将被移到其最终位置上，因此对 $n$ 个元素的表进行排序总共进行至多次 $n-1$ 交换。 在所有的完全依靠交换去移动元素的排序方法中，选择排序属于非常好的一种。 12345678910初始值： 3 1 5 7 2 4 9 6第一趟： 1 3 5 7 2 4 9 6第二趟： 1 2 5 7 3 4 9 6第三趟： 1 2 3 7 5 4 9 6第四趟： 1 2 3 4 5 7 9 6第五趟： 1 2 3 4 5 7 9 6第六趟： 1 2 3 4 5 6 9 7第七趟： 1 2 3 4 5 6 7 9第八趟： 1 2 3 4 5 6 7 9 算法实现： 123456789101112131415161718192021public int[] selectionSort(int[] arr) &#123; if (arr == null || arr.length == 0) return new int[0]; int min, minIdx; // 保存最小值及其索引 for (int i = 0; i &lt; arr.length; i++) &#123; min = arr[i]; // 保存最小值 minIdx = i; // 保存最小值的索引 for (int j = i + 1; j &lt; arr.length; j++) &#123; // 遍历当前位置之后的所有元素 if (arr[j] &lt; min) &#123; min = arr[j]; // 更新最小值 minIdx = j; // 更新最小值的索引 &#125; &#125; // 等到找到最小值以后，将当前位置的值与最小值进行交换 if (minIdx != i) &#123; int tmp = arr[i]; arr[i] = arr[minIdx]; arr[minIdx] = tmp; &#125; &#125; return arr;&#125; 最好情况即为数组已有序，最坏情况为逆序。 但是无论最好最坏情况，选择排序的比较次数都是 $\\frac{n(n-1)}{2}$ ，仅有交换次数不同，最好情况交换 $0$ 次，最坏情况交换 $n-1$ 次。 总的问题规模为 $O(n^2)$ 。 3.2 堆排序 上图是《Introduction to Algorithm》一书第六章对于堆排序的介绍，简单的翻译一下就是：像归并排序而不像插入排序，堆排序的运行时间为 $O(n\\lg n)$ ; 像插入排序而不像归并排序，堆排序是一种原地(in place)排序算法，在任何时刻，数组中只有常数个元素存储在输入数组之外。 3.2.1 堆简介（二叉）堆 数据结构式一种数组对象，如下图所示，它可以被视为一课 完全二叉树，树中每个节点与数组中存放该节点值的那个元素对应。根据完全二叉树的定义，树种除了最后一层外，其它层一定是满的，树的根为 $A[0]$ ，对于下标为 $i$ 的结点而言，父节点下标为 $\\lfloor i/2-1 \\rfloor$ ，左孩子为 $\\lfloor 2i-1 \\rfloor$ ，右孩子为 $\\lfloor 2i \\rfloor$ 。在计算机底层，可以通过 位运算 快速地计算出各个下标。 表示堆的数组 $A$ 是一个具有两个属性的对象： $length[A]$ 是数组中的元素个数，heap-size[A]是存放在 $A$ 中的堆的元素个数，所以有 heap-size[A] &lt;= length[A] . 在堆排序算法中，我们使用的是 大根堆，小根堆 通常用于构造 优先队列。堆可以被看作是一棵树，因为是完全二叉树，所以其高度为 $\\Theta(n)$ ，堆结构的一些基本操作的运行时间至多与树的高度成正比，为 $O(\\lg n)$ 。下面给出一些基本过程，并说明它们在排序算法和优先队列数据结构中如何使用。 MAX-HEAPIFY ：运行时间为 $O(\\lg n)$ ，是保持大根堆性质的关键。 BUILD-MAX-HEAP ：以线性时间运行，可以在无序的输入数组基础上构造出大根堆。 HEAPSORT ：运行时间为 $O(n\\lg n)$ ，对一个数组原地进行排序。 MAX-HEAP-INSERT, HEAP-EXTRACT-MAX, HEAP-INCREASE-KEY, HEAP-MAXIMUM 过程的运行时间为 $O(\\lg n)$ ，可以让堆结构作为优先队列使用。 3.2.2 保持堆的性质MAX-HEAPIFY 是对大根堆进行操作的重要子程序。其输入一个数组 $A$ 和下标 $i$ 。当 MAX-HEAPIFY 被调用时，我们假设以 LEFT(i) 和 RIGHT(i) 为根的两个二叉树都是大根堆，但这时 $A[i]$ 可能小于其子女，这样就违反了大根堆的性质。MAX-HEAPIFY 让$A[i]$ 在大根堆中 下降 ，使得以 $i$ 为根的子树成为最大堆。 下图描述了 MAX-HEAPIFY 的过程，在算法的每一步里， 从元素 A[i], A[LEFT(i)],A[RIGHT(i)] 中找出最大的，并将其下标存储在 $largest$ 中。如果 $A[i]$ 是最大的，则以 $i$ 为根的子树已经是大根堆，程序结束。否则， $i$ 的某个子节点中有最大元素，则交换 $A[i]$ 和 $A[largest]$ ，从而使其满足堆的性质。下标为 $largest$ 的结点在交换后的值是 $A[i]$ ，以该节点为根的子树又有可能违反大根堆的性质。因而要对该子树递归调用 MAX-HEAPIFY 。 3.2.3 建堆我们可以自底向上地用 MAX-HEAPIFY 来将一个数组 $A[0..n-1]$ 变成一个大根堆。 为了证明 BUILD-MAX-HEAP 的正确性，可以使用如下的 循环不变量。 在 $2-3$ 行中 $for$ 循环的每一次迭代开始时，结点 $i+1, i+2, \\dots, n-1$ 都是一个大根堆的根 3.2.4 堆排序算法开始时，对排序算法先用 BUILD-MAX-HEAP 来将输入数组 $A[0..n-1]$ 构造成一个大根堆。因为数组中最大元素在根 $A[0]$ ，则可以通过把它与 $A[n-1]$ 互换来达到最终正确的位置。现在如果从堆中去掉结点 $n-1$ ，可以很容易地将 $A[0..n-2]$ 建成大根堆。原来根的子女仍是大根堆，而新的根元素可能违背了大根堆的性质。这时调用 MAX-HEAPIFY(A, 0) 就可以保存这一性质，构造出大根堆，堆排序算法不断重复这个过程，堆的大小由 $n-1$ 降到 $2$ 。 循环不变量：在每次 $for$ 循环的 $2-5$ 行的迭代开始时，子数组 $A[0..i]$ 是一个包含了 $A[0..n-1]$ 中的 $i$ 个最小元素的最大堆；而子数组 $A[i+1..n-1]$ 包含了已排序的 $A[0..n-1]$ 中的 $n-i$ 个最大元素。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class HeapSort &#123; private int[] arr; public HeapSort(int[] arr) &#123; this.arr = arr; &#125; /** * 堆排序的主要入口方法，共两步。 */ public void sort() &#123; /* * 第一步：将数组堆化 * beginIndex = 第一个非叶子节点。 * 从第一个非叶子节点开始即可。无需从最后一个叶子节点开始。 * 叶子节点可以看作已符合堆要求的节点，根节点就是它自己且自己以下值为最大。 */ int len = arr.length - 1; int beginIndex = (arr.length &gt;&gt; 1)- 1; for (int i = beginIndex; i &gt;= 0; i--) maxHeapify(i, len); /* * 第二步：对堆化数据排序 * 每次都是移出最顶层的根节点A[0]，与最尾部节点位置调换，同时遍历长度 - 1。 * 然后从新整理被换到根节点的末尾元素，使其符合堆的特性。 * 直至未排序的堆长度为 0。 */ for (int i = len; i &gt; 0; i--) &#123; swap(0, i); maxHeapify(0, i - 1); &#125; &#125; private void swap(int i, int j) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; /** * 调整索引为 index 处的数据，使其符合堆的特性。 * * @param index 需要堆化处理的数据的索引 * @param len 未排序的堆（数组）的长度 */ private void maxHeapify(int index, int len) &#123; int li = (index &lt;&lt; 1) + 1; // 左子节点索引 int ri = li + 1; // 右子节点索引 int cMax = li; // 子节点值最大索引，默认左子节点。 if (li &gt; len) return; // 左子节点索引超出计算范围，直接返回。 if (ri &lt;= len &amp;&amp; arr[ri] &gt; arr[li]) // 先判断左右子节点，哪个较大。 cMax = ri; if (arr[cMax] &gt; arr[index]) &#123; swap(cMax, index); // 如果父节点被子节点调换， maxHeapify(cMax, len); // 则需要继续判断换下后的父节点是否符合堆的特性。 &#125; &#125;&#125; 4. 归并排序有很多算法在结构上是递归的，为了解决一个给定的问题，算法要一次或多次地递归调用其自身来解决相关的子问题，这些算法通常采用 分治策略(Divide And Conquer)：将原问题分成 $n$ 个规模较小而结构与原问题相似的子问题；递归地解决这些子问题，最后合并其结果，就得到原问题的解。 分治策略在每一层上都有三个步骤： 分解(Divide)：将原问题分解为一系列的子问题； 解决(Conquer): 递归地解各子问题，若子问题足够小，则直接求解； 合并(Combine): 将子问题的结果合并成原问题的解。 归并排序(Merge Sort) 算法完全按照上述模式，直观操作如下： 分解：将 $n$ 个元素分成各含 $n/2$ 个元素的子序列； 解决：用归并排序对两个子序列递归地排序； 合并：合并两个已排序的子序列得到排序的结果。 在对子序列排序时，其长度为 $1$ 时递归结束，单个元素被认为是已经排好序的。 归并排序的关键步骤在于合并步骤中的合并两个已排序子序列。为做归并，引入一个辅助过程 MERGE(A, p, q, r) . 再举扑克牌这个例子，假如我们现在有两堆已好序的扑克牌，它们面朝上放着，最小的牌放在最上面，而我们要做的是把牌从小到大排成一副完整的牌。在刚开始时我们从两堆牌中取出最上面一张牌，然后比较二者大小，然后将其中较小的牌反面盖上，我们称其为第三堆，然后将较大的牌与另一堆最上面的的最小的牌进行比较。然后把二者较小的牌放到第三堆上，重复此过程，直到其中一堆牌全部被放到第三堆上，第一堆或者第二堆剩下的牌直接全部盖到第三堆上去，此时就完成了排序。 伪代码: 这里用 $\\infty$ 作为哨兵，如果出现哨兵牌则说明该牌不可能是两者之中较小的牌，一旦两张牌同时出现哨兵牌，则说明所有的非哨兵牌已经全部放到第三堆上去了。 初始化：在 for 循环的第一轮迭代开始之前，有 k = p, 因而子数组 A[p..k-1] 是空的。这个空的子数组包含了 L 和 R 中 k - p = 0 个最小的元素。此外，又因为 i = j + 1 ， L[i] 和 R[j] 都是各自所在数组中，尚未被副指挥数组 A 中的最小元素。 保持：为了说明每一轮迭代中都能使循环不变式保持成立，首先架设 $L[i]\\le R[j]$ ，那么 $L[i]$ 就是尚未被复制回数组 A 中的最小元素。由于 A[p..k-1] 包含了 k - p 个最小的元素，因此在第 $14$ 行将 L[i] 复制回 A[k] 中后，子数组 A[p..k] 将包含 k - p + 1 个最小的元素。增加 k, i 的值，会为下一轮迭代重新建立 循环不变量 的值，如果这次有 $L[i] \\ge R[j]$，则第 $16-17$ 行就会执行相应的操作，以使循环不变量保持成立。 终止：在终止时， k = r + 1 ，根据 循环不变量，子数组 A[p..k-1] 包含了 $L[1..n_1+1]$和$R[1..n_2+1]$中 k - p = r - p + 1 个最小元素，并且已经是排好序的。数组 L、R 合起来包含了 $n_1+n_2+2=r-p+3$ 个元素，除了两个最大的元素外，其余的所有元素都已被复制回数组 A 中，这两个最大元素都是哨兵。 $MERGE$ 过程的运行时间为 $\\Theta(n)$ ，此处 $n=r-p+1$ , 其中第 $1-3、8-11$ 行每一行的运行时间都是常量， $4-7$ 行 for循环所需时间为 $\\Theta(n_1+n_2)=\\Theta(n)$, 第 $12-17$ 行的 for 循环共有 $n$ 轮迭代，其中的每一轮迭代所需时间都是常量。 现在，就可以将 $MERGE$ 过程作为合并排序中的一个子程序来使用了， 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 归并排序 */public static int[] mergeSort(int[] A, int left, int right) &#123; if (left &lt; right) &#123; int middle = (left + right) &gt;&gt;&gt; 1; mergeSort(A, left, middle); mergeSort(A, middle + 1, right); return merge(A, left, middle, right); &#125; return A;&#125;/** * @param A A=[2,4,5,7,1,2,3,6], L = [2,4,5,7], R=[1,2,3,6]. * @param left p = 0 * @param middle q = 3 * @param right r = 7 */private static int[] merge(int[] A, int left, int middle, int right) &#123; int lenOfL = middle - left + 1; int lenOfR = right - middle; int[] L = new int[lenOfL + 1]; int[] R = new int[lenOfR + 1]; if (lenOfL &gt;= 0) &#123; System.arraycopy(A, left, L, 0, lenOfL); &#125; for (int j = 0; j &lt; lenOfR; j++) &#123; R[j] = A[middle + j + 1]; &#125; L[lenOfL] = Integer.MAX_VALUE; R[lenOfR] = Integer.MAX_VALUE; int i = 0, j = 0; for (int k = left; k &lt; right + 1; k++) &#123; // k &lt; right + 1，而不是 k &lt; right， // L 和 R 的长度都增加了 1，如果这里不 +1 的话，不能保证 L 和 R 走到头。 A[k] = L[i] &lt;= R[j] ? L[i++] : R[j++]; &#125; return A;&#125; 归并排序的最坏情况运行时间表达式为，由 主定理 可以求得 $T(n) = \\Theta(n\\lg n)$$$T(n)=2T(n/2)+\\Theta(n)$$ 5. 线性时间排序到目前为止，我们所分析过的排序算法都有一个令人感兴趣的性质：排序结果中，各元素的次序基于输入元素间的比较，我们把这类排序算法称为 比较排序。 5.0 排序算法的下界比较排序可以被抽象地视为决策树，表示排序算法作用于给定输入所做的所有比较，而控制结构，数据移动等都被忽略了。下图对应于插入排序算法作用于三个元素的输入序列上的决策树 定理 ： 任意一个比较排序算法在最坏情况下，都需要做 $\\Omega(n\\lg n)$ 次的比较。 证明 ：对于一棵每个排列都作为一个可达叶结点出现的决策树，考虑一棵高度为 $h$ 的、具有 $l$ 个可达叶结点的决策树，它对应于对 $n$ 个元素所做的比较排序。因为 $n$ 个输入元素共有 $n!$ 种排列，每一种都作为一个叶子出现在树中，所以有 $n! \\le l$ ，又由于在一棵高度为 $h$ 的二叉树中，叶子的数目不多于 $2^k$ ，则有$$n! \\le l \\le2^k$$对该式取对数，得到 $h\\ge \\lg(n!) = \\Omega(n\\lg n)$ . 从中也可以推论出：堆排序和归并排序都是渐进最优的比较排序算法。 5.1 计数排序计数排序假设 $n$ 个输入元素中的每一个都是介于 $0..k$ 之间的整数，此处 $k$ 为某个整数。当 $k=O(n)$ 时，计数排序的运行时间为 $\\Theta(n)$ . 计数排序的 基本思想 就是对每一个输入元素 $x$ ，确定出小于 $x$ 的元素个数。有了这一信息，就可以把 $x$ 直接放到它在最终输出数组中的位置上。例如，如果有 $17$ 个元素小于 $x$ ，那么 $x$ 就属于第 $18$ 个输出位置。当有几个元素相同时，就要略作修改，因为同一个位置当然不能存放多个元素，就像散列表的冲突碰撞后要做处理一样。 在下面计数排序的伪代码中，我们假定输入是个数组 $A[1..n]$ , $length[A]=n$，另外还需要两个数组：存放排序结果的 $B[1..n]$ ，以及提供临时存储区的 $C[0..k]$ . 1-3 行代码：初始化，所花时间 $\\Theta(k)$ 4-5 行代码：检查每个输入元素，如果值为 $i$ 即增加 $C[i]$ 的值，于是在第5行之后， $C[i]$就存放了等于元素 $i$ 的个数，所花时间 $\\Theta(n)$ 7-8行代码：通过在数组 $C$ 中记录计数和，可以确定对每一个 $i=0,1,2,\\dots,k$ ，有多少输入元素是小于等于 $i$ 的，所花时间 $\\Theta(k)$ 10-12行代码：把每个元素 $A[j]$ 放在输出数组 $B$ 中与其对应的最终位置上，所花时间 $\\Theta(n)$ 这样，总的时间就是 $\\Theta(n+k)$ ，当 $k=O(n)$ 时，计数排序的运行时间就是 $\\Theta(n)$ ，同时也是稳定的。 1234567891011121314151617181920212223242526public static int[] countingSort(int[] A) &#123; int[] B = new int[A.length]; // 假设A中的数据 a' 有，0 &lt;= a' &amp;&amp; a' &lt; k并且 k = 100 int k = 100; countingSort(A, B, k); return B;&#125;private static void countingSort(int[] A, int[] B, int k) &#123; int[] C = new int[k]; // 计数 for (int j = 0; j &lt; A.length; j++) &#123; int a = A[j]; C[a] += 1; &#125; // 求计数和 for (int i = 1; i &lt; k; i++) &#123; C[i] = C[i] + C[i - 1]; &#125; // 整理 for (int j = A.length - 1; j &gt;= 0; j--) &#123; int a = A[j]; B[C[a] - 1] = a; C[a] -= 1; &#125;&#125; 5.2 基数排序基数排序（英语：Radix sort）是一种非比较型整数排序算法，其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序也不是只能使用于整数。 它是这样实现的：将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排序完成以后，数列就变成一个有序序列。 假如我们想根据三个关键字年、月、日来对日期进行排序，我们可以先比较年，再比较月，再比较日，当然也可以先比较日，再比较月，再比较年。这也就对应了 LSD（Least significant digital）或MSD（Most significant digital），LSD的排序方式由键值的最右边开始，而MSD则相反，由键值的最左边开始。 基数排序的代码是非常直观的 引理1：给定 $n$ 个 $d$ 位数，每一个数位有 $k$ 种可能的值，基数排序算法能以 $\\Theta(d(n+k))$ 的时间正确地对这些数进行排序。 引理2：给定 $n$ 个 $b$ 位数和任何正整数 $r\\le b$ ，$RADIX-SORT$ 能在 $\\Theta(\\frac{b(n+2^r)}{r})$ 的时间内正确地对这些数进行排序。 12345678910111213141516171819202122232425262728// 来源于 百度百科 https://baike.baidu.com/item/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8Fpublic static int[] radixSort(int[] number, int d) &#123; int k = 0; int n = 1; int m = 1; //控制键值排序依据在哪一位 int[][] temp = new int[10][number.length]; //数组的第一维表示可能的余数0-9 int[] order = new int[10]; //数组orderp[i]用来表示该位是i的数的个数 while(m &lt;= d) &#123; for(int i = 0; i &lt; number.length; i++) &#123; int lsd = ((number[i] / n) % 10); temp[lsd][order[lsd]] = number[i]; order[lsd]++; &#125; for(int i = 0; i &lt; 10; i++) &#123; if(order[i] != 0) &#123; for(int j = 0; j &lt; order[i]; j++) &#123; number[k] = temp[i][j]; k++; &#125; &#125; order[i] = 0; &#125; n *= 10; k = 0; m++; &#125; return number;&#125; 菜鸟教程代码实现 基数排序是否要比基于比较的排序算法(如快速排序)要好呢？如果根据常见的情况有 $b=O(\\lg n)$ ，并选择 $r \\approx \\lg n$ ，则基数排序的运行时间为 $\\Theta(n)$ ，这看上去要比快速排序的平均情况 $\\Theta(n\\lg n)$ 要好一些，但是也不是一定的，在两个排序时间中隐含在 $\\Theta$ 中的常数因子是不同的。 5.3 桶排序当 桶排序(bucket sort) 的输入符合均匀分布时，即可以以线性时间运行。与基数排序类似，桶排序也对输入作了某种架设，因而运行的很快。具体来说，计数排序假设输入是由一个小范围内的整数构成，而桶排序择假设输入由一个随机过程产生，该过程将元素均匀地分布在区间 $[0,1)$ 上。 桶排序的思想就是把区间 $[0,1)$ 划分成 $n$ 个相同大小的子区间，或称为 桶。然后，将 $n$ 个输入数分布到各个桶中去。因为输入数均匀分布在 $[0,1)$ 上，所以一般不会有很多数落在一个桶中的情况。为得到结果，先对各个桶中的数进行排序，然后按次序把各桶中的元素列出来即可。 为了说明这个算法是正确的，我们假设两个元素 $A[i]\\le A[j]$ 。由于 $\\lfloor nA[i] \\rfloor \\le \\lfloor nA[j] \\rfloor$ ，元素 $A[i]$ 或者被放入 $A[j]$ 所在桶中，或者被放入一个下标更小的桶中，如果 $A[i],A[j]$ 落在同一个桶中，则第 $7-8$ 行中的 for 循环会将它们按适当的顺序排列；如果 $A[i],A[j]$ 落在不同桶中，则第 $9$ 行会将它们按适当的顺序排列。因此，桶排序是正确的。 除第 $8$ 行外，所有其他各行在最坏情况下的运行时间都是 $O(n)$ ，仍然需要对第 $8$ 行中插入排序的 $n$ 次调用所花的总时间。 为了分析调用插入排序的时间代价，设 $n_i$ 为表示桶 $B[i]$ 中元素个数的随机变量。因为插入排序以二次时间运行，因而桶排序的运行时间：$$T(n)=\\Theta(n) + \\sum_{i=0}^{n-1}O(n_i^2)$$对两边的值取期望( 概率论中的期望 $E(x)$ )，并利用期望函数的线性性质，可以得到：$$E[T(n)]=E[\\Theta(n) + \\sum_{i=0}^{n-1}O(n_i^2)]=\\Theta(n)+ \\sum_{i=0}^{n-1}E[O(n_i^2)]=\\Theta(n)+ \\sum_{i=0}^{n-1}O(E[n_i^2])$$下式对 $i=0,1,\\cdots,n-1$ 是成立的(证明略)。$$E[n_i^2]=2-\\frac{1}{n}$$总之最后我们可以得出这样一个结论：即桶排序的期望运行时间为 $\\Theta(n) + n·O(2-1/n)=\\Theta(n)$ ，于是，整个桶排序算法以线性 期望时间 运行。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 来源于：https://zh.wikipedia.org/wiki/%E6%A1%B6%E6%8E%92%E5%BA%8F#Java%E5%AF%A6%E7%8F%BE%E7%AE%97%E6%B3%95private int indexFor(int a, int min, int step) &#123; return (a - min) / step;&#125;public void bucketSort(int[] arr) &#123; int max = arr[0], min = arr[0]; for (int a : arr) &#123; if (max &lt; a) max = a; if (min &gt; a) min = a; &#125; // 該值也可根據實際情況選擇 int bucketNum = max / 10 - min / 10 + 1; List buckList = new ArrayList&lt;List&lt;Integer&gt;&gt;(); // create bucket for (int i = 1; i &lt;= bucketNum; i++) &#123; buckList.add(new ArrayList&lt;Integer&gt;()); &#125; // push into the bucket for (int i = 0; i &lt; arr.length; i++) &#123; int index = indexFor(arr[i], min, 10); ((ArrayList&lt;Integer&gt;) buckList.get(index)).add(arr[i]); &#125; ArrayList&lt;Integer&gt; bucket = null; int index = 0; for (int i = 0; i &lt; bucketNum; i++) &#123; bucket = (ArrayList&lt;Integer&gt;) buckList.get(i); insertSort(bucket); for (int k : bucket) &#123; arr[index++] = k; &#125; &#125;&#125;// 把桶內元素插入排序private void insertSort(List&lt;Integer&gt; bucket) &#123; for (int i = 1; i &lt; bucket.size(); i++) &#123; int temp = bucket.get(i); int j = i - 1; for (; j &gt;= 0 &amp;&amp; bucket.get(j) &gt; temp; j--) &#123; bucket.set(j + 1, bucket.get(j)); &#125; bucket.set(j + 1, temp); &#125;&#125; 菜鸟教程代码实现 REFERENCES 《Introduction to Algorithm》3rd edition Wikipedia","categories":[{"name":"算法","slug":"算法","permalink":"http://raymond-zhao.top/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://raymond-zhao.top/tags/%E7%AE%97%E6%B3%95/"},{"name":"Java","slug":"Java","permalink":"http://raymond-zhao.top/tags/Java/"}]},{"title":"位运算的运用","slug":"2020-03-16-Java-BitOps","date":"2020-03-16T09:20:00.000Z","updated":"2021-06-20T05:59:48.000Z","comments":true,"path":"2020/03/16/2020-03-16-Java-BitOps/","link":"","permalink":"http://raymond-zhao.top/2020/03/16/2020-03-16-Java-BitOps/","excerpt":"","text":"位运算提升效率原文出自知乎LeetCode 1. 位操作符 &amp; 与运算符：全1为1，反之为0. | 或运算符：全0为0，反之为1；只要一个位为1，结果为1. ^ 异或运算符：两个位相同则为 0，不同则为 1 ~ 取反运算符：0 则变为 1，1 则变为 0 &lt;&lt; 左移运算符：向左进行移位操作，高位丢弃，低位补 0 &gt;&gt; 右移运算符：向右进行移位操作，对无符号数，高位补 0，对于有符号数，高位补符号位 2. 常见位运算很多功能采用基本的运算也可以完成，但是位运算的效率更高，因为省去了将int、Integer 等整数转化为计算机可以直接识别的二进制的过程 2.1 乘除法1234// 数 a 向右移一位，相当于将 a 除以 2；数 a 向左移一位，相当于将 a 乘以 2int a = 2;a &gt;&gt; 1; // a = 1a &lt;&lt; 1; // a = 4 2.2 交换两个数1234567891011121314// 位操作交换两数可以不需要第三个临时变量，虽然普通操作也可以做到，但位运算效率更高// 普通操作void swap(int &amp;a, int &amp;b) &#123; a = a + b; b = a - b; a = a - b;&#125;// 位与操作void swap(int &amp;a, int &amp;b) &#123; a ^= b; // a ^= b ---&gt; a = (a^b); b ^= a; // b = b^(a^b) ---&gt; b = (b^b)^a = a a ^= b; // a = (a^b)^a = (a^a)^b = b&#125; 2.3 判断奇偶数1234// 在计算机中, 偶数的最后一位是0, 而奇数的最后一位是1public boolean isEven(int num) &#123; return (num &amp; 1) == 0;&#125; 2.4 交换符号123456// 将正数变成负数，负数变成正数// 整数取反加1，正好变成其对应的负数(补码表示)// 负数取反加1，则变为其原码，即正数public int reverse(int num) &#123; return ~a + 1;&#125; 2.5 求绝对值123456789101112// 整数的绝对值是其本身，负数的绝对值正好可以对其进行取反加一求得，即我们首先判断其符号位（整数右移 31 位得到 0，负数右移 31 位得到 -1,即 0xffffffff），然后根据符号进行相应的操作int abs(int num) &#123; int i = num &gt;&gt; 31; return i == 0 ? num : (~num + 1);&#125;// 上面的操作可以进行优化，可以将 i == 0 的条件判断语句去掉。我们都知道符号位 i 只有两种情况，即 i = 0 为正，i = -1 为负。对于任何数与 0 异或都会保持不变，与 -1 即 0xffffffff 进行异或就相当于对此数进行取反,因此可以将上面三目元算符转换为((a^i)-i)，即整数时 a 与 0 异或得到本身，再减去 0，负数时与 0xffffffff 异或将 a 进行取反，然后在加上 1，即减去 i(i =-1)int abs(int num) &#123; int i = num &gt;&gt; 31; return ((num^i) - i);&#125; 2.6 高低位交换123456789101112// 给定一个 16 位的无符号整数，将其高 8 位与低 8 位进行交换，求出交换后的值// 34520的二进制表示：// 10000110 11011000// 将其高8位与低8位进行交换，得到一个新的二进制数：// 11011000 10000110// 其十进制为55430// 从上面移位操作我们可以知道，只要将无符号数 a&gt;&gt;8 即可得到其高 8 位移到低 8 位，高位补 0；将 a&lt;&lt;8 即可将 低 8 位移到高 8 位，低 8 位补 0，然后将 a&gt;&gt;8 和 a&lt;&lt;8 进行或操作既可求得交换后的结果。unsigned short a = 34520;a = (a &gt;&gt; 8) | (a &lt;&lt; 8);","categories":[{"name":"算法","slug":"算法","permalink":"http://raymond-zhao.top/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://raymond-zhao.top/tags/%E7%AE%97%E6%B3%95/"}]}],"categories":[{"name":"Spring","slug":"Spring","permalink":"http://raymond-zhao.top/categories/Spring/"},{"name":"消息中间件","slug":"消息中间件","permalink":"http://raymond-zhao.top/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"Java","slug":"Java","permalink":"http://raymond-zhao.top/categories/Java/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://raymond-zhao.top/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"多线程与并发","slug":"多线程与并发","permalink":"http://raymond-zhao.top/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"},{"name":"JVM","slug":"JVM","permalink":"http://raymond-zhao.top/categories/JVM/"},{"name":"MySQL","slug":"MySQL","permalink":"http://raymond-zhao.top/categories/MySQL/"},{"name":"数据结构","slug":"数据结构","permalink":"http://raymond-zhao.top/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"LeetCode","slug":"LeetCode","permalink":"http://raymond-zhao.top/categories/LeetCode/"},{"name":"算法","slug":"算法","permalink":"http://raymond-zhao.top/categories/%E7%AE%97%E6%B3%95/"},{"name":"Redis","slug":"Redis","permalink":"http://raymond-zhao.top/categories/Redis/"},{"name":"Docker","slug":"Docker","permalink":"http://raymond-zhao.top/categories/Docker/"},{"name":"Utils","slug":"Utils","permalink":"http://raymond-zhao.top/categories/Utils/"},{"name":"面试","slug":"面试","permalink":"http://raymond-zhao.top/categories/%E9%9D%A2%E8%AF%95/"},{"name":"操作系统","slug":"操作系统","permalink":"http://raymond-zhao.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"架构师软技能","slug":"架构师软技能","permalink":"http://raymond-zhao.top/categories/%E6%9E%B6%E6%9E%84%E5%B8%88%E8%BD%AF%E6%8A%80%E8%83%BD/"},{"name":"读书观影","slug":"读书观影","permalink":"http://raymond-zhao.top/categories/%E8%AF%BB%E4%B9%A6%E8%A7%82%E5%BD%B1/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://raymond-zhao.top/tags/Spring/"},{"name":"IoC","slug":"IoC","permalink":"http://raymond-zhao.top/tags/IoC/"},{"name":"Kafka","slug":"Kafka","permalink":"http://raymond-zhao.top/tags/Kafka/"},{"name":"Java8","slug":"Java8","permalink":"http://raymond-zhao.top/tags/Java8/"},{"name":"TCP","slug":"TCP","permalink":"http://raymond-zhao.top/tags/TCP/"},{"name":"HTTP","slug":"HTTP","permalink":"http://raymond-zhao.top/tags/HTTP/"},{"name":"HTTPS","slug":"HTTPS","permalink":"http://raymond-zhao.top/tags/HTTPS/"},{"name":"volatile","slug":"volatile","permalink":"http://raymond-zhao.top/tags/volatile/"},{"name":"GC","slug":"GC","permalink":"http://raymond-zhao.top/tags/GC/"},{"name":"慢查询优化","slug":"慢查询优化","permalink":"http://raymond-zhao.top/tags/%E6%85%A2%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/"},{"name":"MySQL Explain","slug":"MySQL-Explain","permalink":"http://raymond-zhao.top/tags/MySQL-Explain/"},{"name":"MySQL 事务","slug":"MySQL-事务","permalink":"http://raymond-zhao.top/tags/MySQL-%E4%BA%8B%E5%8A%A1/"},{"name":"MySQL 索引","slug":"MySQL-索引","permalink":"http://raymond-zhao.top/tags/MySQL-%E7%B4%A2%E5%BC%95/"},{"name":"图","slug":"图","permalink":"http://raymond-zhao.top/tags/%E5%9B%BE/"},{"name":"线程与线程池","slug":"线程与线程池","permalink":"http://raymond-zhao.top/tags/%E7%BA%BF%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"name":"CAS","slug":"CAS","permalink":"http://raymond-zhao.top/tags/CAS/"},{"name":"LeetCode多线程","slug":"LeetCode多线程","permalink":"http://raymond-zhao.top/tags/LeetCode%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"AQS","slug":"AQS","permalink":"http://raymond-zhao.top/tags/AQS/"},{"name":"ConcurrentHashMap","slug":"ConcurrentHashMap","permalink":"http://raymond-zhao.top/tags/ConcurrentHashMap/"},{"name":"HashMap","slug":"HashMap","permalink":"http://raymond-zhao.top/tags/HashMap/"},{"name":"算法","slug":"算法","permalink":"http://raymond-zhao.top/tags/%E7%AE%97%E6%B3%95/"},{"name":"Redis","slug":"Redis","permalink":"http://raymond-zhao.top/tags/Redis/"},{"name":"Docker","slug":"Docker","permalink":"http://raymond-zhao.top/tags/Docker/"},{"name":"剑指 Offer","slug":"剑指-Offer","permalink":"http://raymond-zhao.top/tags/%E5%89%91%E6%8C%87-Offer/"},{"name":"Mac","slug":"Mac","permalink":"http://raymond-zhao.top/tags/Mac/"},{"name":"面试","slug":"面试","permalink":"http://raymond-zhao.top/tags/%E9%9D%A2%E8%AF%95/"},{"name":"JVM","slug":"JVM","permalink":"http://raymond-zhao.top/tags/JVM/"},{"name":"Java类库","slug":"Java类库","permalink":"http://raymond-zhao.top/tags/Java%E7%B1%BB%E5%BA%93/"},{"name":"Linux","slug":"Linux","permalink":"http://raymond-zhao.top/tags/Linux/"},{"name":"软件工程","slug":"软件工程","permalink":"http://raymond-zhao.top/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"},{"name":"设计模式","slug":"设计模式","permalink":"http://raymond-zhao.top/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"读书观影","slug":"读书观影","permalink":"http://raymond-zhao.top/tags/%E8%AF%BB%E4%B9%A6%E8%A7%82%E5%BD%B1/"},{"name":"Java","slug":"Java","permalink":"http://raymond-zhao.top/tags/Java/"}]}